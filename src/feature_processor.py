"""
ç‰¹å¾µè™•ç†å™¨æ¨¡çµ„

ä½¿ç”¨é¡ä¼¼ BaseDataLoader çš„è¨­è¨ˆæ¨¡å¼ï¼Œçµ±ä¸€ç®¡ç†ç‰¹å¾µå·¥ç¨‹å’Œç‰¹å¾µè½‰æ›çš„æµç¨‹ã€‚
æ”¯æ´ç‰¹å¾µçš„å„²å­˜å’Œè¼‰å…¥ï¼Œé¿å…é‡è¤‡è¨ˆç®— PySpark ç‰¹å¾µã€‚
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Optional, Tuple, List, Dict, Any
from abc import ABC, abstractmethod
import pickle
import json
from datetime import datetime

from src.feature_engineer import extract_features
from src.feature_transformer import (
    transform_features_for_unsupervised,
    DEFAULT_SKEWED_FEATURES
)
from src.feature_selector import FeatureSelector, FeatureSelectionStrategy, prepare_feature_set
from src.label_processor import convert_label_to_binary
from src.data_loader import get_project_root


class BaseFeatureProcessor(ABC):
    """
    ç‰¹å¾µè™•ç†å™¨æŠ½è±¡åŸºé¡
    
    å®šç¾©æ‰€æœ‰ç‰¹å¾µè™•ç†å™¨å¿…é ˆå¯¦ä½œçš„çµ±ä¸€ä»‹é¢ã€‚
    éµå¾ªèˆ‡ BaseDataLoader ç›¸åŒçš„è¨­è¨ˆæ¨¡å¼ã€‚
    
    >>> from src.feature_processor import BaseFeatureProcessor
    >>> # BaseFeatureProcessor æ˜¯æŠ½è±¡é¡åˆ¥ï¼Œä¸èƒ½ç›´æ¥å¯¦ä¾‹åŒ–
    """
    
    @abstractmethod
    def extract(
        self,
        df: pd.DataFrame,
        include_time_features: bool = True,
        time_feature_stage: int = 1
    ) -> pd.DataFrame:
        """
        æå–ç‰¹å¾µã€‚
        
        >>> import pandas as pd
        >>> processor = StandardFeatureProcessor()
        >>> df = pd.DataFrame({
        ...     'TotBytes': [100, 200, 300],
        ...     'SrcBytes': [50, 100, 150],
        ...     'StartTime': pd.to_datetime(['2021-08-17 12:00:00', '2021-08-17 12:01:00', '2021-08-17 12:02:00'])
        ... })
        >>> features = processor.extract(df, time_feature_stage=1)
        >>> 'flow_ratio' in features.columns
        True
        
        Args:
            df: æ¸…æ´—å¾Œçš„ DataFrame
            include_time_features: æ˜¯å¦åŒ…å«æ™‚é–“ç‰¹å¾µ
            time_feature_stage: æ™‚é–“ç‰¹å¾µéšæ®µï¼ˆ1-4ï¼‰
        
        Returns:
            åŒ…å«ç‰¹å¾µçš„ DataFrame
        """
        pass
    
    @abstractmethod
    def transform(
        self,
        df: pd.DataFrame,
        skewed_features: Optional[List[str]] = None
    ) -> Tuple[pd.DataFrame, Any, List[str]]:
        """
        è½‰æ›ç‰¹å¾µã€‚
        
        >>> import pandas as pd
        >>> processor = StandardFeatureProcessor()
        >>> df = pd.DataFrame({
        ...     'TotBytes': [100, 200, 300],
        ...     'hour': [9, 10, 11]
        ... })
        >>> transformed, scaler, cols = processor.transform(df, ['TotBytes'])
        >>> 'log_TotBytes' in transformed.columns
        True
        
        Args:
            df: åŒ…å«ç‰¹å¾µçš„ DataFrame
            skewed_features: éœ€è¦å°æ•¸è½‰æ›çš„ç‰¹å¾µåˆ—è¡¨
        
        Returns:
            (è½‰æ›å¾Œçš„ DataFrame, scaler ç‰©ä»¶, è½‰æ›çš„ç‰¹å¾µæ¬„ä½åˆ—è¡¨)
        """
        pass
    
    def save_features(
        self,
        features_df: pd.DataFrame,
        output_path: Optional[Path] = None,
        project_root: Optional[Path] = None,
        stage: Optional[int] = None
    ) -> Path:
        """
        å„²å­˜ç‰¹å¾µå·¥ç¨‹çµæœç‚º Parquet æ ¼å¼ã€‚
        
        >>> import pandas as pd
        >>> import tempfile
        >>> from pathlib import Path
        >>> processor = StandardFeatureProcessor()
        >>> test_df = pd.DataFrame({'feature1': [1, 2, 3], 'feature2': [4, 5, 6]})
        >>> with tempfile.TemporaryDirectory() as tmpdir:
        ...     output = processor.save_features(test_df, Path(tmpdir) / "features.parquet")
        ...     assert output.exists()
        ...     loaded = pd.read_parquet(output)
        ...     len(loaded) == 3
        True
        
        Args:
            features_df: ç‰¹å¾µå·¥ç¨‹å¾Œçš„ DataFrame
            output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ã€‚å¦‚æœç‚º Noneï¼Œå‰‡ä½¿ç”¨é è¨­è·¯å¾‘
            project_root: å°ˆæ¡ˆæ ¹ç›®éŒ„ã€‚å¦‚æœç‚º Noneï¼Œå‰‡è‡ªå‹•åµæ¸¬
            stage: ç‰¹å¾µéšæ®µï¼ˆ3 æˆ– 4ï¼‰ï¼Œç”¨æ–¼æ±ºå®šæª”æ¡ˆåç¨±ã€‚å¦‚æœç‚º Noneï¼Œå‰‡ä½¿ç”¨é è¨­
        
        Returns:
            è¼¸å‡ºæª”æ¡ˆçš„è·¯å¾‘
        """
        if project_root is None:
            project_root = get_project_root()
        
        if output_path is None:
            output_dir = project_root / "data" / "processed"
            output_dir.mkdir(parents=True, exist_ok=True)
            if stage is not None:
                output_path = output_dir / f"features_stage{stage}.parquet"
            else:
                output_path = output_dir / "features_stage4.parquet"
        
        features_df.to_parquet(
            output_path,
            engine='pyarrow',
            index=False
        )
        
        return output_path
    
    def load_features(
        self,
        input_path: Optional[Path] = None,
        project_root: Optional[Path] = None,
        stage: Optional[int] = None
    ) -> pd.DataFrame:
        """
        è¼‰å…¥å·²è™•ç†çš„ç‰¹å¾µ Parquet æª”æ¡ˆã€‚
        
        >>> import pandas as pd
        >>> import tempfile
        >>> from pathlib import Path
        >>> processor = StandardFeatureProcessor()
        >>> test_df = pd.DataFrame({'feature1': [1, 2, 3], 'feature2': [4, 5, 6]})
        >>> with tempfile.TemporaryDirectory() as tmpdir:
        ...     output = processor.save_features(test_df, Path(tmpdir) / "features.parquet")
        ...     loaded = processor.load_features(Path(tmpdir) / "features.parquet")
        ...     len(loaded) == 3
        True
        
        Args:
            input_path: è¼¸å…¥æª”æ¡ˆè·¯å¾‘ã€‚å¦‚æœç‚º Noneï¼Œå‰‡ä½¿ç”¨é è¨­è·¯å¾‘
            project_root: å°ˆæ¡ˆæ ¹ç›®éŒ„ã€‚å¦‚æœç‚º Noneï¼Œå‰‡è‡ªå‹•åµæ¸¬
            stage: ç‰¹å¾µéšæ®µï¼ˆ3 æˆ– 4ï¼‰ï¼Œç”¨æ–¼æ±ºå®šæª”æ¡ˆåç¨±ã€‚å¦‚æœç‚º Noneï¼Œå‰‡å„ªå…ˆè¼‰å…¥éšæ®µ4
        
        Returns:
            åŒ…å«ç‰¹å¾µçš„ DataFrame
        
        Raises:
            FileNotFoundError: å¦‚æœæª”æ¡ˆä¸å­˜åœ¨
        """
        if project_root is None:
            project_root = get_project_root()
        
        if input_path is None:
            if stage is not None:
                input_path = project_root / "data" / "processed" / f"features_stage{stage}.parquet"
            else:
                # å„ªå…ˆè¼‰å…¥éšæ®µ4ï¼Œå¦‚æœæ²’æœ‰å‰‡è¼‰å…¥éšæ®µ3
                stage4_path = project_root / "data" / "processed" / "features_stage4.parquet"
                stage3_path = project_root / "data" / "processed" / "features_stage3.parquet"
                if stage4_path.exists():
                    input_path = stage4_path
                elif stage3_path.exists():
                    input_path = stage3_path
                else:
                    input_path = stage4_path  # é è¨­ä½¿ç”¨éšæ®µ4è·¯å¾‘ï¼ˆæœƒè§¸ç™¼éŒ¯èª¤ï¼‰
        
        if not input_path.exists():
            raise FileNotFoundError(
                f"æ‰¾ä¸åˆ°ç‰¹å¾µæª”æ¡ˆ: {input_path}\n"
                f"è«‹å…ˆåŸ·è¡Œç‰¹å¾µå·¥ç¨‹ç”Ÿæˆç‰¹å¾µæª”æ¡ˆã€‚"
            )
        
        return pd.read_parquet(input_path, engine='pyarrow')
    
    def save_transformed_features(
        self,
        transformed_df: pd.DataFrame,
        scaler: Any,
        transformed_columns: List[str],
        output_path: Optional[Path] = None,
        project_root: Optional[Path] = None
    ) -> Tuple[Path, Path]:
        """
        å„²å­˜è½‰æ›å¾Œçš„ç‰¹å¾µå’Œ scaler ç‰©ä»¶ã€‚
        
        Args:
            transformed_df: è½‰æ›å¾Œçš„ç‰¹å¾µ DataFrame
            scaler: è¨“ç·´å¥½çš„ scaler ç‰©ä»¶ï¼ˆå¦‚ RobustScalerï¼‰
            transformed_columns: è¢«è½‰æ›çš„ç‰¹å¾µæ¬„ä½åˆ—è¡¨
            output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼ˆä¸å«å‰¯æª”åï¼‰ã€‚å¦‚æœç‚º Noneï¼Œå‰‡ä½¿ç”¨é è¨­è·¯å¾‘
            project_root: å°ˆæ¡ˆæ ¹ç›®éŒ„ã€‚å¦‚æœç‚º Noneï¼Œå‰‡è‡ªå‹•åµæ¸¬
        
        Returns:
            (ç‰¹å¾µæª”æ¡ˆè·¯å¾‘, scaler æª”æ¡ˆè·¯å¾‘)
        """
        if project_root is None:
            project_root = get_project_root()
        
        if output_path is None:
            output_dir = project_root / "data" / "processed"
            output_dir.mkdir(parents=True, exist_ok=True)
            base_path = output_dir / "features_transformed"
        else:
            base_path = output_path
        
        # å„²å­˜è½‰æ›å¾Œçš„ç‰¹å¾µ
        features_path = base_path.with_suffix('.parquet')
        transformed_df.to_parquet(
            features_path,
            engine='pyarrow',
            index=False
        )
        
        # å„²å­˜ scaler ç‰©ä»¶
        scaler_path = base_path.with_suffix('.scaler.pkl')
        with open(scaler_path, 'wb') as f:
            pickle.dump(scaler, f)
        
        # å„²å­˜è½‰æ›è³‡è¨Šï¼ˆJSONï¼‰
        info_path = base_path.with_suffix('.info.json')
        info = {
            'transformed_columns': transformed_columns,
            'timestamp': datetime.now().isoformat(),
            'feature_count': len(transformed_columns),
            'data_shape': list(transformed_df.shape)
        }
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        return features_path, scaler_path
    
    def load_transformed_features(
        self,
        input_path: Optional[Path] = None,
        project_root: Optional[Path] = None
    ) -> Tuple[pd.DataFrame, Any, List[str]]:
        """
        è¼‰å…¥è½‰æ›å¾Œçš„ç‰¹å¾µå’Œ scaler ç‰©ä»¶ã€‚
        
        Args:
            input_path: è¼¸å…¥æª”æ¡ˆè·¯å¾‘ï¼ˆä¸å«å‰¯æª”åï¼‰ã€‚å¦‚æœç‚º Noneï¼Œå‰‡ä½¿ç”¨é è¨­è·¯å¾‘
            project_root: å°ˆæ¡ˆæ ¹ç›®éŒ„ã€‚å¦‚æœç‚º Noneï¼Œå‰‡è‡ªå‹•åµæ¸¬
        
        Returns:
            (è½‰æ›å¾Œçš„ç‰¹å¾µ DataFrame, scaler ç‰©ä»¶, è½‰æ›çš„ç‰¹å¾µæ¬„ä½åˆ—è¡¨)
        
        Raises:
            FileNotFoundError: å¦‚æœæª”æ¡ˆä¸å­˜åœ¨
        """
        if project_root is None:
            project_root = get_project_root()
        
        if input_path is None:
            base_path = project_root / "data" / "processed" / "features_transformed"
        else:
            base_path = input_path
        
        features_path = base_path.with_suffix('.parquet')
        scaler_path = base_path.with_suffix('.scaler.pkl')
        info_path = base_path.with_suffix('.info.json')
        
        if not features_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç‰¹å¾µæª”æ¡ˆ: {features_path}")
        if not scaler_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ° scaler æª”æ¡ˆ: {scaler_path}")
        
        # è¼‰å…¥ç‰¹å¾µ
        transformed_df = pd.read_parquet(features_path, engine='pyarrow')
        
        # è¼‰å…¥ scaler
        with open(scaler_path, 'rb') as f:
            scaler = pickle.load(f)
        
        # è¼‰å…¥è½‰æ›è³‡è¨Š
        if info_path.exists():
            with open(info_path, 'r', encoding='utf-8') as f:
                info = json.load(f)
            transformed_columns = info.get('transformed_columns', [])
        else:
            # å¦‚æœæ²’æœ‰ info æª”æ¡ˆï¼Œå¾ DataFrame æ¨æ–·
            transformed_columns = list(transformed_df.columns)
        
        return transformed_df, scaler, transformed_columns


class StandardFeatureProcessor(BaseFeatureProcessor):
    """
    æ¨™æº–ç‰¹å¾µè™•ç†å™¨
    
    å¯¦ä½œå®Œæ•´çš„ç‰¹å¾µè™•ç†æµç¨‹ï¼š
    1. ç‰¹å¾µæå–ï¼ˆåŒ…å« PySpark éšæ®µ4ç‰¹å¾µï¼‰
    2. ç‰¹å¾µé¸æ“‡
    3. ç‰¹å¾µè½‰æ›ï¼ˆLog + RobustScalerï¼‰
    """
    
    def __init__(
        self,
        time_feature_stage: int = 4,
        use_feature_selection: bool = True,
        feature_selection_strategies: Optional[List[FeatureSelectionStrategy]] = None
    ):
        """
        åˆå§‹åŒ–æ¨™æº–ç‰¹å¾µè™•ç†å™¨ã€‚
        
        Args:
            time_feature_stage: æ™‚é–“ç‰¹å¾µéšæ®µï¼ˆ1-4ï¼‰ï¼Œé è¨­ç‚º 4ï¼ˆæœ€å®Œæ•´ï¼‰
            use_feature_selection: æ˜¯å¦ä½¿ç”¨ç‰¹å¾µé¸æ“‡
            feature_selection_strategies: ç‰¹å¾µé¸æ“‡ç­–ç•¥åˆ—è¡¨
        """
        self.time_feature_stage = time_feature_stage
        self.use_feature_selection = use_feature_selection
        self.feature_selection_strategies = (
            feature_selection_strategies 
            if feature_selection_strategies is not None 
            else [FeatureSelectionStrategy.ALL]
        )
    
    def extract(
        self,
        df: pd.DataFrame,
        include_time_features: bool = True,
        time_feature_stage: Optional[int] = None
    ) -> pd.DataFrame:
        """
        æå–ç‰¹å¾µï¼ˆåŒ…å«éšæ®µ4 PySpark ç‰¹å¾µï¼‰ã€‚
        
        Args:
            df: æ¸…æ´—å¾Œçš„ DataFrame
            include_time_features: æ˜¯å¦åŒ…å«æ™‚é–“ç‰¹å¾µ
            time_feature_stage: æ™‚é–“ç‰¹å¾µéšæ®µï¼Œå¦‚æœç‚º None å‰‡ä½¿ç”¨åˆå§‹åŒ–æ™‚çš„è¨­å®š
        
        Returns:
            åŒ…å«ç‰¹å¾µçš„ DataFrame
        """
        if time_feature_stage is None:
            time_feature_stage = self.time_feature_stage
        
        return extract_features(
            df,
            include_time_features=include_time_features,
            time_feature_stage=time_feature_stage
        )
    
    def transform(
        self,
        df: pd.DataFrame,
        skewed_features: Optional[List[str]] = None,
        feature_columns: Optional[List[str]] = None
    ) -> Tuple[pd.DataFrame, Any, List[str]]:
        """
        è½‰æ›ç‰¹å¾µï¼ˆLog + RobustScalerï¼‰ã€‚
        
        Args:
            df: åŒ…å«ç‰¹å¾µçš„ DataFrame
            skewed_features: éœ€è¦å°æ•¸è½‰æ›çš„ç‰¹å¾µåˆ—è¡¨
            feature_columns: æœ€çµ‚ä½¿ç”¨çš„ç‰¹å¾µæ¬„ä½åˆ—è¡¨
        
        Returns:
            (è½‰æ›å¾Œçš„ DataFrame, scaler ç‰©ä»¶, è½‰æ›çš„ç‰¹å¾µæ¬„ä½åˆ—è¡¨)
        """
        if skewed_features is None:
            skewed_features = DEFAULT_SKEWED_FEATURES.copy()
        
        # ç¢ºä¿åªä¿ç•™æ•¸å€¼æ¬„ä½ï¼ˆç§»é™¤ Timestampã€å­—ä¸²ç­‰ï¼‰
        numeric_df = df.select_dtypes(include=[np.number]).copy()
        
        # å¦‚æœæŒ‡å®šäº† feature_columnsï¼Œåªä¿ç•™é€™äº›æ¬„ä½
        if feature_columns is not None:
            available_cols = [col for col in feature_columns if col in numeric_df.columns]
            if not available_cols:
                raise ValueError("æŒ‡å®šçš„ç‰¹å¾µæ¬„ä½ä¸­æ²’æœ‰æ•¸å€¼å‹åˆ¥æ¬„ä½")
            numeric_df = numeric_df[available_cols]
            feature_columns = available_cols
        
        return transform_features_for_unsupervised(
            numeric_df,
            skewed_features=skewed_features,
            feature_columns=list(numeric_df.columns) if feature_columns is None else feature_columns
        )
    
    def _test_stage4_with_sample(
        self,
        features_df: pd.DataFrame,
        sample_size: int = 5000
    ) -> bool:
        """
        ä½¿ç”¨å°æ‰¹é‡è³‡æ–™æ¸¬è©¦éšæ®µ4ç‰¹å¾µå·¥ç¨‹ã€‚
        
        Args:
            features_df: éšæ®µ3ç‰¹å¾µ DataFrame
            sample_size: æ¸¬è©¦æ¨£æœ¬å¤§å°ï¼Œé è¨­ 5000 ç­†
        
        Returns:
            True å¦‚æœæ¸¬è©¦æˆåŠŸï¼ŒFalse å¦‚æœå¤±æ•—
        """
        print(f"   ğŸ§ª ä½¿ç”¨ {sample_size:,} ç­†æ¨£æœ¬æ¸¬è©¦éšæ®µ4ç‰¹å¾µå·¥ç¨‹...")
        
        try:
            # æŠ½å–æ¨£æœ¬
            if len(features_df) > sample_size:
                test_df = features_df.sample(n=sample_size, random_state=42).copy()
            else:
                test_df = features_df.copy()
                print(f"   âš ï¸  è³‡æ–™é‡ ({len(features_df):,} ç­†) å°æ–¼æ¸¬è©¦æ¨£æœ¬å¤§å°ï¼Œä½¿ç”¨å…¨éƒ¨è³‡æ–™æ¸¬è©¦")
            
            # åŸ·è¡Œéšæ®µ4ç‰¹å¾µå·¥ç¨‹
            from src.feature_engineer import _extract_bidirectional_pair_features_spark
            test_result = _extract_bidirectional_pair_features_spark(test_df)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰éšæ®µ4ç‰¹å¾µ
            stage4_features = [
                'bidirectional_flow_count',
                'bidirectional_total_bytes',
                'bidirectional_symmetry'
            ]
            has_stage4_features = any(col in test_result.columns for col in stage4_features)
            
            if has_stage4_features:
                print(f"   âœ… å°æ‰¹é‡æ¸¬è©¦æˆåŠŸï¼šéšæ®µ4ç‰¹å¾µå·¥ç¨‹æ­£å¸¸é‹ä½œ")
                print(f"   ğŸ“Š æ¸¬è©¦æ¨£æœ¬ç”¢ç”Ÿ {len(test_result.columns)} å€‹ç‰¹å¾µï¼ˆåŸå§‹ {len(test_df.columns)} å€‹ï¼‰")
                return True
            else:
                print(f"   âŒ å°æ‰¹é‡æ¸¬è©¦å¤±æ•—ï¼šæœªç”¢ç”Ÿéšæ®µ4ç‰¹å¾µ")
                print(f"   ğŸ“Š æ¸¬è©¦çµæœç‰¹å¾µæ•¸ï¼š{len(test_result.columns)} å€‹ï¼ˆé æœŸæ‡‰å¢åŠ éšæ®µ4ç‰¹å¾µï¼‰")
                return False
                
        except Exception as e:
            print(f"   âŒ å°æ‰¹é‡æ¸¬è©¦å¤±æ•—ï¼š{e}")
            import traceback
            traceback.print_exc()
            return False
    
    def process(
        self,
        cleaned_df: pd.DataFrame,
        save_features: bool = True,
        save_transformed: bool = True,
        project_root: Optional[Path] = None,
        incremental: bool = False
    ) -> Tuple[pd.DataFrame, pd.DataFrame, Any, List[str]]:
        """
        å®Œæ•´çš„ç‰¹å¾µè™•ç†æµç¨‹ï¼šæå– -> é¸æ“‡ -> è½‰æ›ã€‚
        
        Args:
            cleaned_df: æ¸…æ´—å¾Œçš„ DataFrame
            save_features: æ˜¯å¦å„²å­˜ç‰¹å¾µå·¥ç¨‹çµæœ
            save_transformed: æ˜¯å¦å„²å­˜è½‰æ›å¾Œçš„ç‰¹å¾µ
            project_root: å°ˆæ¡ˆæ ¹ç›®éŒ„
            incremental: å¦‚æœç‚º Trueï¼Œä¸” time_feature_stage=4ï¼Œå‰‡å…ˆè¼‰å…¥éšæ®µ3ç‰¹å¾µï¼Œå†åŸ·è¡Œéšæ®µ4
        
        Returns:
            (åŸå§‹ç‰¹å¾µ DataFrame, è½‰æ›å¾Œçš„ç‰¹å¾µ DataFrame, scaler ç‰©ä»¶, è½‰æ›çš„ç‰¹å¾µæ¬„ä½åˆ—è¡¨)
        """
        # 1. ç‰¹å¾µæå–
        actual_stage = self.time_feature_stage  # å¯¦éš›ä½¿ç”¨çš„éšæ®µ
        
        if incremental and self.time_feature_stage == 4:
            # å¢é‡æ¨¡å¼ï¼šå…ˆè¼‰å…¥éšæ®µ3ç‰¹å¾µ
            stage3_path = (project_root or get_project_root()) / "data" / "processed" / "features_stage3.parquet"
            if stage3_path.exists():
                print("   ğŸ“‚ è¼‰å…¥éšæ®µ3ç‰¹å¾µä½œç‚ºåŸºç¤...")
                features_df_before = self.load_features(stage=3, project_root=project_root)
                
                # å…ˆé€²è¡Œå°æ‰¹é‡æ¸¬è©¦ï¼ˆé¿å…é•·æ™‚é–“åŸ·è¡Œå¾Œæ‰ç™¼ç¾éŒ¯èª¤ï¼‰
                print("   ğŸ§ª å…ˆé€²è¡Œå°æ‰¹é‡æ¸¬è©¦ï¼ˆé¿å…é•·æ™‚é–“åŸ·è¡Œå¾Œæ‰ç™¼ç¾éŒ¯èª¤ï¼‰...")
                test_success = self._test_stage4_with_sample(features_df_before, sample_size=5000)
                
                if not test_success:
                    print("   âš ï¸  å°æ‰¹é‡æ¸¬è©¦å¤±æ•—ï¼Œè·³ééšæ®µ4ç‰¹å¾µå·¥ç¨‹ï¼Œä½¿ç”¨éšæ®µ3ç‰¹å¾µç¹¼çºŒ...")
                    features_df = features_df_before
                    actual_stage = 3
                    save_features_current = False
                else:
                    # æ¸¬è©¦æˆåŠŸï¼ŒåŸ·è¡Œå®Œæ•´éšæ®µ4
                    print("   ğŸ”„ å°æ‰¹é‡æ¸¬è©¦é€šéï¼ŒåŸ·è¡Œå®Œæ•´éšæ®µ4ç‰¹å¾µå·¥ç¨‹ï¼ˆPySparkï¼‰...")
                    print("   â±ï¸  é è¨ˆéœ€è¦ 30-60 åˆ†é˜ï¼Œè«‹è€å¿ƒç­‰å¾…...")
                    from src.feature_engineer import _extract_bidirectional_pair_features_spark
                    import numpy as np
                    features_df = _extract_bidirectional_pair_features_spark(features_df_before)
                    
                    # è¨ˆç®— bidirectional_window_flow_ratioï¼ˆä½¿ç”¨éšæ®µå››å·²èšåˆçš„è³‡æ–™ï¼‰
                    if ('bidirectional_total_src_bytes' in features_df.columns and 
                        'bidirectional_total_dst_bytes' in features_df.columns):
                        if 'bidirectional_window_flow_ratio' not in features_df.columns:
                            features_df['bidirectional_window_flow_ratio'] = (
                                features_df['bidirectional_total_src_bytes'].astype(float) / 
                                (features_df['bidirectional_total_dst_bytes'].astype(float) + 1)
                            ).fillna(0.0).replace([np.inf, -np.inf], 0.0)
                    
                    # æª¢æ¸¬éšæ®µ4æ˜¯å¦æˆåŠŸï¼ˆæª¢æŸ¥æ˜¯å¦æœ‰éšæ®µ4ç‰¹å¾µï¼‰
                    stage4_features = [
                        'bidirectional_flow_count',
                        'bidirectional_total_bytes',
                        'bidirectional_symmetry'
                    ]
                    stage4_success = any(col in features_df.columns for col in stage4_features)
                    
                    if not stage4_success:
                        print("   âš ï¸  éšæ®µ4ç‰¹å¾µå·¥ç¨‹å¤±æ•—ï¼Œä½¿ç”¨éšæ®µ3ç‰¹å¾µç¹¼çºŒ...")
                        features_df = features_df_before
                        actual_stage = 3  # ä½¿ç”¨éšæ®µ3é€²è¡Œå¾ŒçºŒè™•ç†
                        # éšæ®µ4å¤±æ•—æ™‚ï¼Œä¸å„²å­˜ç‰¹å¾µæª”æ¡ˆï¼ˆå› ç‚ºæ²’æœ‰æ–°çš„ç‰¹å¾µï¼Œé¿å…è¦†è“‹éšæ®µ3æª”æ¡ˆï¼‰
                        save_features_current = False
                    else:
                        print("   âœ… éšæ®µ4ç‰¹å¾µå·¥ç¨‹æˆåŠŸ")
                        actual_stage = 4
                        save_features_current = save_features
            else:
                # å¦‚æœæ²’æœ‰éšæ®µ3ï¼ŒåŸ·è¡Œå®Œæ•´æµç¨‹
                print("   âš ï¸  æœªæ‰¾åˆ°éšæ®µ3ç‰¹å¾µï¼ŒåŸ·è¡Œå®Œæ•´æµç¨‹...")
                features_df = self.extract(cleaned_df)
                save_features_current = save_features
        else:
            features_df = self.extract(cleaned_df)
            save_features_current = save_features
        
        if save_features_current:
            self.save_features(features_df, project_root=project_root, stage=actual_stage)
        
        # 2. ç‰¹å¾µé¸æ“‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.use_feature_selection:
            X = prepare_feature_set(
                features_df,
                include_base_features=True,
                include_time_features=True,
                time_feature_stage=actual_stage  # ä½¿ç”¨å¯¦éš›éšæ®µ
            )
            
            selector = FeatureSelector(
                remove_constant=True,
                remove_low_variance=True,
                remove_high_correlation=True,
                remove_inf=True,
                remove_high_missing=True,
                correlation_threshold=0.98
            )
            X, _ = selector.select_features(
                X,
                features_df=features_df,
                strategies=self.feature_selection_strategies,
                verbose=False
            )
            feature_columns = list(X.columns)
        else:
            feature_columns = None
        
        # 3. ç‰¹å¾µè½‰æ›
        transformed_df, scaler, transformed_columns = self.transform(
            features_df,
            feature_columns=feature_columns
        )
        
        if save_transformed:
            self.save_transformed_features(
                transformed_df,
                scaler,
                transformed_columns,
                project_root=project_root
            )
        
        return features_df, transformed_df, scaler, transformed_columns


# å·¥å» å‡½æ•¸ï¼ˆå¯é¸ï¼Œæœªä¾†å¯ä»¥æ“´å±•ç‚º Factory Patternï¼‰
def create_feature_processor(
    processor_type: str = "standard",
    **kwargs
) -> BaseFeatureProcessor:
    """
    å‰µå»ºç‰¹å¾µè™•ç†å™¨ï¼ˆå·¥å» å‡½æ•¸ï¼‰ã€‚
    
    >>> processor = create_feature_processor("standard")
    >>> isinstance(processor, StandardFeatureProcessor)
    True
    
    Args:
        processor_type: è™•ç†å™¨é¡å‹ï¼Œç›®å‰åƒ…æ”¯æ´ "standard"
        **kwargs: å‚³éçµ¦è™•ç†å™¨çš„é¡å¤–åƒæ•¸
    
    Returns:
        ç‰¹å¾µè™•ç†å™¨å¯¦ä¾‹
    """
    if processor_type == "standard":
        return StandardFeatureProcessor(**kwargs)
    else:
        raise ValueError(f"æœªçŸ¥çš„è™•ç†å™¨é¡å‹: {processor_type}")


if __name__ == '__main__':
    # ç°¡å–®æ¸¬è©¦
    import doctest
    doctest.testmod(verbose=True)

