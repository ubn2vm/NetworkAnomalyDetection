"""
ç‰¹å¾µé¸æ“‡æ¨¡çµ„

è² è²¬çµ±ä¸€ç®¡ç†ç‰¹å¾µé¸æ“‡é‚è¼¯ï¼Œéµå¾ªå–®ä¸€è·è²¬åŸå‰‡ã€‚
æä¾›å¤šç¨®ç‰¹å¾µé¸æ“‡ç­–ç•¥ï¼ˆå“è³ªæª¢æŸ¥ã€ç›¸é—œæ€§åˆ†æã€é‡è¦æ€§é¸æ“‡ç­‰ï¼‰ã€‚

ä½¿ç”¨ç¯„ä¾‹ï¼š
    >>> import pandas as pd
    >>> from src.feature_selector import FeatureSelector, prepare_feature_set
    >>> 
    >>> # æº–å‚™ç‰¹å¾µ
    >>> X = prepare_feature_set(features_df, include_time_features=True)
    >>> 
    >>> # åŸ·è¡Œç‰¹å¾µé¸æ“‡
    >>> selector = FeatureSelector()
    >>> X_selected, removed = selector.select_features(X, verbose=True)
"""

import pandas as pd
import numpy as np
from typing import List, Optional, Dict, Tuple
from enum import Enum

# åŸºç¤çµ±è¨ˆç‰¹å¾µï¼ˆçµ±ä¸€ç®¡ç†ï¼‰
BASE_STATISTICAL_FEATURES = [
    'Dur',
    'TotBytes', 
    'TotPkts',
    'SrcBytes'
]


class FeatureSelectionStrategy(Enum):
    """ç‰¹å¾µé¸æ“‡ç­–ç•¥"""
    QUALITY_CHECK = "quality_check"  # å“è³ªæª¢æŸ¥
    CORRELATION = "correlation"       # ç›¸é—œæ€§åˆ†æ
    IMPORTANCE = "importance"         # åŸºæ–¼é‡è¦æ€§
    ALL = "all"                       # å…¨éƒ¨ç­–ç•¥


class FeatureSelector:
    """
    ç‰¹å¾µé¸æ“‡å™¨
    
    ä½¿ç”¨ Strategy Pattern æ”¯æ´å¤šç¨®ç‰¹å¾µé¸æ“‡ç­–ç•¥ã€‚
    éµå¾ªå–®ä¸€è·è²¬åŸå‰‡ï¼Œå°ˆé–€è² è²¬ç‰¹å¾µé¸æ“‡é‚è¼¯ã€‚
    
    >>> import pandas as pd
    >>> import numpy as np
    >>> 
    >>> # å‰µå»ºæ¸¬è©¦è³‡æ–™
    >>> X = pd.DataFrame({
    ...     'feature1': [1, 2, 3, 4, 5],
    ...     'feature2': [1, 1, 1, 1, 1],  # å¸¸æ•¸ç‰¹å¾µ
    ...     'feature3': [10, 20, 30, 40, 50]
    ... })
    >>> 
    >>> selector = FeatureSelector(remove_constant=True)
    >>> X_selected, removed = selector.select_features(X, verbose=False)
    >>> len(X_selected.columns)
    2
    >>> 'feature2' in removed['constant']
    True
    """
    
    def __init__(
        self,
        remove_constant: bool = True,
        remove_low_variance: bool = True,
        variance_threshold: float = 1e-6,
        remove_inf: bool = True,
        inf_ratio_threshold: float = 0.1,
        remove_high_missing: bool = True,
        missing_ratio_threshold: float = 0.5,
        remove_high_correlation: bool = True,
        correlation_threshold: float = 0.98
    ):
        """
        åˆå§‹åŒ–ç‰¹å¾µé¸æ“‡å™¨
        
        Args:
            remove_constant: æ˜¯å¦ç§»é™¤å¸¸æ•¸ç‰¹å¾µ
            remove_low_variance: æ˜¯å¦ç§»é™¤ä½è®Šç•°æ•¸ç‰¹å¾µ
            variance_threshold: è®Šç•°æ•¸é–¾å€¼
            remove_inf: æ˜¯å¦ç§»é™¤ç„¡é™å€¼æ¯”ä¾‹éé«˜çš„ç‰¹å¾µ
            inf_ratio_threshold: ç„¡é™å€¼æ¯”ä¾‹é–¾å€¼
            remove_high_missing: æ˜¯å¦ç§»é™¤é«˜ç¼ºå¤±å€¼ç‰¹å¾µ
            missing_ratio_threshold: ç¼ºå¤±å€¼æ¯”ä¾‹é–¾å€¼
            remove_high_correlation: æ˜¯å¦ç§»é™¤é«˜åº¦ç›¸é—œçš„ç‰¹å¾µ
            correlation_threshold: ç›¸é—œæ€§é–¾å€¼
        """
        self.remove_constant = remove_constant
        self.remove_low_variance = remove_low_variance
        self.variance_threshold = variance_threshold
        self.remove_inf = remove_inf
        self.inf_ratio_threshold = inf_ratio_threshold
        self.remove_high_missing = remove_high_missing
        self.missing_ratio_threshold = missing_ratio_threshold
        self.remove_high_correlation = remove_high_correlation
        self.correlation_threshold = correlation_threshold
    
    def select_features(
        self,
        X: pd.DataFrame,
        features_df: Optional[pd.DataFrame] = None,
        strategies: Optional[List[FeatureSelectionStrategy]] = None,
        verbose: bool = False
    ) -> Tuple[pd.DataFrame, Dict[str, List[str]]]:
        """
        åŸ·è¡Œç‰¹å¾µé¸æ“‡
        
        Args:
            X: ç‰¹å¾µ DataFrame
            features_df: åŒ…å«æ¨™ç±¤çš„å®Œæ•´ DataFrameï¼ˆå¯é¸ï¼Œç”¨æ–¼é‡è¦æ€§é¸æ“‡ï¼‰
            strategies: é¸æ“‡ç­–ç•¥åˆ—è¡¨ï¼Œé è¨­ç‚ºå…¨éƒ¨
            verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°è³‡è¨Š
        
        Returns:
            (é¸æ“‡å¾Œçš„ç‰¹å¾µ DataFrame, ç§»é™¤çš„ç‰¹å¾µå­—å…¸)
        """
        if strategies is None:
            strategies = [FeatureSelectionStrategy.ALL]
        
        removed_features = {
            'constant': [],
            'low_variance': [],
            'inf': [],
            'high_missing': [],
            'high_correlation': [],
            'low_importance': []
        }
        
        X_selected = X.copy()
        initial_count = len(X_selected.columns)
        
        # ç­–ç•¥1ï¼šå“è³ªæª¢æŸ¥
        if (FeatureSelectionStrategy.QUALITY_CHECK in strategies or 
            FeatureSelectionStrategy.ALL in strategies):
            X_selected, removed = self._quality_check(X_selected, verbose)
            removed_features.update(removed)
        
        # ç­–ç•¥2ï¼šç›¸é—œæ€§åˆ†æ
        if (FeatureSelectionStrategy.CORRELATION in strategies or 
            FeatureSelectionStrategy.ALL in strategies):
            X_selected, removed = self._correlation_analysis(X_selected, verbose)
            removed_features['high_correlation'] = removed
        
        # ç­–ç•¥3ï¼šé‡è¦æ€§é¸æ“‡ï¼ˆå¦‚æœæœ‰æ¨™ç±¤ï¼‰
        if (FeatureSelectionStrategy.IMPORTANCE in strategies or 
            FeatureSelectionStrategy.ALL in strategies):
            if features_df is not None and 'Label' in features_df.columns:
                X_selected, removed = self._importance_selection(
                    X_selected, features_df, verbose
                )
                removed_features['low_importance'] = removed
        
        final_count = len(X_selected.columns)
        if verbose:
            print(f"âœ… ç‰¹å¾µé¸æ“‡å®Œæˆï¼š{initial_count} â†’ {final_count} å€‹ç‰¹å¾µ")
            total_removed = sum(len(v) for v in removed_features.values())
            if total_removed > 0:
                print(f"   ç§»é™¤ï¼š{total_removed} å€‹ç‰¹å¾µ")
                for key, features in removed_features.items():
                    if features:
                        print(f"     - {key}: {len(features)} å€‹")
        
        return X_selected, removed_features
    
    def _quality_check(
        self, 
        X: pd.DataFrame, 
        verbose: bool = False
    ) -> Tuple[pd.DataFrame, Dict[str, List[str]]]:
        """å“è³ªæª¢æŸ¥ï¼šç§»é™¤ä½å“è³ªç‰¹å¾µ"""
        removed = {
            'constant': [],
            'low_variance': [],
            'inf': [],
            'high_missing': []
        }
        
        # æª¢æŸ¥å¸¸æ•¸ç‰¹å¾µ
        if self.remove_constant:
            constant_features = [
                col for col in X.columns 
                if X[col].nunique() <= 1
            ]
            removed['constant'] = constant_features
            if constant_features and verbose:
                print(f"   ç™¼ç¾ {len(constant_features)} å€‹å¸¸æ•¸ç‰¹å¾µï¼š{constant_features[:5]}{'...' if len(constant_features) > 5 else ''}")
        
        # æª¢æŸ¥ä½è®Šç•°æ•¸ç‰¹å¾µ
        if self.remove_low_variance:
            low_variance_features = []
            for col in X.select_dtypes(include=[np.number]).columns:
                if col in X.columns:
                    var_value = X[col].var()
                    if pd.notna(var_value) and var_value < self.variance_threshold:
                        low_variance_features.append(col)
            removed['low_variance'] = low_variance_features
            if low_variance_features and verbose:
                print(f"   ç™¼ç¾ {len(low_variance_features)} å€‹ä½è®Šç•°æ•¸ç‰¹å¾µ")
        
        # æª¢æŸ¥ç„¡é™å€¼ç‰¹å¾µ
        if self.remove_inf:
            inf_features = []
            for col in X.select_dtypes(include=[np.number]).columns:
                if col in X.columns:
                    inf_count = np.isinf(X[col]).sum()
                    inf_ratio = inf_count / len(X) if len(X) > 0 else 0
                    if inf_ratio > self.inf_ratio_threshold:
                        inf_features.append(col)
            removed['inf'] = inf_features
            if inf_features and verbose:
                print(f"   ç™¼ç¾ {len(inf_features)} å€‹ç„¡é™å€¼ç‰¹å¾µï¼ˆæ¯”ä¾‹ > {self.inf_ratio_threshold*100:.1f}%ï¼‰")
        
        # æª¢æŸ¥é«˜ç¼ºå¤±å€¼ç‰¹å¾µ
        if self.remove_high_missing:
            missing_features = []
            for col in X.columns:
                missing_count = X[col].isna().sum()
                missing_ratio = missing_count / len(X) if len(X) > 0 else 0
                if missing_ratio > self.missing_ratio_threshold:
                    missing_features.append(col)
            removed['high_missing'] = missing_features
            if missing_features and verbose:
                print(f"   ç™¼ç¾ {len(missing_features)} å€‹é«˜ç¼ºå¤±å€¼ç‰¹å¾µï¼ˆæ¯”ä¾‹ > {self.missing_ratio_threshold*100:.1f}%ï¼‰")
        
        # åˆä½µæ‰€æœ‰è¦ç§»é™¤çš„ç‰¹å¾µ
        all_removed = set()
        for feature_list in removed.values():
            all_removed.update(feature_list)
        
        if all_removed and verbose:
            print(f"   âœ… ç§»é™¤ {len(all_removed)} å€‹ä½å“è³ªç‰¹å¾µ")
        
        X_cleaned = X[[col for col in X.columns if col not in all_removed]]
        return X_cleaned, removed
    
    def _correlation_analysis(
        self, 
        X: pd.DataFrame, 
        verbose: bool = False
    ) -> Tuple[pd.DataFrame, List[str]]:
        """ç›¸é—œæ€§åˆ†æï¼šç§»é™¤é«˜åº¦ç›¸é—œçš„ç‰¹å¾µ"""
        if not self.remove_high_correlation or len(X.columns) <= 1:
            return X, []
        
        # æ¸…ç†è³‡æ–™ä»¥ä¾¿è¨ˆç®—ç›¸é—œæ€§
        X_clean = X.copy()
        numeric_cols = X_clean.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            X_clean.loc[:, numeric_cols] = X_clean.loc[:, numeric_cols].replace(
                [np.inf, -np.inf], np.nan
            )
            median_values = X_clean.loc[:, numeric_cols].median()
            X_clean.loc[:, numeric_cols] = X_clean.loc[:, numeric_cols].fillna(
                median_values.fillna(0)
            )
        
        # è¨ˆç®—ç›¸é—œæ€§
        numeric_cols = X_clean.select_dtypes(include=[np.number]).columns.tolist()
        if len(numeric_cols) <= 1:
            return X, []
        
        corr_matrix = X_clean[numeric_cols].corr().abs()
        features_to_remove = set()
        high_corr_pairs = []
        
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                col_i = corr_matrix.columns[i]
                col_j = corr_matrix.columns[j]
                corr_value = corr_matrix.iloc[i, j]
                
                if pd.notna(corr_value) and corr_value > self.correlation_threshold:
                    high_corr_pairs.append((col_i, col_j, corr_value))
                    features_to_remove.add(col_j)  # ä¿ç•™ç¬¬ä¸€å€‹ï¼Œç§»é™¤ç¬¬äºŒå€‹
        
        removed = list(features_to_remove)
        if removed and verbose:
            print(f"   ç™¼ç¾ {len(high_corr_pairs)} å°é«˜åº¦ç›¸é—œçš„ç‰¹å¾µï¼ˆç›¸é—œæ€§ > {self.correlation_threshold:.2f}ï¼‰")
            if len(high_corr_pairs) <= 10:
                for pair in high_corr_pairs:
                    print(f"     {pair[0]} <-> {pair[1]}: {pair[2]:.4f}")
            else:
                for pair in high_corr_pairs[:10]:
                    print(f"     {pair[0]} <-> {pair[1]}: {pair[2]:.4f}")
                print(f"     ... é‚„æœ‰ {len(high_corr_pairs) - 10} å°")
            print(f"   âœ… ç§»é™¤ {len(removed)} å€‹å†—é¤˜ç‰¹å¾µ")
        
        X_cleaned = X[[col for col in X.columns if col not in features_to_remove]]
        return X_cleaned, removed
    
    def _importance_selection(
        self,
        X: pd.DataFrame,
        features_df: pd.DataFrame,
        verbose: bool = False,
        min_features: int = 15,
        max_features: int = 25,
        importance_threshold: float = 0.98
    ) -> Tuple[pd.DataFrame, List[str]]:
        """
        åŸºæ–¼ XGBoost ç‰¹å¾µé‡è¦æ€§é¸æ“‡
        
        Args:
            X: ç‰¹å¾µ DataFrame
            features_df: åŒ…å«æ¨™ç±¤çš„å®Œæ•´ DataFrame
            verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°è³‡è¨Š
            min_features: æœ€å°‘ä¿ç•™ç‰¹å¾µæ•¸
            max_features: æœ€å¤šä¿ç•™ç‰¹å¾µæ•¸
            importance_threshold: ç´¯ç©é‡è¦æ€§é–¾å€¼
        
        Returns:
            (é¸æ“‡å¾Œçš„ç‰¹å¾µ DataFrame, ç§»é™¤çš„ç‰¹å¾µåˆ—è¡¨)
        """
        try:
            from src.models import ModelFactory, ModelType
            from src.label_processor import convert_label_to_binary
            
            # è½‰æ›æ¨™ç±¤
            if 'label_binary' not in features_df.columns:
                features_df = convert_label_to_binary(features_df, verbose=False)
            y = features_df['label_binary']
            
            # ç¢ºä¿ç´¢å¼•å°é½Š
            if len(X) != len(y):
                common_idx = X.index.intersection(y.index)
                X = X.loc[common_idx]
                y = y.loc[common_idx]
            
            # ä½¿ç”¨å°æ¨£æœ¬å¿«é€Ÿè¨“ç·´
            sample_size = min(100000, len(X))
            if sample_size < len(X):
                # ä½¿ç”¨ RandomState ç¢ºä¿å¯é‡ç¾æ€§ï¼ˆå…¼å®¹èˆŠç‰ˆ NumPyï¼‰
                rng = np.random.RandomState(42)
                sample_idx = rng.choice(len(X), sample_size, replace=False)
                X_sample = X.iloc[sample_idx]
                y_sample = y.iloc[sample_idx]
            else:
                X_sample = X
                y_sample = y
            
            if verbose:
                print(f"   ä½¿ç”¨ {len(X_sample):,} ç­†æ¨£æœ¬å¿«é€Ÿè¨“ç·´ XGBoost...")
            
            # è¨“ç·´æ¨¡å‹ç²å–é‡è¦æ€§
            xgb_model = ModelFactory.create(ModelType.XGBOOST)
            xgb_model.train(
                X_sample, y_sample,
                test_size=0.2, random_state=42,
                n_estimators=50, max_depth=4, learning_rate=0.1,
                verbose=False
            )
            
            feature_importance = xgb_model.get_feature_importance()
            sorted_importance = sorted(
                feature_importance.items(), 
                key=lambda x: x[1], 
                reverse=True
            )
            
            # é¸æ“‡ç´¯ç©é‡è¦æ€§é”åˆ°é–¾å€¼çš„ç‰¹å¾µ
            total_importance = sum(imp for _, imp in sorted_importance)
            cumulative_importance = 0
            important_features = []
            
            for feature, importance in sorted_importance:
                cumulative_importance += importance
                important_features.append(feature)
                if ((cumulative_importance / total_importance >= importance_threshold 
                     and len(important_features) >= min_features) or 
                    len(important_features) >= max_features):
                    break
            
            if len(important_features) < min_features:
                important_features = [f[0] for f in sorted_importance[:min_features]]
                cumulative_importance = sum(imp for _, imp in sorted_importance[:min_features])
            
            removed = [col for col in X.columns if col not in important_features]
            
            if verbose:
                print(f"   Top {len(important_features)} æœ€é‡è¦ç‰¹å¾µï¼ˆç´¯ç©é‡è¦æ€§ {cumulative_importance/total_importance*100:.1f}%ï¼‰ï¼š")
                for i, (feature, importance) in enumerate(sorted_importance[:len(important_features)], 1):
                    print(f"     {i:2d}. {feature:30s}: {importance:.4f} ({importance/total_importance*100:.2f}%)")
                print(f"   âœ… åŸºæ–¼ç‰¹å¾µé‡è¦æ€§ï¼Œå¾ {len(X.columns)} å€‹ç‰¹å¾µæ¸›å°‘åˆ° {len(important_features)} å€‹ç‰¹å¾µ")
            
            X_selected = X[[col for col in X.columns if col in important_features]]
            return X_selected, removed
            
        except Exception as e:
            if verbose:
                print(f"   âš ï¸  ç‰¹å¾µé‡è¦æ€§åˆ†æå¤±æ•—ï¼š{e}")
            return X, []
    
    def check_time_feature_bias(
        self,
        X: pd.DataFrame,
        features_df: pd.DataFrame,
        time_features: Optional[List[str]] = None,
        importance_threshold: float = 0.05,
        sample_size: int = 10000,
        verbose: bool = False
    ) -> Tuple[pd.DataFrame, Dict[str, float]]:
        """
        æª¢æŸ¥æ™‚é–“ç‰¹å¾µæ˜¯å¦éæ–¼é‡è¦ï¼ˆé¿å…æ™‚é–“åå·®ï¼‰
        
        æ”»æ“Šå¯èƒ½ç™¼ç”Ÿåœ¨ä»»ä½•æ™‚é–“ï¼Œéåº¦ä¾è³´æ™‚é–“ç‰¹å¾µå¯èƒ½å°è‡´ï¼š
        - åœ¨ç‰¹å®šæ™‚é–“æ®µèª¤å ±ç‡è¼ƒé«˜
        - åœ¨ç‰¹å®šæ™‚é–“æ®µæ¼å ±ç‡è¼ƒé«˜
        - æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸‹é™
        
        >>> import pandas as pd
        >>> import numpy as np
        >>> from src.label_processor import convert_label_to_binary
        >>> 
        >>> # å‰µå»ºæ¸¬è©¦è³‡æ–™
        >>> features_df = pd.DataFrame({
        ...     'hour': [1, 2, 3, 4, 5],
        ...     'cos_hour': [0.5, 0.6, 0.7, 0.8, 0.9],
        ...     'Label': ['Normal', 'Botnet', 'Normal', 'Botnet', 'Normal']
        ... })
        >>> X = pd.DataFrame({
        ...     'hour': [1, 2, 3, 4, 5],
        ...     'cos_hour': [0.5, 0.6, 0.7, 0.8, 0.9],
        ...     'feature1': [10, 20, 30, 40, 50]
        ... })
        >>> 
        >>> selector = FeatureSelector()
        >>> X_checked, importance_dict = selector.check_time_feature_bias(
        ...     X, features_df, verbose=False
        ... )
        >>> isinstance(X_checked, pd.DataFrame)
        True
        
        Args:
            X: ç‰¹å¾µ DataFrame
            features_df: åŒ…å«æ¨™ç±¤çš„å®Œæ•´ DataFrame
            time_features: è¦æª¢æŸ¥çš„æ™‚é–“ç‰¹å¾µåˆ—è¡¨ã€‚å¦‚æœç‚º Noneï¼Œå‰‡ä½¿ç”¨é è¨­åˆ—è¡¨ï¼š
                ['hour', 'cos_hour', 'sin_hour']
            importance_threshold: æ™‚é–“ç‰¹å¾µç¸½é‡è¦æ€§é–¾å€¼ï¼ˆé è¨­ 0.05ï¼Œå³ 5%ï¼‰
            sample_size: ç”¨æ–¼å¿«é€Ÿæª¢æŸ¥çš„æ¨£æœ¬æ•¸ï¼ˆé è¨­ 10000ï¼‰
            verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°è³‡è¨Š
        
        Returns:
            (è™•ç†å¾Œçš„ç‰¹å¾µ DataFrame, æ™‚é–“ç‰¹å¾µé‡è¦æ€§å­—å…¸)
        """
        if 'Label' not in features_df.columns:
            if verbose:
                print("   âš ï¸  ç„¡æ¨™ç±¤ï¼Œç„¡æ³•æª¢æŸ¥æ™‚é–“ç‰¹å¾µé‡è¦æ€§ï¼Œä¿ç•™æ‰€æœ‰æ™‚é–“ç‰¹å¾µ")
            return X, {}
        
        # é è¨­æ™‚é–“ç‰¹å¾µåˆ—è¡¨
        if time_features is None:
            time_features = ['hour', 'cos_hour', 'sin_hour']
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ™‚é–“ç‰¹å¾µå­˜åœ¨
        existing_time_features = [f for f in time_features if f in X.columns]
        if not existing_time_features:
            if verbose:
                print("   âš ï¸  æœªæ‰¾åˆ°æ™‚é–“ç‰¹å¾µï¼Œè·³éæ™‚é–“åå·®æª¢æŸ¥")
            return X, {}
        
        try:
            from src.models import ModelFactory, ModelType
            from src.label_processor import convert_label_to_binary
            
            # æº–å‚™æ¨™ç±¤
            if 'label_binary' not in features_df.columns:
                features_df_temp = convert_label_to_binary(features_df, verbose=False)
            else:
                features_df_temp = features_df.copy()
            y_temp = features_df_temp['label_binary']
            
            # ç¢ºä¿ç´¢å¼•å°é½Š
            common_idx = X.index.intersection(y_temp.index)
            if len(common_idx) == 0:
                if verbose:
                    print("   âš ï¸  ç´¢å¼•ä¸åŒ¹é…ï¼Œè·³éæ™‚é–“åå·®æª¢æŸ¥")
                return X, {}
            
            X_temp = X.loc[common_idx].copy()
            y_temp = y_temp.loc[common_idx]
            
            # ä½¿ç”¨å°æ¨£æœ¬å¿«é€Ÿæª¢æŸ¥æ™‚é–“ç‰¹å¾µé‡è¦æ€§
            actual_sample_size = min(sample_size, len(X_temp))
            if actual_sample_size < len(X_temp):
                # ä½¿ç”¨ RandomState ç¢ºä¿å¯é‡ç¾æ€§ï¼ˆå…¼å®¹èˆŠç‰ˆ NumPyï¼‰
                rng = np.random.RandomState(42)
                sample_idx = rng.choice(len(X_temp), actual_sample_size, replace=False)
                X_sample = X_temp.iloc[sample_idx]
                y_sample = y_temp.iloc[sample_idx]
            else:
                X_sample = X_temp
                y_sample = y_temp
            
            if verbose:
                print(f"   ä½¿ç”¨ {len(X_sample):,} ç­†æ¨£æœ¬å¿«é€Ÿæª¢æŸ¥æ™‚é–“ç‰¹å¾µé‡è¦æ€§...")
            
            # è¨“ç·´æ¨¡å‹ç²å–é‡è¦æ€§
            time_check_model = ModelFactory.create(ModelType.XGBOOST)
            
            # è¨ˆç®—ä¸å¹³è¡¡æ¬Šé‡
            negative_count = (y_sample == 0).sum()
            positive_count = (y_sample == 1).sum()
            scale_pos_weight = negative_count / positive_count if positive_count > 0 else 1.0
            
            time_check_model.train(
                X_sample, y_sample,
                test_size=0.2, random_state=42,
                n_estimators=50, max_depth=4, learning_rate=0.1,
                scale_pos_weight=scale_pos_weight,
                verbose=False
            )
            
            feature_importance = time_check_model.get_feature_importance()
            
            # è¨ˆç®—æ™‚é–“ç‰¹å¾µé‡è¦æ€§
            time_importance_dict = {}
            total_time_importance = 0.0
            
            for time_feat in existing_time_features:
                importance = feature_importance.get(time_feat, 0.0)
                time_importance_dict[time_feat] = importance
                total_time_importance += importance
            
            if verbose:
                print(f"   ğŸ“Š æ™‚é–“ç‰¹å¾µé‡è¦æ€§åˆ†æï¼š")
                for time_feat in existing_time_features:
                    imp = time_importance_dict.get(time_feat, 0.0)
                    print(f"     {time_feat}: {imp:.4f} ({imp*100:.2f}%)")
                print(f"     æ™‚é–“ç‰¹å¾µç¸½é‡è¦æ€§: {total_time_importance:.4f} ({total_time_importance*100:.2f}%)")
            
            # æ±ºå®šæ˜¯å¦ç§»é™¤æ™‚é–“ç‰¹å¾µ
            if total_time_importance > importance_threshold:
                if verbose:
                    print(f"\n   âš ï¸  è­¦å‘Šï¼šæ™‚é–“ç‰¹å¾µç¸½é‡è¦æ€§ ({total_time_importance:.4f}) è¶…éé–¾å€¼ ({importance_threshold:.4f})")
                    print(f"   ğŸ’¡ æ”»æ“Šå¯èƒ½ç™¼ç”Ÿåœ¨ä»»ä½•æ™‚é–“ï¼Œéåº¦ä¾è³´æ™‚é–“ç‰¹å¾µå¯èƒ½å°è‡´ï¼š")
                    print(f"      - åœ¨ç‰¹å®šæ™‚é–“æ®µèª¤å ±ç‡è¼ƒé«˜")
                    print(f"      - åœ¨ç‰¹å®šæ™‚é–“æ®µæ¼å ±ç‡è¼ƒé«˜")
                    print(f"      - æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸‹é™")
                    print(f"\n   ğŸ”§ å»ºè­°ç§»é™¤æ™‚é–“ç‰¹å¾µï¼Œåªä¿ç•™è¡Œç‚ºç‰¹å¾µ...")
                
                # ç§»é™¤æ‰€æœ‰æ™‚é–“ç›¸é—œç‰¹å¾µï¼ˆä½†ä¿ç•™æ™‚é–“çª—å£èšåˆç‰¹å¾µï¼Œå› ç‚ºå®ƒå€‘æ˜¯è¡Œç‚ºç‰¹å¾µï¼‰
                time_features_to_remove = [
                    'hour', 'day_of_week', 'day_of_month', 
                    'is_weekend', 'is_work_hour', 'is_night',
                    'sin_hour', 'cos_hour', 
                    'sin_day_of_week', 'cos_day_of_week',
                    'sin_day_of_month', 'cos_day_of_month'
                ]
                # åªç§»é™¤å¯¦éš›å­˜åœ¨çš„ç‰¹å¾µ
                time_features_to_remove = [f for f in time_features_to_remove if f in X.columns]
                
                if time_features_to_remove:
                    X = X[[col for col in X.columns if col not in time_features_to_remove]]
                    if verbose:
                        print(f"   âœ… å·²ç§»é™¤ {len(time_features_to_remove)} å€‹æ™‚é–“ç‰¹å¾µï¼š{time_features_to_remove}")
                        print(f"   âœ… ä¿ç•™æ™‚é–“çª—å£èšåˆç‰¹å¾µï¼ˆå¦‚ flows_per_minute_by_srcï¼‰ï¼Œå› ç‚ºå®ƒå€‘åæ˜ è¡Œç‚ºæ¨¡å¼")
            else:
                if verbose:
                    print(f"   âœ… æ™‚é–“ç‰¹å¾µé‡è¦æ€§åœ¨å¯æ¥å—ç¯„åœå…§ï¼Œä¿ç•™æ‰€æœ‰ç‰¹å¾µ")
                    print(f"   ğŸ’¡ å¦‚æœæ“”å¿ƒæ™‚é–“åå·®ï¼Œå¯ä»¥æ‰‹å‹•ç§»é™¤æ™‚é–“ç‰¹å¾µ")
            
            return X, time_importance_dict
            
        except Exception as e:
            if verbose:
                print(f"   âš ï¸  ç„¡æ³•æª¢æŸ¥æ™‚é–“ç‰¹å¾µé‡è¦æ€§ï¼š{e}")
                import traceback
                traceback.print_exc()
            return X, {}


def prepare_feature_set(
    features_df: pd.DataFrame,
    include_base_features: bool = True,
    include_time_features: bool = True,
    time_feature_stage: int = 1
) -> pd.DataFrame:
    """
    æº–å‚™å®Œæ•´çš„ç‰¹å¾µé›†åˆï¼ˆçµ±ä¸€æ¥å£ï¼‰
    
    çµåˆåŸºç¤çµ±è¨ˆç‰¹å¾µå’Œå·¥ç¨‹ç‰¹å¾µï¼Œæä¾›çµ±ä¸€çš„ç‰¹å¾µæº–å‚™æ¥å£ã€‚
    
    >>> import pandas as pd
    >>> from src.feature_engineer import extract_features
    >>> 
    >>> # å‡è¨­å·²ç¶“æœ‰ features_df
    >>> # features_df = extract_features(cleaned_df)
    >>> # X = prepare_feature_set(features_df, include_time_features=True)
    >>> # 'Dur' in X.columns  # åŸºç¤ç‰¹å¾µ
    >>> # 'flow_ratio' in X.columns  # å·¥ç¨‹ç‰¹å¾µ
    
    Args:
        features_df: ç¶“éç‰¹å¾µå·¥ç¨‹çš„ DataFrame
        include_base_features: æ˜¯å¦åŒ…å«åŸºç¤çµ±è¨ˆç‰¹å¾µ
        include_time_features: æ˜¯å¦åŒ…å«æ™‚é–“ç‰¹å¾µ
        time_feature_stage: æ™‚é–“ç‰¹å¾µéšæ®µï¼ˆ1, 2, 3, æˆ– 4ï¼‰
            - 1: åŸºæœ¬æ™‚é–“ç‰¹å¾µ
            - 2: æ™‚é–“é–“éš”ç‰¹å¾µ
            - 3: æ™‚é–“çª—å£èšåˆç‰¹å¾µï¼ˆæŒ‰ SrcAddrï¼‰
            - 4: é›™å‘æµ Pair èšåˆç‰¹å¾µï¼ˆæŒ‰ IP Pairï¼Œéœ€è¦ PySparkï¼‰
    
    Returns:
        åŒ…å«æ‰€æœ‰ç‰¹å¾µçš„ DataFrame
    """
    from src.feature_engineer import get_feature_columns
    
    feature_cols = []
    
    # åŸºç¤çµ±è¨ˆç‰¹å¾µ
    if include_base_features:
        feature_cols.extend(BASE_STATISTICAL_FEATURES)
    
    # å·¥ç¨‹ç‰¹å¾µ
    engineered_features = get_feature_columns(
        include_time_features=include_time_features,
        time_feature_stage=time_feature_stage
    )
    feature_cols.extend(engineered_features)
    
    # åªé¸æ“‡å­˜åœ¨çš„æ¬„ä½
    available_cols = [col for col in feature_cols if col in features_df.columns]
    X = features_df[available_cols].copy()
    
    return X


def get_base_statistical_features() -> List[str]:
    """
    ç²å–åŸºç¤çµ±è¨ˆç‰¹å¾µåˆ—è¡¨
    
    >>> features = get_base_statistical_features()
    >>> 'Dur' in features
    True
    >>> 'TotBytes' in features
    True
    
    Returns:
        åŸºç¤çµ±è¨ˆç‰¹å¾µåˆ—è¡¨çš„å‰¯æœ¬
    """
    return BASE_STATISTICAL_FEATURES.copy()


if __name__ == '__main__':
    # ç°¡å–®æ¸¬è©¦
    import doctest
    doctest.testmod(verbose=True)

