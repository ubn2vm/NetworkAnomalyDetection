"""
çµ±ä¸€çš„è³‡æ–™è¼‰å…¥å™¨æ¨¡çµ„

ä½¿ç”¨ Factory Pattern æ”¯æ´ä¸åŒè³‡æ–™ä¾†æºå’Œæ ¼å¼çš„è¼‰å…¥ã€‚
æä¾›é›™å‘æµå’Œ API è³‡æ–™ä¾†æºçš„çµ±ä¸€ä»‹é¢ã€‚
"""

import pandas as pd
from pathlib import Path
from typing import Optional
from abc import ABC, abstractmethod
from enum import Enum


class DataSourceType(Enum):
    """è³‡æ–™ä¾†æºé¡å‹æšèˆ‰"""
    BIDIRECTIONAL_BINETFLOW = "bidirectional_binetflow"
    BIDIRECTIONAL_BINETFLOW_SPARK = "bidirectional_binetflow_spark"
    API = "api"


def get_project_root() -> Path:
    """
    å–å¾—å°ˆæ¡ˆæ ¹ç›®éŒ„è·¯å¾‘ã€‚

    >>> root = get_project_root()
    >>> root.exists()
    True
    >>> root.name == 'NetworkAnomalyDetection'
    True

    Returns:
        å°ˆæ¡ˆæ ¹ç›®éŒ„çš„ Path ç‰©ä»¶ã€‚
    """
    current_file = Path(__file__)
    # å¾ src/data_loader.py å¾€ä¸Šå…©å±¤åˆ°å°ˆæ¡ˆæ ¹ç›®éŒ„
    return current_file.parent.parent


class BaseDataLoader(ABC):
    """è³‡æ–™è¼‰å…¥å™¨æŠ½è±¡åŸºé¡
    
    å®šç¾©æ‰€æœ‰è³‡æ–™è¼‰å…¥å™¨å¿…é ˆå¯¦ä½œçš„çµ±ä¸€ä»‹é¢ã€‚
    
    >>> from src.data_loader import BaseDataLoader
    >>> # BaseDataLoader æ˜¯æŠ½è±¡é¡åˆ¥ï¼Œä¸èƒ½ç›´æ¥å¯¦ä¾‹åŒ–
    >>> # loader = BaseDataLoader()  # é€™æœƒå¤±æ•—
    """
    
    @abstractmethod
    def load(self, file_path: Optional[Path] = None) -> pd.DataFrame:
        """
        è¼‰å…¥åŸå§‹è³‡æ–™ã€‚
        
        Args:
            file_path: è³‡æ–™æª”æ¡ˆè·¯å¾‘ã€‚å¦‚æœç‚º Noneï¼Œå‰‡ä½¿ç”¨é è¨­è·¯å¾‘ã€‚
        
        Returns:
            åŒ…å«åŸå§‹è³‡æ–™çš„ DataFrameã€‚
        
        Raises:
            FileNotFoundError: å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ã€‚
        """
        pass
    
    @abstractmethod
    def clean(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ¸…æ´—è³‡æ–™ã€‚
        
        Args:
            df: åŸå§‹è³‡æ–™ DataFrameã€‚
        
        Returns:
            æ¸…æ´—å¾Œçš„ DataFrameã€‚
        """
        pass
    
    def save_cleaned_data(
        self,
        df: pd.DataFrame,
        output_path: Optional[Path] = None,
        project_root: Optional[Path] = None
    ) -> Path:
        """
        å„²å­˜æ¸…æ´—å¾Œçš„è³‡æ–™ç‚º Parquet æ ¼å¼ã€‚

        >>> import pandas as pd
        >>> import tempfile
        >>> from pathlib import Path
        >>> loader = BidirectionalBinetflowLoader()
        >>> test_df = pd.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
        >>> with tempfile.TemporaryDirectory() as tmpdir:
        ...     output = loader.save_cleaned_data(test_df, Path(tmpdir) / "test.parquet")
        ...     assert output.exists()
        ...     loaded = pd.read_parquet(output)
        ...     len(loaded) == 3
        True

        Args:
            df: æ¸…æ´—å¾Œçš„ DataFrameã€‚
            output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ã€‚å¦‚æœç‚º Noneï¼Œå‰‡ä½¿ç”¨é è¨­è·¯å¾‘ã€‚
            project_root: å°ˆæ¡ˆæ ¹ç›®éŒ„ã€‚å¦‚æœç‚º Noneï¼Œå‰‡è‡ªå‹•åµæ¸¬ã€‚

        Returns:
            è¼¸å‡ºæª”æ¡ˆçš„è·¯å¾‘ã€‚
        """
        if project_root is None:
            project_root = get_project_root()
        
        if output_path is None:
            output_dir = project_root / "data" / "processed"
            output_dir.mkdir(parents=True, exist_ok=True)
            output_path = output_dir / "cleaned_data.parquet"
        
        df.to_parquet(
            output_path,
            engine='pyarrow',
            index=False
        )
        
        return output_path


class BidirectionalBinetflowLoader(BaseDataLoader):
    """é›™å‘æµ Binetflow è¼‰å…¥å™¨
    
    è®€å– .binetflow æ ¼å¼ï¼ˆCSVï¼‰çš„é›™å‘æµè³‡æ–™ã€‚
    
    >>> from src.data_loader import BidirectionalBinetflowLoader
    >>> loader = BidirectionalBinetflowLoader()
    >>> # df = loader.load()  # éœ€è¦å¯¦éš›æª”æ¡ˆ
    >>> # cleaned = loader.clean(df)
    """
    
    def load(self, file_path: Optional[Path] = None) -> pd.DataFrame:
        """
        å¾åŸå§‹ NetFlow æª”æ¡ˆè®€å–é›™å‘æµè³‡æ–™ï¼ˆ.binetflow æ ¼å¼ï¼‰ã€‚

        >>> from pathlib import Path
        >>> loader = BidirectionalBinetflowLoader()
        >>> # éœ€è¦å¯¦éš›æª”æ¡ˆæ‰èƒ½æ¸¬è©¦
        >>> # df = loader.load()
        >>> # assert 'StartTime' in df.columns

        Args:
            file_path: åŸå§‹è³‡æ–™æª”æ¡ˆè·¯å¾‘ã€‚å¦‚æœç‚º Noneï¼Œå‰‡ä½¿ç”¨é è¨­è·¯å¾‘ã€‚

        Returns:
            åŒ…å«åŸå§‹é›™å‘æµ NetFlow è³‡æ–™çš„ DataFrameã€‚

        Raises:
            FileNotFoundError: å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ã€‚
        """
        if file_path is None:
            project_root = get_project_root()
            file_path = project_root / "data" / "raw" / "capture20110817.binetflow"
        
        if not file_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {file_path}")
        
        # è®€å– .binetflow æ ¼å¼ï¼ˆCSV æ ¼å¼ï¼‰
        df = pd.read_csv(file_path)
        
        return df
    
    def clean(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ¸…æ´—èˆ‡è½‰æ›é›™å‘æµ NetFlow è³‡æ–™ã€‚

        >>> import pandas as pd
        >>> loader = BidirectionalBinetflowLoader()
        >>> test_df = pd.DataFrame({
        ...     'StartTime': ['2011-08-17 12:01:01.780', '2011-08-17 12:02:01.780'],
        ...     'Dur': [3.124, 5.456],
        ...     'Proto': ['TCP', 'UDP'],
        ...     'SrcAddr': ['192.168.1.1', '10.0.0.1'],
        ...     'Sport': ['80', '443'],
        ...     'DstAddr': ['172.16.0.1', '192.168.1.100'],
        ...     'Dport': ['8080', '22'],
        ...     'TotBytes': [1000, 2000],
        ...     'TotPkts': [10, 20],
        ...     'Label': ['Background', 'Botnet']
        ... })
        >>> cleaned = loader.clean(test_df)
        >>> pd.api.types.is_datetime64_any_dtype(cleaned['StartTime'])
        True
        >>> 'Dur' in cleaned.columns
        True

        Args:
            df: åŸå§‹é›™å‘æµ NetFlow DataFrameã€‚

        Returns:
            æ¸…æ´—å¾Œçš„ DataFrameï¼ŒåŒ…å«æ­£ç¢ºçš„è³‡æ–™å‹åˆ¥è½‰æ›ã€‚
        """
        df = df.copy()
        
        # è½‰æ› StartTime ç‚º datetime
        if 'StartTime' in df.columns:
            df['StartTime'] = pd.to_datetime(df['StartTime'], errors='coerce')
        
        # ç¢ºä¿æ•¸å€¼æ¬„ä½ç‚ºæ­£ç¢ºå‹åˆ¥
        numeric_cols = ['Dur', 'sTos', 'dTos', 'TotPkts', 'TotBytes', 'SrcBytes', 'DstBytes']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # ç¢ºä¿åŸ è™Ÿç‚ºæ•¸å€¼ï¼ˆä½¿ç”¨ coerce å°‡ç„¡æ³•è½‰æ›çš„å€¼è¨­ç‚º NaNï¼Œé¿å… FutureWarningï¼‰
        port_cols = ['Sport', 'Dport']
        for col in port_cols:
            if col in df.columns:
                # ä½¿ç”¨ coerce æ›¿ä»£ ignoreï¼Œç„¡æ³•è½‰æ›çš„å€¼æœƒè®Šæˆ NaN
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        return df


class APIDataLoader(BaseDataLoader):
    """API è³‡æ–™è¼‰å…¥å™¨ï¼ˆæ¨¡æ“¬ï¼‰
    
    å¾ API è¼‰å…¥è³‡æ–™ã€‚ç›®å‰ç‚ºæ¡†æ¶å¯¦ä½œï¼Œå¾…å¾ŒçºŒå®Œå–„ã€‚
    
    >>> from src.data_loader import APIDataLoader
    >>> loader = APIDataLoader()
    >>> # å¾…å¯¦ä½œ
    """
    
    def load(self, file_path: Optional[Path] = None) -> pd.DataFrame:
        """
        å¾ API è¼‰å…¥è³‡æ–™ï¼ˆæ¨¡æ“¬å¯¦ä½œï¼‰ã€‚

        >>> loader = APIDataLoader()
        >>> # å¾…å¯¦ä½œ
        >>> # df = loader.load()
        >>> # assert isinstance(df, pd.DataFrame)

        Args:
            file_path: API ç«¯é» URLï¼ˆå¯é¸ï¼‰ã€‚

        Returns:
            åŒ…å« API è³‡æ–™çš„ DataFrameã€‚

        Raises:
            NotImplementedError: API è¼‰å…¥å™¨å¾…å¯¦ä½œã€‚
        """
        # TODO: å¯¦ä½œ API è¼‰å…¥é‚è¼¯
        # ç¯„ä¾‹å¯¦ä½œæ–¹å‘ï¼š
        # 1. ä½¿ç”¨ requests æˆ– httpx ç™¼é€ HTTP è«‹æ±‚
        # 2. è§£æ JSON/CSV å›æ‡‰
        # 3. è½‰æ›ç‚º DataFrame
        raise NotImplementedError(
            "API è¼‰å…¥å™¨å¾…å¯¦ä½œã€‚"
            "è«‹å¯¦ä½œ HTTP è«‹æ±‚é‚è¼¯ï¼Œå°‡ API å›æ‡‰è½‰æ›ç‚º DataFrameã€‚"
        )
    
    def clean(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ¸…æ´— API è³‡æ–™ã€‚

        Args:
            df: å¾ API è¼‰å…¥çš„åŸå§‹ DataFrameã€‚

        Returns:
            æ¸…æ´—å¾Œçš„ DataFrameã€‚
        """
        # TODO: å¯¦ä½œ API è³‡æ–™æ¸…æ´—é‚è¼¯
        # æ ¹æ“šå¯¦éš› API å›æ‡‰æ ¼å¼é€²è¡Œæ¸…æ´—
        return df.copy()


class BidirectionalBinetflowLoaderSpark(BaseDataLoader):
    """é›™å‘æµ Binetflow è¼‰å…¥å™¨ï¼ˆPySpark å–®æ©Ÿæ¨¡å¼ï¼‰
    
    ä½¿ç”¨ PySpark å–®æ©Ÿæ¨¡å¼è®€å– Parquet æ ¼å¼çš„é›™å‘æµè³‡æ–™ã€‚
    é©åˆè™•ç†å¤§æª”æ¡ˆï¼Œè‡ªå‹•åˆ©ç”¨å¤šæ ¸å¿ƒåŠ é€Ÿã€‚
    
    >>> from src.data_loader import BidirectionalBinetflowLoaderSpark
    >>> loader = BidirectionalBinetflowLoaderSpark()
    >>> # df = loader.load()  # éœ€è¦å¯¦éš›æª”æ¡ˆ
    >>> # cleaned = loader.clean(df)
    """
    
    def __init__(self, spark_session=None):
        """
        åˆå§‹åŒ– PySpark è¼‰å…¥å™¨ã€‚
        
        Args:
            spark_session: å¯é¸çš„ SparkSession å¯¦ä¾‹ã€‚å¦‚æœç‚º Noneï¼Œå‰‡è‡ªå‹•å‰µå»ºå–®æ©Ÿæ¨¡å¼ Sessionã€‚
        """
        self._spark = spark_session
        self._spark_created = False
    
    @property
    def spark(self):
        """å–å¾—æˆ–å‰µå»º SparkSessionï¼ˆå»¶é²åˆå§‹åŒ–ï¼‰"""
        if self._spark is None:
            try:
                from pyspark.sql import SparkSession
                import os
                from pathlib import Path
                
                # è¨­å®š Spark è‡¨æ™‚ç›®éŒ„ï¼ˆé¿å… Windows æ¬Šé™å•é¡Œï¼‰
                project_root = get_project_root()
                spark_temp_dir = project_root / "spark_temp"
                spark_temp_dir.mkdir(parents=True, exist_ok=True)
                
                # è¨­å®šç’°å¢ƒè®Šæ•¸
                os.environ['SPARK_LOCAL_DIRS'] = str(spark_temp_dir)
                
                # æ ¹æ“šç³»çµ±è¨˜æ†¶é«”è‡ªå‹•èª¿æ•´ Spark è¨˜æ†¶é«”é…ç½®
                try:
                    import psutil
                    total_memory_gb = psutil.virtual_memory().total / (1024**3)
                    available_memory_gb = psutil.virtual_memory().available / (1024**3)
                    
                    if available_memory_gb >= 20:
                        driver_memory = "12g"
                        executor_memory = "12g"
                        shuffle_partitions = 400
                    elif available_memory_gb >= 16:
                        driver_memory = "8g"
                        executor_memory = "8g"
                        shuffle_partitions = 300
                    elif available_memory_gb >= 12:
                        driver_memory = "6g"
                        executor_memory = "6g"
                        shuffle_partitions = 250
                    else:
                        driver_memory = "4g"
                        executor_memory = "4g"
                        shuffle_partitions = 200
                except ImportError:
                    driver_memory = "8g"
                    executor_memory = "8g"
                    shuffle_partitions = 300
                
                # å‰µå»º SparkSessionï¼ˆå–®æ©Ÿæ¨¡å¼ï¼‰
                self._spark = SparkSession.builder \
                    .appName("NetworkAnomalyDetection") \
                    .master("local[*]") \
                    .config("spark.driver.memory", driver_memory) \
                    .config("spark.executor.memory", executor_memory) \
                    .config("spark.sql.shuffle.partitions", str(shuffle_partitions)) \
                    .config("spark.local.dir", str(spark_temp_dir)) \
                    .config("spark.sql.warehouse.dir", str(spark_temp_dir)) \
                    .config("spark.hadoop.fs.file.impl", "org.apache.hadoop.fs.LocalFileSystem") \
                    .config("spark.hadoop.fs.defaultFS", "file:///") \
                    .getOrCreate()
                
                self._spark_created = True
                print(f"âœ… SparkSession å»ºç«‹å®Œæˆï¼ˆå–®æ©Ÿæ¨¡å¼ï¼Œä½¿ç”¨æ‰€æœ‰æ ¸å¿ƒï¼‰")
            except ImportError:
                raise ImportError(
                    "PySpark æœªå®‰è£ã€‚è«‹åŸ·è¡Œ: pip install pyspark"
                )
        return self._spark
    
    def __del__(self):
        """æ¸…ç† SparkSessionï¼ˆå¦‚æœæ˜¯ç”±æ­¤é¡åˆ¥å‰µå»ºçš„ï¼‰"""
        if self._spark_created and self._spark is not None:
            try:
                self._spark.stop()
            except:
                pass
    
    def load(self, file_path: Optional[Path] = None, use_parquet: bool = True) -> pd.DataFrame:
        """
        å¾ Parquet æª”æ¡ˆå¿«é€Ÿè¼‰å…¥è³‡æ–™ï¼ˆå„ªå…ˆï¼‰ï¼Œæˆ–å¾åŸå§‹ CSV è®€å–ã€‚
        
        ä½¿ç”¨ PySpark å–®æ©Ÿæ¨¡å¼è®€å– CSVï¼Œä½†ä½¿ç”¨ Pandas å¯«å…¥ Parquet ä»¥é¿å… Windows å•é¡Œã€‚
        å°æ–¼å·²å­˜åœ¨çš„ Parquet æª”æ¡ˆï¼Œç›´æ¥ä½¿ç”¨ Pandas è®€å–ä»¥æå‡é€Ÿåº¦ã€‚

        >>> from pathlib import Path
        >>> loader = BidirectionalBinetflowLoaderSpark()
        >>> # éœ€è¦å¯¦éš›æª”æ¡ˆæ‰èƒ½æ¸¬è©¦
        >>> # df = loader.load()
        >>> # assert 'StartTime' in df.columns

        Args:
            file_path: è³‡æ–™æª”æ¡ˆè·¯å¾‘ã€‚å¦‚æœç‚º Noneï¼Œå‰‡ä½¿ç”¨é è¨­è·¯å¾‘ã€‚
            use_parquet: æ˜¯å¦å„ªå…ˆä½¿ç”¨ Parquet æ ¼å¼ï¼ˆé è¨­ Trueï¼‰ã€‚

        Returns:
            åŒ…å«åŸå§‹é›™å‘æµ NetFlow è³‡æ–™çš„ DataFrameã€‚

        Raises:
            FileNotFoundError: å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ã€‚
            ImportError: å¦‚æœ PySpark æœªå®‰è£ã€‚
        """
        project_root = get_project_root()
        
        # å„ªå…ˆä½¿ç”¨ Parquet æ ¼å¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if use_parquet:
            parquet_path = project_root / "data" / "processed" / "capture20110817_cleaned_spark.parquet"
            if parquet_path.exists():
                # å„ªåŒ–ï¼šç›´æ¥ç”¨ Pandas è®€å– Parquetï¼ˆæ¯” Spark å¿«å¾ˆå¤šï¼‰
                print(f"âœ… ä½¿ç”¨ Pandas è®€å– Parquet æª”æ¡ˆ: {parquet_path}")
                try:
                    pandas_df = pd.read_parquet(parquet_path, engine='pyarrow')
                    print(f"âœ… è¼‰å…¥å®Œæˆï¼š{len(pandas_df):,} ç­†è³‡æ–™")
                    return pandas_df
                except Exception as e:
                    print(f"âš ï¸ è®€å– Parquet å¤±æ•—: {e}")
                    print("   å°‡å¾åŸå§‹ CSV é‡æ–°è¼‰å…¥...")
        
        # å¦‚æœæ²’æœ‰ Parquet æˆ–è®€å–å¤±æ•—ï¼Œå¾åŸå§‹ CSV è®€å–ï¼ˆä½¿ç”¨ Spark åŠ é€Ÿï¼‰
        if file_path is None:
            file_path = project_root / "data" / "raw" / "capture20110817.binetflow"
        
        if not file_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {file_path}")
        
        print(f"ğŸ“‚ ä½¿ç”¨ PySpark è®€å–åŸå§‹ CSV æª”æ¡ˆ: {file_path}")
        
        # ä½¿ç”¨ Spark è®€å– CSVï¼ˆæ¯” Pandas å¿«ï¼Œç‰¹åˆ¥æ˜¯å¤§æª”æ¡ˆï¼‰
        spark_df = self.spark.read \
            .option("header", "true") \
            .option("inferSchema", "true") \
            .csv(str(file_path))
        
        # è½‰æ›ç‚º Pandas DataFrame
        print("æ­£åœ¨è½‰æ›ç‚º Pandas DataFrame...")
        pandas_df = spark_df.toPandas()
        
        # è‡ªå‹•å„²å­˜ç‚º Parquet ä»¥ä¾›ä¸‹æ¬¡ä½¿ç”¨ï¼ˆä½¿ç”¨ Pandas é¿å… Windows Hadoop å•é¡Œï¼‰
        if use_parquet:
            parquet_path = project_root / "data" / "processed" / "capture20110817_cleaned_spark.parquet"
            parquet_path.parent.mkdir(parents=True, exist_ok=True)
            
            # ä½¿ç”¨ Pandas å¯«å…¥ Parquetï¼ˆé¿å… Windows ä¸Šçš„ Hadoop å•é¡Œï¼‰
            print(f"ğŸ’¾ æ­£åœ¨å„²å­˜ç‚º Parquet æ ¼å¼: {parquet_path}")
            try:
                pandas_df.to_parquet(
                    parquet_path, 
                    engine='pyarrow', 
                    index=False
                )
                print(f"âœ… Parquet æª”æ¡ˆå·²å„²å­˜ï¼Œä¸‹æ¬¡è¼‰å…¥å°‡æ›´å¿«")
            except Exception as e:
                print(f"âš ï¸ å„²å­˜ Parquet å¤±æ•—: {e}")
                print("   å°‡ç¹¼çºŒä½¿ç”¨åŸå§‹è³‡æ–™ï¼Œä½†ä¸‹æ¬¡ä»éœ€è¦é‡æ–°è¼‰å…¥")
        
        print(f"âœ… è¼‰å…¥å®Œæˆï¼š{len(pandas_df):,} ç­†è³‡æ–™")
        return pandas_df

    
    def clean(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ¸…æ´—èˆ‡è½‰æ›é›™å‘æµ NetFlow è³‡æ–™ã€‚
        
        ä½¿ç”¨èˆ‡ BidirectionalBinetflowLoader ç›¸åŒçš„æ¸…æ´—é‚è¼¯ã€‚

        >>> import pandas as pd
        >>> loader = BidirectionalBinetflowLoaderSpark()
        >>> test_df = pd.DataFrame({
        ...     'StartTime': ['2011-08-17 12:01:01.780', '2011-08-17 12:02:01.780'],
        ...     'Dur': [3.124, 5.456],
        ...     'Proto': ['TCP', 'UDP'],
        ...     'SrcAddr': ['192.168.1.1', '10.0.0.1'],
        ...     'Sport': ['80', '443'],
        ...     'DstAddr': ['172.16.0.1', '192.168.1.100'],
        ...     'Dport': ['8080', '22'],
        ...     'TotBytes': [1000, 2000],
        ...     'TotPkts': [10, 20],
        ...     'Label': ['Background', 'Botnet']
        ... })
        >>> cleaned = loader.clean(test_df)
        >>> pd.api.types.is_datetime64_any_dtype(cleaned['StartTime'])
        True
        >>> 'Dur' in cleaned.columns
        True

        Args:
            df: åŸå§‹é›™å‘æµ NetFlow DataFrameã€‚

        Returns:
            æ¸…æ´—å¾Œçš„ DataFrameï¼ŒåŒ…å«æ­£ç¢ºçš„è³‡æ–™å‹åˆ¥è½‰æ›ã€‚
        """
        # ä½¿ç”¨èˆ‡ BidirectionalBinetflowLoader ç›¸åŒçš„æ¸…æ´—é‚è¼¯
        df = df.copy()
        
        # è½‰æ› StartTime ç‚º datetime
        if 'StartTime' in df.columns:
            df['StartTime'] = pd.to_datetime(df['StartTime'], errors='coerce')
        
        # ç¢ºä¿æ•¸å€¼æ¬„ä½ç‚ºæ­£ç¢ºå‹åˆ¥
        numeric_cols = ['Dur', 'sTos', 'dTos', 'TotPkts', 'TotBytes', 'SrcBytes', 'DstBytes']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # ç¢ºä¿åŸ è™Ÿç‚ºæ•¸å€¼ï¼ˆä½¿ç”¨ coerce å°‡ç„¡æ³•è½‰æ›çš„å€¼è¨­ç‚º NaNï¼Œé¿å… FutureWarningï¼‰
        port_cols = ['Sport', 'Dport']
        for col in port_cols:
            if col in df.columns:
                # ä½¿ç”¨ coerce æ›¿ä»£ ignoreï¼Œç„¡æ³•è½‰æ›çš„å€¼æœƒè®Šæˆ NaN
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        return df


class DataLoaderFactory:
    """è³‡æ–™è¼‰å…¥å™¨å·¥å» 
    
    æ ¹æ“šè³‡æ–™ä¾†æºé¡å‹å‰µå»ºå°æ‡‰çš„è³‡æ–™è¼‰å…¥å™¨ã€‚
    
    >>> from src.data_loader import DataLoaderFactory, DataSourceType
    >>> loader = DataLoaderFactory.create(DataSourceType.BIDIRECTIONAL_BINETFLOW)
    >>> isinstance(loader, BaseDataLoader)
    True
    >>> isinstance(loader, BidirectionalBinetflowLoader)
    True
    """
    
    _loaders = {
        DataSourceType.BIDIRECTIONAL_BINETFLOW: BidirectionalBinetflowLoader,
        DataSourceType.BIDIRECTIONAL_BINETFLOW_SPARK: BidirectionalBinetflowLoaderSpark,
        DataSourceType.API: APIDataLoader,
    }
    
    @classmethod
    def create(cls, source_type: DataSourceType) -> BaseDataLoader:
        """
        å‰µå»ºè³‡æ–™è¼‰å…¥å™¨ã€‚

        >>> from src.data_loader import DataLoaderFactory, DataSourceType
        >>> loader = DataLoaderFactory.create(DataSourceType.BIDIRECTIONAL_BINETFLOW)
        >>> isinstance(loader, BidirectionalBinetflowLoader)
        True

        Args:
            source_type: è³‡æ–™ä¾†æºé¡å‹ï¼ˆDataSourceType æšèˆ‰ï¼‰ã€‚

        Returns:
            å°æ‡‰çš„è³‡æ–™è¼‰å…¥å™¨å¯¦ä¾‹ï¼ˆBaseDataLoader çš„å­é¡åˆ¥ï¼‰ã€‚

        Raises:
            ValueError: å¦‚æœè³‡æ–™ä¾†æºé¡å‹ä¸å­˜åœ¨ã€‚
        """
        if source_type not in cls._loaders:
            available_types = [st.value for st in cls._loaders.keys()]
            raise ValueError(
                f"ä¸æ”¯æ´çš„è³‡æ–™ä¾†æºé¡å‹: {source_type.value}ã€‚"
                f"å¯ç”¨çš„é¡å‹: {available_types}"
            )
        return cls._loaders[source_type]()
    
    @classmethod
    def get_available_types(cls):
        """
        å–å¾—æ‰€æœ‰å¯ç”¨çš„è³‡æ–™ä¾†æºé¡å‹ã€‚

        >>> from src.data_loader import DataLoaderFactory
        >>> types = DataLoaderFactory.get_available_types()
        >>> len(types) >= 1
        True
        >>> DataSourceType.BIDIRECTIONAL_BINETFLOW in types
        True

        Returns:
            å¯ç”¨çš„è³‡æ–™ä¾†æºé¡å‹åˆ—è¡¨ã€‚
        """
        return list(cls._loaders.keys())
    
    @classmethod
    def is_supported(cls, source_type: DataSourceType) -> bool:
        """
        æª¢æŸ¥è³‡æ–™ä¾†æºé¡å‹æ˜¯å¦è¢«æ”¯æ´ã€‚

        >>> from src.data_loader import DataLoaderFactory, DataSourceType
        >>> DataLoaderFactory.is_supported(DataSourceType.BIDIRECTIONAL_BINETFLOW)
        True
        >>> DataLoaderFactory.is_supported(DataSourceType.API)
        True

        Args:
            source_type: è³‡æ–™ä¾†æºé¡å‹ã€‚

        Returns:
            å¦‚æœæ”¯æ´å‰‡è¿”å› Trueï¼Œå¦å‰‡è¿”å› Falseã€‚
        """
        return source_type in cls._loaders

