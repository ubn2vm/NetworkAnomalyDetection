"""
çµ±ä¸€çš„ç‰¹å¾µå·¥ç¨‹æ¨¡çµ„

å¾ NetFlow è³‡æ–™ä¸­æå–ç‰¹å¾µï¼Œæ”¯æ´é›™å‘æµè³‡æ–™æ ¼å¼ã€‚
æ ¹æ“š EDA åˆ†æçµæœï¼Œæå–é—œéµç‰¹å¾µç”¨æ–¼ç•°å¸¸æª¢æ¸¬ã€‚

æ™‚é–“ç‰¹å¾µæ”¯æ´ï¼š
- éšæ®µ1ï¼šåŸºæœ¬æ™‚é–“ç‰¹å¾µï¼ˆhour, day_of_week, is_weekendç­‰ï¼‰å’Œé€±æœŸæ€§ç·¨ç¢¼
- éšæ®µ2ï¼šæ™‚é–“é–“éš”ç‰¹å¾µï¼ˆtime_since_last_flowç­‰ï¼‰
- éšæ®µ3ï¼šæ™‚é–“çª—å£èšåˆç‰¹å¾µï¼ˆflows_per_minute_by_srcç­‰ï¼‰
- éšæ®µ4ï¼šé›™å‘æµ Pair èšåˆç‰¹å¾µï¼ˆä½¿ç”¨ Spark Window Functionï¼‰
"""

import pandas as pd
import numpy as np
from typing import Optional, List
from pathlib import Path


def _extract_basic_time_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    æå–åŸºæœ¬æ™‚é–“ç‰¹å¾µï¼ˆéšæ®µ1ï¼‰ã€‚
    
    åŒ…æ‹¬ï¼š
    - åŸºæœ¬æ™‚é–“ç‰¹å¾µï¼šhour, day_of_week, day_of_month, is_weekend, is_work_hour, is_night
    - é€±æœŸæ€§ç·¨ç¢¼ï¼šsin_hour, cos_hour, sin_day_of_week, cos_day_of_week
    
    Args:
        df: è¼¸å…¥çš„ DataFrameï¼Œå¿…é ˆåŒ…å« 'StartTime' æ¬„ä½ï¼ˆdatetime é¡å‹ï¼‰
    
    Returns:
        åŒ…å«æ™‚é–“ç‰¹å¾µçš„ DataFrame
    """
    features_df = df.copy()
    
    if 'StartTime' not in features_df.columns:
        return features_df
    
    # ç¢ºä¿ StartTime æ˜¯ datetime é¡å‹
    if not pd.api.types.is_datetime64_any_dtype(features_df['StartTime']):
        features_df['StartTime'] = pd.to_datetime(features_df['StartTime'], errors='coerce')
    
    # åŸºæœ¬æ™‚é–“ç‰¹å¾µ
    features_df['hour'] = features_df['StartTime'].dt.hour
    features_df['day_of_week'] = features_df['StartTime'].dt.dayofweek  # 0=Monday, 6=Sunday
    features_df['day_of_month'] = features_df['StartTime'].dt.day
    features_df['is_weekend'] = (features_df['StartTime'].dt.dayofweek >= 5).astype(int)
    features_df['is_work_hour'] = ((features_df['StartTime'].dt.hour >= 9) & 
                                   (features_df['StartTime'].dt.hour < 17)).astype(int)
    features_df['is_night'] = ((features_df['StartTime'].dt.hour >= 22) | 
                              (features_df['StartTime'].dt.hour < 6)).astype(int)
    
    # é€±æœŸæ€§ç·¨ç¢¼ï¼ˆå°‡æ™‚é–“è½‰æ›ç‚ºé€±æœŸæ€§ç‰¹å¾µï¼Œæœ‰åŠ©æ–¼æ¨¡å‹ç†è§£æ™‚é–“çš„å¾ªç’°æ€§ï¼‰
    # å°æ™‚çš„é€±æœŸæ€§ç·¨ç¢¼ï¼ˆ24å°æ™‚å¾ªç’°ï¼‰
    features_df['sin_hour'] = np.sin(2 * np.pi * features_df['hour'] / 24)
    features_df['cos_hour'] = np.cos(2 * np.pi * features_df['hour'] / 24)
    
    # æ˜ŸæœŸçš„é€±æœŸæ€§ç·¨ç¢¼ï¼ˆ7å¤©å¾ªç’°ï¼‰
    features_df['sin_day_of_week'] = np.sin(2 * np.pi * features_df['day_of_week'] / 7)
    features_df['cos_day_of_week'] = np.cos(2 * np.pi * features_df['day_of_week'] / 7)
    
    # æ—¥æœŸçš„é€±æœŸæ€§ç·¨ç¢¼ï¼ˆå‡è¨­30å¤©å¾ªç’°ï¼Œç”¨æ–¼è­˜åˆ¥æœˆåˆ/æœˆåº•æ¨¡å¼ï¼‰
    features_df['sin_day_of_month'] = np.sin(2 * np.pi * features_df['day_of_month'] / 30)
    features_df['cos_day_of_month'] = np.cos(2 * np.pi * features_df['day_of_month'] / 30)
    
    return features_df


def _extract_time_interval_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    æå–æ™‚é–“é–“éš”ç‰¹å¾µï¼ˆéšæ®µ2ï¼‰ã€‚
    
    åŒ…æ‹¬ï¼š
    - time_since_last_flow: è·é›¢ä¸Šä¸€æ¬¡æµçš„æ™‚é–“é–“éš”ï¼ˆç§’ï¼‰
    - time_to_next_flow: è·é›¢ä¸‹ä¸€æ¬¡æµçš„æ™‚é–“é–“éš”ï¼ˆç§’ï¼‰
    
    æ³¨æ„ï¼šéœ€è¦æŒ‰æºIPæ’åºï¼Œè¨ˆç®—æ™‚é–“é–“éš”ã€‚
    
    Args:
        df: è¼¸å…¥çš„ DataFrameï¼Œå¿…é ˆåŒ…å« 'StartTime' å’Œ 'SrcAddr' æ¬„ä½
    
    Returns:
        åŒ…å«æ™‚é–“é–“éš”ç‰¹å¾µçš„ DataFrame
    """
    features_df = df.copy()
    
    if 'StartTime' not in features_df.columns or 'SrcAddr' not in features_df.columns:
        return features_df
    
    # ç¢ºä¿ StartTime æ˜¯ datetime é¡å‹
    if not pd.api.types.is_datetime64_any_dtype(features_df['StartTime']):
        features_df['StartTime'] = pd.to_datetime(features_df['StartTime'], errors='coerce')
    
    # æŒ‰æºIPå’Œæ™‚é–“æ’åº
    features_df = features_df.sort_values(['SrcAddr', 'StartTime']).reset_index(drop=True)
    
    # è¨ˆç®—è·é›¢ä¸Šä¸€æ¬¡æµçš„æ™‚é–“é–“éš”ï¼ˆæŒ‰æºIPåˆ†çµ„ï¼‰
    features_df['time_since_last_flow'] = features_df.groupby('SrcAddr')['StartTime'].diff().dt.total_seconds()
    features_df['time_since_last_flow'] = features_df['time_since_last_flow'].fillna(0)
    
    # è¨ˆç®—è·é›¢ä¸‹ä¸€æ¬¡æµçš„æ™‚é–“é–“éš”
    features_df['time_to_next_flow'] = features_df.groupby('SrcAddr')['StartTime'].diff(-1).dt.total_seconds().abs()
    features_df['time_to_next_flow'] = features_df['time_to_next_flow'].fillna(0)
    
    return features_df


def _extract_time_window_features(df: pd.DataFrame, window_size: str = '1min') -> pd.DataFrame:
    """
    æå–æ™‚é–“çª—å£èšåˆç‰¹å¾µï¼ˆéšæ®µ3ï¼‰ã€‚
    
    åŒ…æ‹¬ï¼š
    - flows_per_minute_by_src: è©²æºIPåœ¨è©²åˆ†é˜å…§çš„æµæ•¸é‡
    - unique_dst_per_minute_by_src: è©²æºIPåœ¨è©²åˆ†é˜å…§é€£æ¥çš„ä¸åŒç›®æ¨™IPæ•¸é‡
    - unique_dport_per_minute_by_src: è©²æºIPåœ¨è©²åˆ†é˜å…§é€£æ¥çš„ä¸åŒç›®æ¨™ç«¯å£æ•¸é‡
    - total_bytes_per_minute_by_src: è©²æºIPåœ¨è©²åˆ†é˜å…§çš„ç¸½ä½å…ƒçµ„æ•¸
    
    æ³¨æ„ï¼šè¨ˆç®—æˆæœ¬è¼ƒé«˜ï¼Œéœ€è¦åˆ†çµ„èšåˆã€‚
    
    Args:
        df: è¼¸å…¥çš„ DataFrameï¼Œå¿…é ˆåŒ…å« 'StartTime', 'SrcAddr', 'DstAddr', 'Dport', 'TotBytes' æ¬„ä½
        window_size: æ™‚é–“çª—å£å¤§å°ï¼Œé è¨­ç‚º '1min'ï¼ˆ1åˆ†é˜ï¼‰
    
    Returns:
        åŒ…å«æ™‚é–“çª—å£èšåˆç‰¹å¾µçš„ DataFrame
    """
    features_df = df.copy()
    
    required_cols = ['StartTime', 'SrcAddr']
    if not all(col in features_df.columns for col in required_cols):
        return features_df
    
    # ç¢ºä¿ StartTime æ˜¯ datetime é¡å‹
    if not pd.api.types.is_datetime64_any_dtype(features_df['StartTime']):
        features_df['StartTime'] = pd.to_datetime(features_df['StartTime'], errors='coerce')
    
    # å»ºç«‹æ™‚é–“çª—å£
    features_df['time_window'] = features_df['StartTime'].dt.floor(window_size)
    
    # è¨ˆç®—è©²æºIPåœ¨è©²åˆ†é˜å…§çš„æµæ•¸é‡
    flows_per_min = features_df.groupby(['SrcAddr', 'time_window']).size()
    flows_per_min = flows_per_min.reset_index(name='flows_per_minute_by_src')
    features_df = features_df.merge(flows_per_min, on=['SrcAddr', 'time_window'], how='left')
    
    # è¨ˆç®—è©²æºIPåœ¨è©²åˆ†é˜å…§é€£æ¥çš„ä¸åŒç›®æ¨™IPæ•¸é‡
    if 'DstAddr' in features_df.columns:
        unique_dst_per_min = features_df.groupby(['SrcAddr', 'time_window'])['DstAddr'].nunique()
        unique_dst_per_min = unique_dst_per_min.reset_index(name='unique_dst_per_minute_by_src')
        features_df = features_df.merge(unique_dst_per_min, on=['SrcAddr', 'time_window'], how='left')
    else:
        features_df['unique_dst_per_minute_by_src'] = 0
    
    # è¨ˆç®—è©²æºIPåœ¨è©²åˆ†é˜å…§é€£æ¥çš„ä¸åŒç›®æ¨™ç«¯å£æ•¸é‡
    if 'Dport' in features_df.columns:
        unique_dport_per_min = features_df.groupby(['SrcAddr', 'time_window'])['Dport'].nunique()
        unique_dport_per_min = unique_dport_per_min.reset_index(name='unique_dport_per_minute_by_src')
        features_df = features_df.merge(unique_dport_per_min, on=['SrcAddr', 'time_window'], how='left')
    else:
        features_df['unique_dport_per_minute_by_src'] = 0
    
    # è¨ˆç®—è©²æºIPåœ¨è©²åˆ†é˜å…§çš„ç¸½ä½å…ƒçµ„æ•¸
    if 'TotBytes' in features_df.columns:
        total_bytes_per_min = features_df.groupby(['SrcAddr', 'time_window'])['TotBytes'].sum()
        total_bytes_per_min = total_bytes_per_min.reset_index(name='total_bytes_per_minute_by_src')
        features_df = features_df.merge(total_bytes_per_min, on=['SrcAddr', 'time_window'], how='left')
    else:
        features_df['total_bytes_per_minute_by_src'] = 0
    
    # ç§»é™¤è‡¨æ™‚æ¬„ä½
    features_df.drop(columns=['time_window'], errors='ignore', inplace=True)
    
    return features_df


def _get_or_create_spark_session():
    """
    å–å¾—æˆ–å‰µå»º SparkSessionï¼ˆç”¨æ–¼é›™å‘æµç‰¹å¾µå·¥ç¨‹ï¼‰ã€‚
    
    Returns:
        SparkSession å¯¦ä¾‹
    
    Raises:
        ImportError: å¦‚æœ PySpark æœªå®‰è£
    """
    try:
        from pyspark.sql import SparkSession
        import os
        import sys
        
        # è¨­å®š Python è·¯å¾‘ï¼ˆWindows ä¸Šé¿å… Python worker é€£ç·šå•é¡Œï¼‰
        python_exe = sys.executable
        os.environ['PYSPARK_PYTHON'] = python_exe
        os.environ['PYSPARK_DRIVER_PYTHON'] = python_exe
        
        # å˜—è©¦å–å¾—ç¾æœ‰çš„ SparkSession
        spark = SparkSession.getActiveSession()
        if spark is not None:
            return spark
        
        # å‰µå»ºæ–°çš„ SparkSession
        from src.data_loader import get_project_root
        project_root = get_project_root()
        spark_temp_dir = project_root / "spark_temp"
        spark_temp_dir.mkdir(parents=True, exist_ok=True)
        
        os.environ['SPARK_LOCAL_DIRS'] = str(spark_temp_dir)
        # Windows ä¸Šè¨­å®š HADOOP_HOMEï¼ˆé¿å… winutils.exe éŒ¯èª¤ï¼‰
        os.environ['HADOOP_HOME'] = str(spark_temp_dir)
        os.environ['hadoop.home.dir'] = str(spark_temp_dir)
        
        # æ ¹æ“šç³»çµ±è¨˜æ†¶é«”è‡ªå‹•èª¿æ•´ Spark è¨˜æ†¶é«”é…ç½®
        try:
            import psutil
            total_memory_gb = psutil.virtual_memory().total / (1024**3)
            available_memory_gb = psutil.virtual_memory().available / (1024**3)
            
            # å»ºè­°é…ç½®ï¼šä½¿ç”¨å¯ç”¨è¨˜æ†¶é«”çš„ 50-60%ï¼Œä½†è‡³å°‘ 4gï¼Œæœ€å¤š 12g
            if available_memory_gb >= 20:
                driver_memory = "12g"
                executor_memory = "12g"
                shuffle_partitions = 400
            elif available_memory_gb >= 16:
                driver_memory = "8g"
                executor_memory = "8g"
                shuffle_partitions = 300
            elif available_memory_gb >= 12:
                driver_memory = "6g"
                executor_memory = "6g"
                shuffle_partitions = 250
            else:
                driver_memory = "4g"
                executor_memory = "4g"
                shuffle_partitions = 200
            
            print(f"   ğŸ’¾ ç³»çµ±è¨˜æ†¶é«”ï¼š{total_memory_gb:.1f} GBï¼ˆå¯ç”¨ï¼š{available_memory_gb:.1f} GBï¼‰")
            print(f"   âš™ï¸  Spark è¨˜æ†¶é«”é…ç½®ï¼šDriver={driver_memory}, Executor={executor_memory}")
        except ImportError:
            # å¦‚æœ psutil æœªå®‰è£ï¼Œä½¿ç”¨é è¨­å€¼
            driver_memory = "8g"
            executor_memory = "8g"
            shuffle_partitions = 300
            print(f"   âš™ï¸  Spark è¨˜æ†¶é«”é…ç½®ï¼šDriver={driver_memory}, Executor={executor_memory}ï¼ˆé è¨­å€¼ï¼‰")
        
        # Windows ä¸Šä½¿ç”¨ local[1] é¿å… Python worker é€£ç·šå•é¡Œ
        # å¦‚æœéœ€è¦ä¸¦è¡Œè™•ç†ï¼Œå¯ä»¥æ”¹å› local[*]ï¼Œä½†éœ€è¦ç¢ºä¿ PYSPARK_PYTHON è¨­å®šæ­£ç¢º
        import platform
        is_windows = platform.system() == 'Windows'
        master_url = "local[1]" if is_windows else "local[*]"
        
        spark = SparkSession.builder \
            .appName("BidirectionalFlowFeatures") \
            .master(master_url) \
            .config("spark.driver.memory", driver_memory) \
            .config("spark.executor.memory", executor_memory) \
            .config("spark.sql.shuffle.partitions", str(shuffle_partitions)) \
            .config("spark.local.dir", str(spark_temp_dir)) \
            .config("spark.sql.warehouse.dir", str(spark_temp_dir)) \
            .config("spark.executor.tempDir", str(spark_temp_dir)) \
            .config("spark.driver.tempDir", str(spark_temp_dir)) \
            .config("spark.hadoop.fs.file.impl", "org.apache.hadoop.fs.LocalFileSystem") \
            .config("spark.hadoop.fs.defaultFS", "file:///") \
            .config("spark.python.worker.reuse", "false") \
            .config("spark.python.worker.timeout", "600") \
            .config("spark.sql.execution.pyspark.udf.faulthandler.enabled", "true") \
            .config("spark.python.worker.faulthandler.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .getOrCreate()
        
        return spark
    except ImportError:
        raise ImportError(
            "PySpark æœªå®‰è£ã€‚è«‹åŸ·è¡Œ: pip install pyspark\n"
            "é›™å‘æµç‰¹å¾µå·¥ç¨‹éœ€è¦ PySpark æ”¯æ´ã€‚"
        )


def _extract_bidirectional_pair_features_pandas(
    df: pd.DataFrame,
    window_size: str = '1min'
) -> pd.DataFrame:
    """
    ä½¿ç”¨ç´” Pandas æå–é›™å‘æµ Pair èšåˆç‰¹å¾µï¼ˆéšæ®µ4ï¼Œé™ç´šæ–¹æ¡ˆï¼‰ã€‚
    
    é€™æ˜¯ PySpark ç‰ˆæœ¬çš„æ›¿ä»£å¯¦ç¾ï¼Œé©ç”¨æ–¼ Windows ç’°å¢ƒæˆ– PySpark ä¸å¯ç”¨çš„æƒ…æ³ã€‚
    
    Args:
        df: è¼¸å…¥çš„ DataFrameï¼Œå¿…é ˆåŒ…å« 'StartTime', 'SrcAddr', 'DstAddr', 'SrcBytes', 'DstBytes', 'TotBytes', 'TotPkts' æ¬„ä½
        window_size: æ™‚é–“çª—å£å¤§å°ï¼Œé è¨­ç‚º '1min'ï¼ˆ1åˆ†é˜ï¼‰
    
    Returns:
        åŒ…å«é›™å‘æµ Pair èšåˆç‰¹å¾µçš„ DataFrame
    """
    features_df = df.copy()
    
    required_cols = ['StartTime', 'SrcAddr', 'DstAddr']
    if not all(col in features_df.columns for col in required_cols):
        return features_df
    
    # ç¢ºä¿ StartTime æ˜¯ datetime é¡å‹
    if not pd.api.types.is_datetime64_any_dtype(features_df['StartTime']):
        features_df['StartTime'] = pd.to_datetime(features_df['StartTime'], errors='coerce')
    
    # æ¨™æº–åŒ– IP Pairï¼šç¢ºä¿ (A, B) å’Œ (B, A) è¢«è¦–ç‚ºåŒä¸€ Pair
    features_df['ip_pair_min'] = features_df[['SrcAddr', 'DstAddr']].min(axis=1)
    features_df['ip_pair_max'] = features_df[['SrcAddr', 'DstAddr']].max(axis=1)
    features_df['ip_pair'] = features_df['ip_pair_min'].astype(str) + '_' + features_df['ip_pair_max'].astype(str)
    
    # å»ºç«‹æ™‚é–“çª—å£
    features_df['time_window_start'] = features_df['StartTime'].dt.floor(window_size)
    
    # ç¢ºä¿å¿…è¦çš„æ•¸å€¼æ¬„ä½å­˜åœ¨
    for col_name in ['TotBytes', 'TotPkts', 'SrcBytes', 'DstBytes']:
        if col_name not in features_df.columns:
            features_df[col_name] = 0
    
    # ä½¿ç”¨ Pandas groupby é€²è¡Œèšåˆ
    group_cols = ['ip_pair', 'time_window_start']
    
    # èšåˆç‰¹å¾µ
    agg_dict = {
        'TotBytes': 'sum',
        'TotPkts': 'sum',
        'SrcBytes': 'sum',
        'DstBytes': 'sum'
    }
    
    if 'Dur' in features_df.columns:
        agg_dict['Dur'] = 'mean'
    
    # è¨ˆç®—èšåˆç‰¹å¾µ
    bidirectional_features = features_df.groupby(group_cols).agg(agg_dict).reset_index()
    
    # é‡æ–°å‘½åèšåˆæ¬„ä½
    bidirectional_features = bidirectional_features.rename(columns={
        'TotBytes': 'bidirectional_total_bytes',
        'TotPkts': 'bidirectional_total_packets',
        'SrcBytes': 'bidirectional_total_src_bytes',
        'DstBytes': 'bidirectional_total_dst_bytes',
        'Dur': 'bidirectional_avg_duration'
    })
    
    # è¨ˆç®—æµæ•¸é‡
    flow_counts = features_df.groupby(group_cols).size().reset_index(name='bidirectional_flow_count')
    bidirectional_features = bidirectional_features.merge(flow_counts, on=group_cols, how='left')
    
    # å¦‚æœæ²’æœ‰ Dur æ¬„ä½ï¼Œæ·»åŠ é è¨­å€¼
    if 'bidirectional_avg_duration' not in bidirectional_features.columns:
        bidirectional_features['bidirectional_avg_duration'] = 0.0
    
    # è¨ˆç®—é›™å‘æµé‡å°ç¨±æ€§
    # symmetry = min(SrcBytes, DstBytes) / max(SrcBytes, DstBytes)
    max_bytes = bidirectional_features[['bidirectional_total_src_bytes', 'bidirectional_total_dst_bytes']].max(axis=1)
    min_bytes = bidirectional_features[['bidirectional_total_src_bytes', 'bidirectional_total_dst_bytes']].min(axis=1)
    bidirectional_features['bidirectional_symmetry'] = (min_bytes / max_bytes).fillna(0.0)
    bidirectional_features.loc[max_bytes == 0, 'bidirectional_symmetry'] = 0.0
    
    # è¨ˆç®—å¹³å‡æ¯å€‹æµçš„ä½å…ƒçµ„æ•¸å’Œå°åŒ…æ•¸
    bidirectional_features['bidirectional_avg_bytes_per_flow'] = (
        bidirectional_features['bidirectional_total_bytes'] / bidirectional_features['bidirectional_flow_count']
    ).fillna(0.0)
    bidirectional_features['bidirectional_avg_packets_per_flow'] = (
        bidirectional_features['bidirectional_total_packets'] / bidirectional_features['bidirectional_flow_count']
    ).fillna(0.0)
    
    # å°‡é›™å‘æµç‰¹å¾µåˆä½µå›åŸå§‹ DataFrame
    merge_cols = ['ip_pair', 'time_window_start']
    features_df = features_df.merge(
        bidirectional_features[merge_cols + [
            'bidirectional_flow_count',
            'bidirectional_total_bytes',
            'bidirectional_total_packets',
            'bidirectional_total_src_bytes',
            'bidirectional_total_dst_bytes',
            'bidirectional_symmetry',
            'bidirectional_avg_bytes_per_flow',
            'bidirectional_avg_packets_per_flow',
            'bidirectional_avg_duration'
        ]],
        on=merge_cols,
        how='left'
    )
    
    # å¡«å……ç¼ºå¤±å€¼ï¼Œä¸¦ç¢ºä¿æ‰€æœ‰å¿…è¦çš„æ¬„ä½éƒ½å­˜åœ¨
    bidirectional_cols = [
        'bidirectional_flow_count',
        'bidirectional_total_bytes',
        'bidirectional_total_packets',
        'bidirectional_total_src_bytes',
        'bidirectional_total_dst_bytes',
        'bidirectional_symmetry',
        'bidirectional_avg_bytes_per_flow',
        'bidirectional_avg_packets_per_flow',
        'bidirectional_avg_duration'
    ]
    for col_name in bidirectional_cols:
        if col_name not in features_df.columns:
            # å¦‚æœæ¬„ä½ä¸å­˜åœ¨ï¼Œå‰µå»ºå®ƒä¸¦è¨­ç‚º 0
            features_df[col_name] = 0.0
        else:
            # å¦‚æœæ¬„ä½å­˜åœ¨ï¼Œå¡«å…… NaN å€¼
            features_df[col_name] = features_df[col_name].fillna(0)
    
    # ç§»é™¤è‡¨æ™‚æ¬„ä½
    features_df.drop(columns=['ip_pair_min', 'ip_pair_max', 'ip_pair', 'time_window_start'], errors='ignore', inplace=True)
    
    return features_df


def _extract_bidirectional_pair_features_spark(
    df: pd.DataFrame, 
    window_size: str = '1min',
    spark_session=None
) -> pd.DataFrame:
    """
    ä½¿ç”¨ Spark Window Function æå–é›™å‘æµ Pair èšåˆç‰¹å¾µï¼ˆéšæ®µ4ï¼‰ã€‚
    
    å°‡ Src -> Dst å’Œ Dst -> Src çš„æµé‡é—œè¯èµ·ä¾†ï¼Œé‡å°åŒä¸€å€‹ (SrcIP, DstIP) Pair
    åœ¨æ™‚é–“çª—å£å…§èšåˆæ•¸æ“šï¼Œæ¨¡æ“¬ Session è¡Œç‚ºã€‚
    
    ç‰¹å¾µåŒ…æ‹¬ï¼š
    - bidirectional_flow_count: è©² IP Pair åœ¨æ™‚é–“çª—å£å…§çš„æµæ•¸é‡ï¼ˆé›™å‘åˆè¨ˆï¼‰
    - bidirectional_total_bytes: è©² IP Pair åœ¨æ™‚é–“çª—å£å…§çš„ç¸½ä½å…ƒçµ„æ•¸ï¼ˆé›™å‘åˆè¨ˆï¼‰
    - bidirectional_total_packets: è©² IP Pair åœ¨æ™‚é–“çª—å£å…§çš„ç¸½å°åŒ…æ•¸ï¼ˆé›™å‘åˆè¨ˆï¼‰
    - bidirectional_symmetry: é›™å‘æµé‡å°ç¨±æ€§ï¼ˆDstBytes / SrcBytesï¼Œç¯„åœ 0-1ï¼‰
    - bidirectional_avg_bytes_per_flow: å¹³å‡æ¯å€‹æµçš„ä½å…ƒçµ„æ•¸
    - bidirectional_avg_packets_per_flow: å¹³å‡æ¯å€‹æµçš„å°åŒ…æ•¸
    
    >>> import pandas as pd
    >>> df = pd.DataFrame({
    ...     'StartTime': pd.to_datetime(['2021-08-17 12:01:00', '2021-08-17 12:01:10', '2021-08-17 12:01:20']),
    ...     'SrcAddr': ['192.168.1.1', '10.0.0.1', '192.168.1.1'],
    ...     'DstAddr': ['10.0.0.1', '192.168.1.1', '10.0.0.1'],
    ...     'SrcBytes': [1000, 500, 200],
    ...     'DstBytes': [100, 200, 50],
    ...     'TotBytes': [1100, 700, 250],
    ...     'TotPkts': [10, 5, 2]
    ... })
    >>> features = _extract_bidirectional_pair_features_spark(df)
    >>> 'bidirectional_flow_count' in features.columns
    True
    >>> 'bidirectional_symmetry' in features.columns
    True
    
    Args:
        df: è¼¸å…¥çš„ DataFrameï¼Œå¿…é ˆåŒ…å« 'StartTime', 'SrcAddr', 'DstAddr', 'SrcBytes', 'DstBytes', 'TotBytes', 'TotPkts' æ¬„ä½
        window_size: æ™‚é–“çª—å£å¤§å°ï¼Œé è¨­ç‚º '1min'ï¼ˆ1åˆ†é˜ï¼‰
        spark_session: å¯é¸çš„ SparkSession å¯¦ä¾‹ã€‚å¦‚æœç‚º Noneï¼Œå‰‡è‡ªå‹•å‰µå»º
    
    Returns:
        åŒ…å«é›™å‘æµ Pair èšåˆç‰¹å¾µçš„ DataFrame
    """
    features_df = df.copy()
    
    required_cols = ['StartTime', 'SrcAddr', 'DstAddr']
    if not all(col in features_df.columns for col in required_cols):
        return features_df
    
    # ç¢ºä¿ StartTime æ˜¯ datetime é¡å‹
    if not pd.api.types.is_datetime64_any_dtype(features_df['StartTime']):
        features_df['StartTime'] = pd.to_datetime(features_df['StartTime'], errors='coerce')
    
    try:
        from pyspark.sql import SparkSession
        from pyspark.sql.functions import (
            col, window, count, sum as spark_sum, avg, 
            min as spark_min, max as spark_max, 
            when, concat, lit, least, greatest, expr
        )
        from pyspark.sql.types import TimestampType
        
        # å–å¾—æˆ–å‰µå»º SparkSession
        if spark_session is None:
            spark = _get_or_create_spark_session()
        else:
            spark = spark_session
        
        # æº–å‚™è½‰æ›ç‚º Spark DataFrame çš„è³‡æ–™
        # åªé¸æ“‡éœ€è¦çš„æ¬„ä½ï¼Œä¸¦ç¢ºä¿é¡å‹å…¼å®¹
        required_spark_cols = ['StartTime', 'SrcAddr', 'DstAddr']
        optional_cols = ['TotBytes', 'TotPkts', 'SrcBytes', 'DstBytes', 'Dur']
        
        # é¸æ“‡å­˜åœ¨çš„æ¬„ä½
        cols_to_use = [col for col in required_spark_cols if col in features_df.columns]
        cols_to_use.extend([col for col in optional_cols if col in features_df.columns])
        
        # å‰µå»ºä¸€å€‹ä¹¾æ·¨çš„ DataFrame ç”¨æ–¼è½‰æ›
        df_for_spark = features_df[cols_to_use].copy()
        
        # è™•ç† NaN å€¼ï¼ˆSpark ä¸å–œæ­¡æŸäº›é¡å‹çš„ NaNï¼‰
        # æ³¨æ„ï¼šä½¿ç”¨ col_name è€Œä¸æ˜¯ colï¼Œé¿å…è¦†è“‹ PySpark çš„ col å‡½æ•¸
        for col_name in df_for_spark.columns:
            if df_for_spark[col_name].dtype in ['object', 'string']:
                # å­—ä¸²æ¬„ä½ï¼šå°‡ NaN è½‰ç‚ºç©ºå­—ä¸²
                df_for_spark[col_name] = df_for_spark[col_name].fillna('')
            elif pd.api.types.is_numeric_dtype(df_for_spark[col_name]):
                # æ•¸å€¼æ¬„ä½ï¼šå°‡ NaN è½‰ç‚º 0
                df_for_spark[col_name] = df_for_spark[col_name].fillna(0)
        
        # ç¢ºä¿ StartTime æ˜¯ datetimeï¼ˆä¸æ˜¯ objectï¼‰
        if 'StartTime' in df_for_spark.columns:
            if not pd.api.types.is_datetime64_any_dtype(df_for_spark['StartTime']):
                df_for_spark['StartTime'] = pd.to_datetime(df_for_spark['StartTime'], errors='coerce')
            # å°‡ NaT è½‰ç‚º Noneï¼ˆSpark å¯ä»¥è™•ç†ï¼‰
            df_for_spark['StartTime'] = df_for_spark['StartTime'].where(pd.notnull(df_for_spark['StartTime']), None)
        
        # å°‡ Pandas DataFrame è½‰æ›ç‚º Spark DataFrame
        # ä½¿ç”¨ inferSchema=False ä¸¦æ‰‹å‹•æŒ‡å®šé¡å‹ï¼Œé¿å…é¡å‹æ¨æ–·å•é¡Œ
        try:
            spark_df = spark.createDataFrame(df_for_spark)
        except Exception as e:
            # å¦‚æœè½‰æ›å¤±æ•—ï¼Œå˜—è©¦åªä½¿ç”¨åŸºæœ¬æ¬„ä½
            print(f"   âš ï¸  å®Œæ•´è½‰æ›å¤±æ•—ï¼Œå˜—è©¦åŸºæœ¬æ¬„ä½è½‰æ›: {e}")
            basic_cols = ['StartTime', 'SrcAddr', 'DstAddr']
            if all(col_name in df_for_spark.columns for col_name in basic_cols):
                spark_df = spark.createDataFrame(df_for_spark[basic_cols])
            else:
                raise
        
        # æ¨™æº–åŒ– IP Pairï¼šç¢ºä¿ (A, B) å’Œ (B, A) è¢«è¦–ç‚ºåŒä¸€ Pair
        # ä½¿ç”¨ min å’Œ max ä¾†æ¨™æº–åŒ–ï¼Œç¢ºä¿ Pair çš„é †åºä¸€è‡´
        spark_df = spark_df.withColumn(
            "ip_pair_min",
            least(col("SrcAddr"), col("DstAddr"))
        ).withColumn(
            "ip_pair_max",
            greatest(col("SrcAddr"), col("DstAddr"))
        ).withColumn(
            "ip_pair",
            concat(col("ip_pair_min"), lit("_"), col("ip_pair_max"))
        )
        
        # ç¢ºä¿ StartTime æ˜¯ TimestampType
        if spark_df.schema["StartTime"].dataType != TimestampType():
            spark_df = spark_df.withColumn(
                "StartTime",
                col("StartTime").cast(TimestampType())
            )
        
        # ä½¿ç”¨ Window Function æŒ‰ (ip_pair, time_window) èšåˆ
        # é€™æ¯” Self-Join æ›´é«˜æ•ˆ
        # ä¿®æ­£ï¼šPySpark window å‡½æ•¸ä½¿ç”¨å­—ç¬¦ä¸²æ ¼å¼çš„æ™‚é–“é–“éš”
        if window_size == '1min':
            window_expr = window(col("StartTime"), "1 minute").alias("time_window")
        elif window_size == '1hour' or window_size == '1h':
            window_expr = window(col("StartTime"), "1 hour").alias("time_window")
        else:
            # å˜—è©¦ç›´æ¥ä½¿ç”¨ï¼ˆå¦‚æœæ ¼å¼æ­£ç¢ºï¼Œå¦‚ "5 minutes", "30 seconds" ç­‰ï¼‰
            window_expr = window(col("StartTime"), window_size).alias("time_window")
        
        # ç¢ºä¿å¿…è¦çš„æ•¸å€¼æ¬„ä½å­˜åœ¨ï¼ˆå¦‚æœä¸å­˜åœ¨å‰‡æ·»åŠ é è¨­å€¼ 0ï¼‰
        for col_name in ['TotBytes', 'TotPkts', 'SrcBytes', 'DstBytes']:
            if col_name not in spark_df.columns:
                spark_df = spark_df.withColumn(col_name, lit(0))
        
        # è¨ˆç®—èšåˆç‰¹å¾µ
        agg_exprs = [
            # æµæ•¸é‡ï¼ˆé›™å‘åˆè¨ˆï¼‰
            count("*").alias("bidirectional_flow_count"),
            
            # ç¸½ä½å…ƒçµ„æ•¸ï¼ˆé›™å‘åˆè¨ˆï¼‰
            spark_sum("TotBytes").alias("bidirectional_total_bytes"),
            
            # ç¸½å°åŒ…æ•¸ï¼ˆé›™å‘åˆè¨ˆï¼‰
            spark_sum("TotPkts").alias("bidirectional_total_packets"),
            
            # ç¸½ä¸Šè¡Œä½å…ƒçµ„æ•¸ï¼ˆSrcBytes ç¸½å’Œï¼‰
            spark_sum("SrcBytes").alias("bidirectional_total_src_bytes"),
            
            # ç¸½ä¸‹è¡Œä½å…ƒçµ„æ•¸ï¼ˆDstBytes ç¸½å’Œï¼‰
            spark_sum("DstBytes").alias("bidirectional_total_dst_bytes")
        ]
        
        # å¹³å‡æŒçºŒæ™‚é–“ï¼ˆå¦‚æœæœ‰ Dur æ¬„ä½ï¼‰
        if "Dur" in spark_df.columns:
            agg_exprs.append(avg("Dur").alias("bidirectional_avg_duration"))
        else:
            agg_exprs.append(lit(0.0).alias("bidirectional_avg_duration"))
        
        bidirectional_features = spark_df.groupBy(
            "ip_pair",
            window_expr
        ).agg(*agg_exprs)
        
        # è¨ˆç®—é›™å‘æµé‡å°ç¨±æ€§
        # symmetry = min(SrcBytes, DstBytes) / max(SrcBytes, DstBytes)
        # ç¯„åœ 0-1ï¼Œ1 è¡¨ç¤ºå®Œå…¨å°ç¨±ï¼Œ0 è¡¨ç¤ºå®Œå…¨ä¸å°ç¨±
        # æ³¨æ„ï¼šä½¿ç”¨ greatest è€Œä¸æ˜¯ spark_maxï¼ˆspark_max æ˜¯èšåˆå‡½æ•¸ï¼Œä¸èƒ½ç”¨æ–¼æ¯”è¼ƒå…©å€‹æ¬„ä½ï¼‰
        bidirectional_features = bidirectional_features.withColumn(
            "bidirectional_symmetry",
            when(
                greatest(col("bidirectional_total_src_bytes"), col("bidirectional_total_dst_bytes")) > lit(0),
                least(col("bidirectional_total_src_bytes"), col("bidirectional_total_dst_bytes")) / 
                greatest(col("bidirectional_total_src_bytes"), col("bidirectional_total_dst_bytes"))
            ).otherwise(lit(0.0))
        )
        
        # è¨ˆç®—å¹³å‡æ¯å€‹æµçš„ä½å…ƒçµ„æ•¸å’Œå°åŒ…æ•¸
        bidirectional_features = bidirectional_features.withColumn(
            "bidirectional_avg_bytes_per_flow",
            col("bidirectional_total_bytes") / col("bidirectional_flow_count")
        ).withColumn(
            "bidirectional_avg_packets_per_flow",
            col("bidirectional_total_packets") / col("bidirectional_flow_count")
        )
        
        # æå– time_window çš„ start æ™‚é–“ï¼ˆç”¨æ–¼å¾ŒçºŒ mergeï¼‰
        # åœ¨ PySpark ä¸­ï¼Œè¨ªå•çµæ§‹æ¬„ä½éœ€è¦ä½¿ç”¨ expr() æˆ– col()["field"] èªæ³•
        # ä½¿ç”¨ expr("time_window.start") ä¾†è¨ªå• window çµæ§‹çš„ start æ¬„ä½
        bidirectional_features = bidirectional_features.withColumn(
            "time_window_start",
            expr("time_window.start")
        )
        
        # è½‰æ›å› Pandas DataFrame
        bidirectional_features_pd = bidirectional_features.toPandas()
        
        # è™•ç† time_window çµæ§‹ï¼ˆå¦‚æœéœ€è¦çš„è©±ï¼‰
        if 'time_window' in bidirectional_features_pd.columns:
            if len(bidirectional_features_pd) > 0 and isinstance(bidirectional_features_pd['time_window'].iloc[0], dict):
                bidirectional_features_pd['time_window_start'] = pd.to_datetime(
                    bidirectional_features_pd['time_window'].apply(lambda x: x['start'] if isinstance(x, dict) else x)
                )
        
        # å°‡é›™å‘æµç‰¹å¾µåˆä½µå›åŸå§‹ DataFrame
        # éœ€è¦ç‚ºæ¯ç­†åŸå§‹è¨˜éŒ„æ‰¾åˆ°å°æ‡‰çš„ IP Pair å’Œæ™‚é–“çª—å£
        features_df['ip_pair_min'] = features_df[['SrcAddr', 'DstAddr']].min(axis=1)
        features_df['ip_pair_max'] = features_df[['SrcAddr', 'DstAddr']].max(axis=1)
        features_df['ip_pair'] = features_df['ip_pair_min'].astype(str) + '_' + features_df['ip_pair_max'].astype(str)
        features_df['time_window_start'] = features_df['StartTime'].dt.floor(window_size)
        
        # Merge é›™å‘æµç‰¹å¾µ
        # ç¢ºä¿æ™‚é–“æ¬„ä½æ ¼å¼ä¸€è‡´
        if 'time_window_start' not in bidirectional_features_pd.columns:
            # å¦‚æœæ²’æœ‰ time_window_startï¼Œå˜—è©¦å¾ time_window çµæ§‹ä¸­æå–
            if 'time_window' in bidirectional_features_pd.columns:
                if len(bidirectional_features_pd) > 0:
                    if isinstance(bidirectional_features_pd['time_window'].iloc[0], dict):
                        bidirectional_features_pd['time_window_start'] = pd.to_datetime(
                            bidirectional_features_pd['time_window'].apply(lambda x: x.get('start', x) if isinstance(x, dict) else x)
                        )
                    else:
                        bidirectional_features_pd['time_window_start'] = pd.to_datetime(bidirectional_features_pd['time_window'])
        
        # ç¢ºä¿æ™‚é–“æ¬„ä½æ ¼å¼ä¸€è‡´
        if 'time_window_start' in bidirectional_features_pd.columns:
            bidirectional_features_pd['time_window_start'] = pd.to_datetime(bidirectional_features_pd['time_window_start'])
        
        merge_cols = ['ip_pair', 'time_window_start']
        
        # ç¢ºä¿ merge_cols éƒ½å­˜åœ¨
        if all(col in bidirectional_features_pd.columns for col in merge_cols):
            features_df = features_df.merge(
                bidirectional_features_pd[merge_cols + [
                    'bidirectional_flow_count',
                    'bidirectional_total_bytes',
                    'bidirectional_total_packets',
                    'bidirectional_total_src_bytes',
                    'bidirectional_total_dst_bytes',
                    'bidirectional_symmetry',
                    'bidirectional_avg_bytes_per_flow',
                    'bidirectional_avg_packets_per_flow',
                    'bidirectional_avg_duration'
                ]],
                on=merge_cols,
                how='left'
            )
        else:
            print(f"âš ï¸  ç„¡æ³• merge é›™å‘æµç‰¹å¾µï¼šç¼ºå°‘å¿…è¦çš„ merge æ¬„ä½")
            print(f"   éœ€è¦çš„æ¬„ä½: {merge_cols}")
            print(f"   å¯¦éš›æ¬„ä½: {list(bidirectional_features_pd.columns)}")
            # å³ä½¿ merge å¤±æ•—ï¼Œä¹Ÿå‰µå»ºå¿…è¦çš„æ¬„ä½ï¼ˆè¨­ç‚º 0ï¼‰ï¼Œä»¥ä¾¿å¾ŒçºŒè¨ˆç®— bidirectional_window_flow_ratio
            print(f"   å‰µå»ºé è¨­çš„é›™å‘æµç‰¹å¾µæ¬„ä½ï¼ˆå€¼ç‚º 0ï¼‰")
        
        # å¡«å……ç¼ºå¤±å€¼ï¼ˆå¦‚æœæŸäº› IP Pair æ²’æœ‰å°æ‡‰çš„ç‰¹å¾µï¼‰
        # åŒæ™‚ç¢ºä¿æ‰€æœ‰å¿…è¦çš„æ¬„ä½éƒ½å­˜åœ¨
        bidirectional_cols = [
            'bidirectional_flow_count',
            'bidirectional_total_bytes',
            'bidirectional_total_packets',
            'bidirectional_total_src_bytes',
            'bidirectional_total_dst_bytes',
            'bidirectional_symmetry',
            'bidirectional_avg_bytes_per_flow',
            'bidirectional_avg_packets_per_flow',
            'bidirectional_avg_duration'
        ]
        for col_name in bidirectional_cols:
            if col_name not in features_df.columns:
                # å¦‚æœæ¬„ä½ä¸å­˜åœ¨ï¼Œå‰µå»ºå®ƒä¸¦è¨­ç‚º 0
                features_df[col_name] = 0.0
            else:
                # å¦‚æœæ¬„ä½å­˜åœ¨ï¼Œå¡«å…… NaN å€¼
                features_df[col_name] = features_df[col_name].fillna(0)
        
        # ç§»é™¤è‡¨æ™‚æ¬„ä½
        features_df.drop(columns=['ip_pair_min', 'ip_pair_max', 'ip_pair', 'time_window_start'], errors='ignore', inplace=True)
        
        return features_df
        
    except ImportError:
        # å¦‚æœ PySpark æœªå®‰è£ï¼Œä½¿ç”¨ Pandas é™ç´šæ–¹æ¡ˆ
        print("âš ï¸  PySpark æœªå®‰è£ï¼Œä½¿ç”¨ Pandas é™ç´šæ–¹æ¡ˆ...")
        return _extract_bidirectional_pair_features_pandas(features_df, window_size)
    except Exception as e:
        # å¦‚æœ Spark è™•ç†å¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨ Pandas é™ç´šæ–¹æ¡ˆ
        import platform
        is_windows = platform.system() == 'Windows'
        error_msg = str(e)
        
        # æª¢æŸ¥æ˜¯å¦ç‚º PySpark ç›¸é—œéŒ¯èª¤ï¼ˆåŒ…æ‹¬æ‰€æœ‰å¯èƒ½çš„éŒ¯èª¤é¡å‹ï¼‰
        pyspark_errors = [
            'Python worker',
            'crashed',
            'collectToPython',
            'EOFException',
            'SparkException',
            'SocketTimeoutException',
            'WinError 10038',
            'not a socket'
        ]
        
        is_pyspark_error = any(err.lower() in error_msg.lower() for err in pyspark_errors)
        
        # Windows ä¸Šçš„å¸¸è¦‹éŒ¯èª¤ï¼Œæˆ–ä»»ä½• PySpark ç›¸é—œéŒ¯èª¤ï¼Œè‡ªå‹•é™ç´šåˆ° Pandas
        if is_windows or is_pyspark_error:
            error_preview = error_msg[:150].replace('\n', ' ') if len(error_msg) > 150 else error_msg
            print(f"âš ï¸  PySpark åŸ·è¡Œå¤±æ•—ï¼Œä½¿ç”¨ Pandas é™ç´šæ–¹æ¡ˆ...")
            print(f"   éŒ¯èª¤è¨Šæ¯ï¼š{error_preview}...")
            print("   ğŸ’¡ æç¤ºï¼šPandas ç‰ˆæœ¬è¼ƒæ…¢ä½†æ›´ç©©å®šï¼Œé©åˆ Windows ç’°å¢ƒ")
            try:
                return _extract_bidirectional_pair_features_pandas(features_df, window_size)
            except Exception as pandas_error:
                print(f"âš ï¸  Pandas é™ç´šæ–¹æ¡ˆä¹Ÿå¤±æ•—: {pandas_error}")
                print("   è¿”å›åŸå§‹ DataFrameï¼ˆä¸åŒ…å«é›™å‘æµç‰¹å¾µï¼‰")
                return features_df
        else:
            # å…¶ä»–éŒ¯èª¤ï¼Œè¿”å›åŸå§‹ DataFrame
            print(f"âš ï¸  é›™å‘æµç‰¹å¾µå·¥ç¨‹å¤±æ•—: {e}")
            print("   è¿”å›åŸå§‹ DataFrameï¼ˆä¸åŒ…å«é›™å‘æµç‰¹å¾µï¼‰")
            return features_df


def extract_features(
    df: pd.DataFrame,
    flow_type: str = 'auto',
    include_time_features: bool = True,
    time_feature_stage: int = 1,
    include_bidirectional_features: bool = False
) -> pd.DataFrame:
    """
    å¾ NetFlow è³‡æ–™ä¸­æå–ç‰¹å¾µã€‚
    
    æ ¹æ“š notebooks/bidirectional/01_EDA.ipynb çš„åˆ†æçµæœï¼Œ
    æå–ä»¥ä¸‹é—œéµç‰¹å¾µï¼š
    - DstBytes: ç›®çš„ç«¯ä½å…ƒçµ„æ•¸
    - flow_ratio: ä¸Šä¸‹è¡Œæµé‡æ¯”ï¼ˆæª¢æ¸¬å¤–æ´©è¡Œç‚ºï¼‰
    - bytes_symmetry: è¡Œç‚ºå°ç¨±æ€§ï¼ˆæª¢æ¸¬æƒæè¡Œç‚ºï¼‰
    - is_scanning: æƒæè¡Œç‚ºæ¨™è¨˜
    - src_ratio: ä¾†æºæµé‡æ¯”ä¾‹ï¼ˆ%ï¼‰
    - dst_ratio: ç›®çš„æµé‡æ¯”ä¾‹ï¼ˆ%ï¼‰
    - packet_size: å¹³å‡å°åŒ…å¤§å°ï¼ˆbytesï¼‰
    - bytes_per_second: ä½å…ƒçµ„å‚³è¼¸é€Ÿç‡
    - packets_per_second: å°åŒ…å‚³è¼¸é€Ÿç‡
    
    >>> import pandas as pd
    >>> df = pd.DataFrame({
    ...     'TotBytes': [1000, 2000, 3000],
    ...     'SrcBytes': [600, 1200, 1800],
    ...     'TotPkts': [10, 20, 30],
    ...     'Dur': [1.0, 2.0, 3.0],
    ...     'State': ['CON', 'RST', 'CON']
    ... })
    >>> features = extract_features(df)
    >>> 'flow_ratio' in features.columns
    True
    >>> 'bytes_symmetry' in features.columns
    True
    >>> 'is_scanning' in features.columns
    True
    >>> 'packet_size' in features.columns
    True
    >>> 'src_ratio' in features.columns
    True
    
    Args:
        df: è¼¸å…¥çš„ NetFlow DataFrame
        flow_type: æµé¡å‹ ('auto', 'bidirectional')ï¼Œç›®å‰åƒ…æ”¯æ´ bidirectional
        include_time_features: æ˜¯å¦åŒ…å«æ™‚é–“ç‰¹å¾µ
        time_feature_stage: æ™‚é–“ç‰¹å¾µéšæ®µï¼ˆ1-4ï¼‰
            - 1: åŸºæœ¬æ™‚é–“ç‰¹å¾µ
            - 2: æ™‚é–“é–“éš”ç‰¹å¾µ
            - 3: æ™‚é–“çª—å£èšåˆç‰¹å¾µï¼ˆæŒ‰ SrcAddrï¼‰
            - 4: é›™å‘æµ Pair èšåˆç‰¹å¾µï¼ˆæŒ‰ IP Pairï¼Œéœ€è¦ PySparkï¼‰
        include_bidirectional_features: æ˜¯å¦åŒ…å«é›™å‘æµç‰¹å¾µï¼ˆå·²æ£„ç”¨ï¼Œä½¿ç”¨ time_feature_stage=4ï¼‰
    
    Returns:
        åŒ…å«ç‰¹å¾µçš„ DataFrame
    """
    # è¤‡è£½ DataFrame é¿å…ä¿®æ”¹åŸå§‹è³‡æ–™
    features_df = df.copy()
    
    # 1. è¨ˆç®—ç›®çš„ç«¯ä½å…ƒçµ„æ•¸
    if 'DstBytes' not in features_df.columns:
        if 'TotBytes' in features_df.columns and 'SrcBytes' in features_df.columns:
            features_df['DstBytes'] = features_df['TotBytes'] - features_df['SrcBytes']
        else:
            raise ValueError("ç¼ºå°‘å¿…è¦æ¬„ä½ï¼šéœ€è¦ 'TotBytes' å’Œ 'SrcBytes' æˆ– 'DstBytes'")
    
    # 2. è¨ˆç®— flow_ratioï¼ˆä¸Šä¸‹è¡Œæµé‡æ¯”ï¼‰
    # ç”¨æ–¼æª¢æ¸¬å¤–æ´©è¡Œç‚ºï¼šSrcBytes é å¤§æ–¼ DstBytes æ™‚ï¼Œflow_ratio æœƒå¾ˆå¤§
    features_df['flow_ratio'] = features_df['SrcBytes'] / (features_df['DstBytes'] + 1)  # +1 é¿å…é™¤é›¶
    
    # 3. è¨ˆç®— bytes_symmetryï¼ˆè¡Œç‚ºå°ç¨±æ€§ï¼‰
    # ç”¨æ–¼æª¢æ¸¬æƒæè¡Œç‚ºï¼šDstBytes é å°æ–¼ SrcBytes æ™‚ï¼Œbytes_symmetry æ¥è¿‘ 0
    features_df['bytes_symmetry'] = features_df['DstBytes'] / (features_df['SrcBytes'] + 1)  # +1 é¿å…é™¤é›¶
    
    # 4. è¨ˆç®—æµé‡æ–¹å‘æ¯”ä¾‹ï¼ˆæ–°å¢ï¼‰
    # ç”¨æ–¼åˆ†ææµé‡æ–¹å‘å¹³è¡¡ï¼Œè­˜åˆ¥å–®å‘æµé‡ç•°å¸¸
    if 'TotBytes' in features_df.columns:
        # é¿å…é™¤é›¶ï¼šå°‡ TotBytes ç‚º 0 çš„æƒ…æ³è¨­ç‚º NaN
        tot_bytes_safe = features_df['TotBytes'].replace(0, np.nan)
        features_df['src_ratio'] = (features_df['SrcBytes'] / tot_bytes_safe * 100).fillna(0)
        features_df['dst_ratio'] = (features_df['DstBytes'] / tot_bytes_safe * 100).fillna(0)
    
    # 5. è¨ˆç®—å¹³å‡å°åŒ…å¤§å°ï¼ˆæ–°å¢ï¼‰
    # ç”¨æ–¼è­˜åˆ¥ç•°å¸¸å°åŒ…å¤§å°æ¨¡å¼
    if 'TotPkts' in features_df.columns:
        features_df['packet_size'] = features_df['TotBytes'] / (features_df['TotPkts'] + 1)  # +1 é¿å…é™¤é›¶
    
    # 6. è¨ˆç®—æµé‡å¼·åº¦ç‰¹å¾µï¼ˆæ–°å¢ï¼‰
    # ç”¨æ–¼æª¢æ¸¬é«˜å¼·åº¦æµé‡ï¼ˆDDoSã€æƒæç­‰ï¼‰
    if 'Dur' in features_df.columns:
        features_df['bytes_per_second'] = features_df['TotBytes'] / (features_df['Dur'] + 1)  # +1 é¿å…é™¤é›¶
        features_df['packets_per_second'] = features_df['TotPkts'] / (features_df['Dur'] + 1)  # +1 é¿å…é™¤é›¶
    
    # 7. è¨ˆç®— is_scanningï¼ˆæƒæè¡Œç‚ºæ¨™è¨˜ï¼‰
    # æ ¹æ“š State æ¬„ä½åˆ¤æ–·æ˜¯å¦ç‚ºæƒæè¡Œç‚º
    if 'State' in features_df.columns:
        scanning_states = ['RST', 'REQ', 'S_', 'INT', 'URP']
        features_df['is_scanning'] = features_df['State'].isin(scanning_states).astype(int)
    else:
        # å¦‚æœæ²’æœ‰ State æ¬„ä½ï¼Œä½¿ç”¨ bytes_symmetry ä½œç‚ºæ›¿ä»£æŒ‡æ¨™
        features_df['is_scanning'] = (features_df['bytes_symmetry'] < 0.1).astype(int)
    
    # 8. æå–æ™‚é–“ç‰¹å¾µï¼ˆå¯é¸ï¼‰
    if include_time_features and 'StartTime' in features_df.columns:
        if time_feature_stage >= 1:
            features_df = _extract_basic_time_features(features_df)
        if time_feature_stage >= 2:
            features_df = _extract_time_interval_features(features_df)
        if time_feature_stage >= 3:
            features_df = _extract_time_window_features(features_df)
        if time_feature_stage >= 4:
            # éšæ®µ4ï¼šé›™å‘æµ Pair èšåˆç‰¹å¾µï¼ˆä½¿ç”¨ Spark Window Functionï¼‰
            features_df = _extract_bidirectional_pair_features_spark(features_df)
            
            # è¨ˆç®— bidirectional_window_flow_ratioï¼ˆä½¿ç”¨éšæ®µå››å·²èšåˆçš„è³‡æ–™ï¼‰
            # åœ¨æ™‚é–“çª—å£å…§èšåˆå¾Œçš„ä¸Šä¸‹è¡Œæµé‡æ¯”ï¼Œç”¨æ–¼æª¢æ¸¬æŒçºŒæ€§å¤–æ´©è¡Œç‚º
            # ç¢ºä¿å¿…è¦çš„æ¬„ä½å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å‰µå»ºé è¨­å€¼
            if 'bidirectional_total_src_bytes' not in features_df.columns:
                features_df['bidirectional_total_src_bytes'] = 0.0
            if 'bidirectional_total_dst_bytes' not in features_df.columns:
                features_df['bidirectional_total_dst_bytes'] = 0.0
            
            # è¨ˆç®— bidirectional_window_flow_ratio
            # ç¢ºä¿æ‰€æœ‰å€¼éƒ½æ˜¯æ•¸å€¼é¡å‹ï¼Œä¸¦è™•ç† NaN å’Œç„¡é™å€¼
            features_df['bidirectional_window_flow_ratio'] = (
                features_df['bidirectional_total_src_bytes'].astype(float) / 
                (features_df['bidirectional_total_dst_bytes'].astype(float) + 1)  # +1 é¿å…é™¤é›¶
            ).fillna(0.0).replace([np.inf, -np.inf], 0.0)
    
    # 9. è™•ç†ç„¡é™å€¼å’Œç•°å¸¸å€¼
    # å°‡ç„¡é™å€¼æ›¿æ›ç‚º NaNï¼Œç„¶å¾Œç”¨ 0 å¡«å……
    features_df = features_df.replace([np.inf, -np.inf], np.nan)
    
    # å°æ–¼æ•¸å€¼ç‰¹å¾µï¼Œç”¨ 0 å¡«å…… NaNï¼ˆæˆ–å¯ä»¥æ ¹æ“šæ¥­å‹™é‚è¼¯é¸æ“‡å…¶ä»–ç­–ç•¥ï¼‰
    numeric_cols = features_df.select_dtypes(include=[np.number]).columns
    features_df[numeric_cols] = features_df[numeric_cols].fillna(0)
    
    return features_df


def get_feature_columns(include_time_features: bool = True, time_feature_stage: int = 1) -> List[str]:
    """
    è¿”å›ç‰¹å¾µå·¥ç¨‹ç”¢ç”Ÿçš„ç‰¹å¾µæ¬„ä½åç¨±åˆ—è¡¨ã€‚
    
    >>> cols = get_feature_columns()
    >>> 'flow_ratio' in cols
    True
    >>> 'bytes_symmetry' in cols
    True
    >>> 'packet_size' in cols
    True
    >>> 'src_ratio' in cols
    True
    
    Args:
        include_time_features: æ˜¯å¦åŒ…å«æ™‚é–“ç‰¹å¾µ
        time_feature_stage: æ™‚é–“ç‰¹å¾µéšæ®µï¼ˆ1, 2, æˆ– 3ï¼‰
    
    Returns:
        ç‰¹å¾µæ¬„ä½åç¨±åˆ—è¡¨
    """
    base_features = [
        'DstBytes',
        'flow_ratio',
        'bytes_symmetry',
        'is_scanning',
        'src_ratio',
        'dst_ratio',
        'packet_size',
        'bytes_per_second',
        'packets_per_second'
    ]
    
    if not include_time_features:
        return base_features
    
    # éšæ®µ1ï¼šåŸºæœ¬æ™‚é–“ç‰¹å¾µ
    time_features_stage1 = [
        'hour',
        'day_of_week',
        'day_of_month',
        'is_weekend',
        'is_work_hour',
        'is_night',
        'sin_hour',
        'cos_hour',
        'sin_day_of_week',
        'cos_day_of_week',
        'sin_day_of_month',
        'cos_day_of_month'
    ]
    
    # éšæ®µ2ï¼šæ™‚é–“é–“éš”ç‰¹å¾µ
    time_features_stage2 = [
        'time_since_last_flow',
        'time_to_next_flow'
    ]
    
    # éšæ®µ3ï¼šæ™‚é–“çª—å£èšåˆç‰¹å¾µ
    time_features_stage3 = [
        'flows_per_minute_by_src',
        'unique_dst_per_minute_by_src',
        'unique_dport_per_minute_by_src',
        'total_bytes_per_minute_by_src'
    ]
    
    # éšæ®µ4ï¼šé›™å‘æµ Pair èšåˆç‰¹å¾µ
    time_features_stage4 = [
        'bidirectional_flow_count',
        'bidirectional_total_bytes',
        'bidirectional_total_packets',
        'bidirectional_total_src_bytes',
        'bidirectional_total_dst_bytes',
        'bidirectional_symmetry',
        'bidirectional_avg_bytes_per_flow',
        'bidirectional_avg_packets_per_flow',
        'bidirectional_avg_duration',
        'bidirectional_window_flow_ratio'  # æ™‚é–“çª—å£å…§èšåˆå¾Œçš„ä¸Šä¸‹è¡Œæµé‡æ¯”
    ]
    
    all_features = base_features + time_features_stage1
    
    if time_feature_stage >= 2:
        all_features.extend(time_features_stage2)
    
    if time_feature_stage >= 3:
        all_features.extend(time_features_stage3)
    
    if time_feature_stage >= 4:
        all_features.extend(time_features_stage4)
    
    return all_features


if __name__ == '__main__':
    # ç°¡å–®æ¸¬è©¦
    import doctest
    doctest.testmod(verbose=True)

