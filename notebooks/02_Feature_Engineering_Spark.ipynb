{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - Feature Engineering with Spark\n",
        "\n",
        "本 Notebook 聚焦於：\n",
        "- 使用 PySpark 進行大規模資料處理\n",
        "- 時間窗口聚合特徵工程（Source IP × 1 分鐘）\n",
        "- 計算 Count、Diversity、Bytes Ratio、Port Entropy 等特徵\n",
        "- 證明 Raw NetFlow 不可直接進模型，需要特徵工程\n",
        "- 產製特徵表並儲存至 `data/processed/features.parquet`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 環境設定與 Spark Session 初始化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# 設定 Python 路徑（避免 Spark 找不到 Python）\n",
        "python_exe = sys.executable\n",
        "os.environ['PYSPARK_PYTHON'] = python_exe\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = python_exe\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, count, sum as spark_sum, min as spark_min, max as spark_max, avg, countDistinct,\n",
        "    window, size, expr,  when, to_timestamp, split, log2, isnan, isnull\n",
        ")\n",
        "from pyspark.sql.types import StringType, DoubleType\n",
        "import math\n",
        "\n",
        "# 設定專案路徑\n",
        "PROJECT_ROOT = Path.cwd().resolve()\n",
        "if PROJECT_ROOT.name == \"notebooks\":\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "INPUT_PATH = DATA_DIR / \"capture20110817_cleaned_spark.parquet\"\n",
        "OUTPUT_PATH = DATA_DIR / \"features.parquet\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 先關閉現有的 SparkSession\n",
        "try:\n",
        "    spark.stop()\n",
        "    print(\"已關閉現有的 SparkSession\")\n",
        "except:\n",
        "    print(\"沒有現有的 SparkSession 需要關閉\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 建立 SparkSession\n",
        "# 設定臨時目錄（使用專案目錄，路徑更短，避免 Windows 路徑長度限制）\n",
        "import os\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "# 使用專案目錄下的臨時資料夾（路徑更短）\n",
        "PROJECT_ROOT = Path.cwd().resolve()\n",
        "if PROJECT_ROOT.name == \"notebooks\":\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "\n",
        "# 建立臨時目錄\n",
        "spark_temp_dir = str(PROJECT_ROOT / \"spark_temp\")\n",
        "os.makedirs(spark_temp_dir, exist_ok=True)\n",
        "\n",
        "# 設定 Hadoop 環境變數（Windows 上避免 HADOOP_HOME 錯誤）\n",
        "os.environ['HADOOP_HOME'] = spark_temp_dir\n",
        "os.environ['hadoop.home.dir'] = spark_temp_dir\n",
        "\n",
        "print(f\"設定 Spark 臨時目錄: {spark_temp_dir}\")\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NetworkAnomalyFeatureEngineering\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.local.dir\", spark_temp_dir) \\\n",
        "    .config(\"spark.sql.warehouse.dir\", spark_temp_dir) \\\n",
        "    .config(\"spark.executor.tempDir\", spark_temp_dir) \\\n",
        "    .config(\"spark.driver.tempDir\", spark_temp_dir) \\\n",
        "    .config(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.LocalFileSystem\") \\\n",
        "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"    SparkSession 建立成功！\")\n",
        "print(f\"    Spark 版本: {spark.version}\")\n",
        "print(f\"    臨時目錄: {spark_temp_dir}\")\n",
        "print(f\"    Spark Context: {spark.sparkContext}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 讀取資料與基本檢查\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 讀取 Spark 相容的 Parquet 檔案\n",
        "if not INPUT_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"找不到 {INPUT_PATH}. 請先執行 scripts/load_raw_data_pyspark.py 產生 Parquet 檔。\"\n",
        "    )\n",
        "\n",
        "print(f\"正在讀取: {INPUT_PATH}\")\n",
        "\n",
        "# 直接使用 Spark 讀取 Parquet 檔案\n",
        "df = spark.read.parquet(str(INPUT_PATH))\n",
        "\n",
        "print(f\"✅ Spark 讀取完成！\")\n",
        "print(f\"   總筆數: {df.count():,}\")\n",
        "print(f\"   欄位數: {len(df.columns)}\")\n",
        "print(f\"   欄位名稱: {df.columns}\")\n",
        "print(\"\\n資料結構（Schema）：\")\n",
        "df.printSchema()\n",
        "\n",
        "print(\"\\n前 5 筆資料：\")\n",
        "df.show(5, truncate=False)\n",
        "\n",
        "# 檢查並轉換時間欄位（從字串轉為 timestamp）\n",
        "if \"Date_Flow_Start\" in df.columns:\n",
        "    from pyspark.sql.types import TimestampType, StringType\n",
        "    \n",
        "    # 檢查當前資料型別\n",
        "    current_type = df.schema[\"Date_Flow_Start\"].dataType\n",
        "    print(f\"\\n時間欄位資訊：\")\n",
        "    print(f\"   目前型別: {current_type}\")\n",
        "    \n",
        "    # 如果是字串，轉換為 timestamp\n",
        "    if isinstance(current_type, StringType):\n",
        "        print(\"   轉換 Date_Flow_Start 為 timestamp 類型...\")\n",
        "        df = df.withColumn(\"Date_Flow_Start\", to_timestamp(col(\"Date_Flow_Start\")))\n",
        "        print(\"   ✅ 轉換完成\")\n",
        "    \n",
        "    # 顯示時間範圍\n",
        "    print(\"\\n時間範圍統計：\")\n",
        "    time_stats = df.agg(\n",
        "        spark_min(\"Date_Flow_Start\").alias(\"最早時間\"),\n",
        "        spark_max(\"Date_Flow_Start\").alias(\"最晚時間\"),\n",
        "        count(\"Date_Flow_Start\").alias(\"有效記錄數\")\n",
        "    )\n",
        "    time_stats.show(truncate=False)\n",
        "    \n",
        "    # 計算時間跨度（使用 unix_timestamp 轉換為秒數）\n",
        "    from pyspark.sql.functions import unix_timestamp\n",
        "    time_span_result = df.agg(\n",
        "        (unix_timestamp(spark_max(\"Date_Flow_Start\")) - unix_timestamp(spark_min(\"Date_Flow_Start\"))).alias(\"時間跨度_秒\")\n",
        "    ).collect()[0][\"時間跨度_秒\"]\n",
        "    if time_span_result is not None:\n",
        "        hours = time_span_result / 3600\n",
        "        days = hours / 24\n",
        "        print(f\"\\n   時間跨度: {days:.2f} 天 ({hours:.2f} 小時, {time_span_result:,} 秒)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 資料預處理\n",
        "\n",
        "### 3.1 拆分 IP:Port 欄位\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 拆分 Src_IP_Port 和 Dst_IP_Port\n",
        "# 格式: \"IP:Port\" -> 提取 IP 和 Port\n",
        "# 同一 IP 的不同 Port 應歸為同一來源，需要按 src_ip 分組，而不是 Src_IP_Port\n",
        "\n",
        "# 拆分 Source IP:Port\n",
        "df = df.withColumn(\"src_ip\", split(col(\"Src_IP_Port\"), \":\").getItem(0)) \\\n",
        "       .withColumn(\"src_port\", split(col(\"Src_IP_Port\"), \":\").getItem(1).cast(\"int\"))\n",
        "\n",
        "# 拆分 Destination IP:Port\n",
        "df = df.withColumn(\"dst_ip\", split(col(\"Dst_IP_Port\"), \":\").getItem(0)) \\\n",
        "       .withColumn(\"dst_port\", split(col(\"Dst_IP_Port\"), \":\").getItem(1).cast(\"int\"))\n",
        "\n",
        "print(\"✅ IP:Port 欄位拆分完成\")\n",
        "print(\"\\n檢查拆分結果（前 5 筆）：\")\n",
        "df.select(\"Src_IP_Port\", \"src_ip\", \"src_port\", \"Dst_IP_Port\", \"dst_ip\", \"dst_port\").show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 標籤二元化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"開始標籤二元化處理...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 步驟 1: 使用 sample 取得 Label 樣本（避免 shuffle）\n",
        "print(\"\\n[步驟 1] 檢查 Label 欄位的樣本值...\")\n",
        "try:\n",
        "    # 使用 sample 取得少量樣本來檢查 Label 值\n",
        "    sample_df = df.select(\"Label\").sample(False, 0.001, seed=42).distinct().limit(20)\n",
        "    sample_labels = [row[\"Label\"] for row in sample_df.collect()]\n",
        "    print(f\"✅ 從樣本中找到 {len(sample_labels)} 個 Label 值\")\n",
        "    print(f\"   Label 樣本: {sample_labels}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ 樣本檢查失敗: {e}\")\n",
        "    sample_labels = []\n",
        "\n",
        "# 步驟 2: 識別 Botnet 標籤\n",
        "print(\"\\n[步驟 2] 識別 Botnet 標籤...\")\n",
        "botnet_labels = []\n",
        "if sample_labels:\n",
        "    # 方法 1: 尋找包含 \"botnet\" 或 \"bot\" 的標籤\n",
        "    botnet_labels = [label for label in sample_labels if label and (\"botnet\" in str(label).lower() or \"bot\" in str(label).lower())]\n",
        "    print(f\"   方法 1 - 找到 Botnet 標籤: {botnet_labels}\")\n",
        "    \n",
        "    # 方法 2: 如果沒有找到，使用更寬鬆的匹配（非正常標籤）\n",
        "    if not botnet_labels:\n",
        "        normal_labels = [\"Background\", \"background\", \"LEGIT\", \"legit\", \"Normal\", \"normal\"]\n",
        "        botnet_labels = [label for label in sample_labels if label and str(label) not in normal_labels]\n",
        "        print(f\"   方法 2 - 使用更寬鬆匹配，找到異常標籤: {botnet_labels}\")\n",
        "\n",
        "# 步驟 3: 執行標籤二元化（使用簡單的條件判斷，避免需要知道所有標籤）\n",
        "print(\"\\n[步驟 3] 執行標籤二元化...\")\n",
        "try:\n",
        "    # 使用簡單的條件判斷：包含 \"botnet\" 或 \"bot\" 的標籤 → 1，否則 → 0\n",
        "    # 這樣不需要知道所有標籤值，避免 shuffle\n",
        "    df = df.withColumn(\n",
        "        \"label_binary\",\n",
        "        when(\n",
        "            col(\"Label\").rlike(\"(?i).*botnet.*\") | col(\"Label\").rlike(\"(?i).*bot.*\"),\n",
        "            1\n",
        "        ).otherwise(0)\n",
        "    )\n",
        "    \n",
        "    if botnet_labels:\n",
        "        print(f\"✅ 標籤二元化完成（預期 Botnet 標籤: {botnet_labels}）\")\n",
        "    else:\n",
        "        print(\"✅ 標籤二元化完成（使用模式匹配：包含 'botnet' 或 'bot' → 1）\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 錯誤: {e}\")\n",
        "\n",
        "# 步驟 4: 顯示結果（使用 sample 避免 shuffle）\n",
        "print(\"\\n[步驟 4] 顯示二元化結果（使用樣本）...\")\n",
        "try:\n",
        "    # 使用 sample 來顯示結果，避免對整個資料集進行 groupBy\n",
        "    sample_result = df.sample(False, 0.01, seed=42)\n",
        "    \n",
        "    print(\"\\n二元化標籤分布摘要（樣本）：\")\n",
        "    sample_result.groupBy(\"label_binary\").count().orderBy(\"label_binary\").show()\n",
        "    \n",
        "    print(\"\\n原始標籤與二元化標籤對應（樣本，前 10 個）：\")\n",
        "    sample_result.groupBy(\"Label\", \"label_binary\").count().orderBy(\"count\", ascending=False).show(10, truncate=False)\n",
        "    \n",
        "    # 使用簡單的 count 來顯示整體分布（不需要 shuffle）\n",
        "    print(\"\\n整體二元化標籤分布（使用近似計數）：\")\n",
        "    total_count = df.count()\n",
        "    botnet_count = df.filter(col(\"label_binary\") == 1).count()\n",
        "    normal_count = total_count - botnet_count\n",
        "    print(f\"   總筆數: {total_count:,}\")\n",
        "    print(f\"   異常 (label_binary=1): {botnet_count:,} ({botnet_count/total_count*100:.2f}%)\")\n",
        "    print(f\"   正常 (label_binary=0): {normal_count:,} ({normal_count/total_count*100:.2f}%)\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"✅ 標籤二元化處理完成！\")\n",
        "    print(\"=\" * 60)\n",
        "except Exception as e:\n",
        "    print(f\"❌ 錯誤: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 時間欄位處理\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 確保時間欄位是 timestamp 類型 (抽樣檢查即可)\n",
        "# capture20110817_cleaned_spark.parquet 已經將 datetime 轉換為字串，讓 Spark 可以正確讀取\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"時間欄位處理\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if \"Date_Flow_Start\" in df.columns:\n",
        "    df = df.withColumn(\"timestamp\", to_timestamp(col(\"Date_Flow_Start\")))\n",
        "    print(\"✅ 時間欄位轉換完成\")\n",
        "    \n",
        "    # 顯示樣本（不觸發 shuffle，快速）\n",
        "    print(\"\\n時間資料樣本（前 10 筆）：\")\n",
        "    time_samples = df.select(\"timestamp\").head(10)\n",
        "    for i, row in enumerate(time_samples, 1):\n",
        "        print(f\"   {i:2d}. {row['timestamp']}\")\n",
        "    \n",
        "    # 使用較大樣本進行時間範圍估算（不觸發 shuffle）\n",
        "    print(\"\\n時間範圍估算（基於前 10000 筆樣本）：\")\n",
        "    try:\n",
        "        sample_size = 10000\n",
        "        time_samples = df.select(\"timestamp\").head(sample_size)\n",
        "        \n",
        "        if time_samples:\n",
        "            import pandas as pd\n",
        "            timestamps = [row['timestamp'] for row in time_samples if row['timestamp'] is not None]\n",
        "            \n",
        "            if timestamps:\n",
        "                timestamps_pd = pd.Series(timestamps)\n",
        "                print(f\"   樣本大小: {len(timestamps)} 筆\")\n",
        "                print(f\"   樣本最早時間: {timestamps_pd.min()}\")\n",
        "                print(f\"   樣本最晚時間: {timestamps_pd.max()}\")\n",
        "                time_span = timestamps_pd.max() - timestamps_pd.min()\n",
        "                print(f\"   樣本時間跨度: {time_span}\")\n",
        "                print(f\"\\n   ⚠️ 注意：這是樣本統計，不是完整資料集的統計\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠️ 計算樣本統計時發生錯誤: {e}\")\n",
        "    \n",
        "    print(\"\\n✅ 時間欄位處理完成！\")\n",
        "else:\n",
        "    print(\"⚠️ 找不到 Date_Flow_Start 欄位\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 時間窗口聚合特徵工程\n",
        "\n",
        "### 4.1 基礎統計特徵（Source IP × 1 分鐘窗口）\n",
        "原始資料：\n",
        "- 每個 NetFlow 記錄 = 1 筆\n",
        "- 例如：IP A 在 12:01:00-12:02:00 有 100 個連接 → 100 筆原始記錄\n",
        "\n",
        "特徵表：\n",
        "- 每個 (src_ip, 1分鐘窗口) = 1 筆特徵\n",
        "- 例如：IP A 在 12:01:00-12:02:00 → 1 筆特徵（聚合了 100 個連接）\n",
        "\n",
        "壓縮比 = 原始筆數 / 特徵筆數\n",
        "       = 8,087,512 / 1,650,021\n",
        "       ≈ 4.9\n",
        "這表示平均每個 (src_ip, 1分鐘窗口) 約有 4.9 筆原始記錄"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 以 Source IP + 1 分鐘窗口進行聚合\n",
        "# 計算：Flow Count, Total Bytes, Total Packets, Average Duration\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"基礎統計特徵計算\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 步驟 1: 檢查必要的欄位是否存在\n",
        "print(\"\\n[步驟 1] 檢查必要欄位...\")\n",
        "required_columns = [\"src_ip\", \"timestamp\", \"Bytes\", \"Packets\", \"Duration\", \"label_binary\"]\n",
        "missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "if missing_columns:\n",
        "    print(f\"❌ 缺少必要欄位: {missing_columns}\")\n",
        "    print(f\"   當前欄位: {df.columns}\")\n",
        "    raise ValueError(f\"缺少必要欄位: {missing_columns}\")\n",
        "else:\n",
        "    print(f\"✅ 所有必要欄位都存在: {required_columns}\")\n",
        "\n",
        "# 步驟 2: 檢查 timestamp 欄位的型別\n",
        "print(\"\\n[步驟 2] 檢查 timestamp 欄位型別...\")\n",
        "try:\n",
        "    timestamp_type = df.schema[\"timestamp\"].dataType\n",
        "    print(f\"   timestamp 型別: {timestamp_type}\")\n",
        "    if \"Timestamp\" not in str(timestamp_type):\n",
        "        print(\"   ⚠️ 警告: timestamp 欄位可能不是 TimestampType\")\n",
        "except Exception as e:\n",
        "    print(f\"   ❌ 檢查 timestamp 欄位時發生錯誤: {e}\")\n",
        "\n",
        "# 步驟 3: 測試 window() 函數\n",
        "print(\"\\n[步驟 3] 測試 window() 函數...\")\n",
        "try:\n",
        "    from pyspark.sql.functions import window\n",
        "    test_window = window(col(\"timestamp\"), \"1 minute\")\n",
        "    print(\"✅ window() 函數可用\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ window() 函數不可用: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# 步驟 4: 執行 groupBy 和聚合\n",
        "print(\"\\n[步驟 4] 執行 groupBy 和聚合（這可能需要一些時間）...\")\n",
        "try:\n",
        "    features_base = df.groupBy(\n",
        "        \"src_ip\",\n",
        "        window(col(\"timestamp\"), \"1 minute\").alias(\"time_window\")\n",
        "    ).agg(\n",
        "        count(\"*\").alias(\"flow_count\"),  # 流量數量\n",
        "        spark_sum(\"Bytes\").alias(\"total_bytes\"),  # 總位元組數\n",
        "        spark_sum(\"Packets\").alias(\"total_packets\"),  # 總封包數\n",
        "        avg(\"Duration\").alias(\"avg_duration\"),  # 平均持續時間\n",
        "        spark_min(\"Duration\").alias(\"min_duration\"),\n",
        "        spark_max(\"Duration\").alias(\"max_duration\"),\n",
        "        # ==========================================\n",
        "        # [新增] 聚合 Label 作為ML模型訓練的標籤 (aka 答案)\n",
        "        # 如果窗口內有任一 Botnet 流量 (1)，則該窗口視為異常 (1)\n",
        "        spark_max(\"label_binary\").alias(\"label\") \n",
        "        # ==========================================\n",
        "    )\n",
        "    print(\"✅ groupBy 和聚合操作完成\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ groupBy 和聚合操作失敗: {e}\")\n",
        "    print(f\"   錯誤類型: {type(e).__name__}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# 步驟 5: 計算特徵表筆數（這會觸發 action）\n",
        "print(\"\\n[步驟 5] 計算特徵表筆數（這可能需要一些時間）...\")\n",
        "try:\n",
        "    feature_count = features_base.count()\n",
        "    print(f\"✅ 基礎統計特徵計算完成\")\n",
        "    print(f\"   特徵表筆數: {feature_count:,}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 計算特徵表筆數時發生錯誤: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# 步驟 6: 顯示前 5 筆特徵\n",
        "print(\"\\n[步驟 6] 顯示前 5 筆特徵...\")\n",
        "try:\n",
        "    print(\"\\n前 5 筆特徵：\")\n",
        "    features_base.show(5, truncate=False)\n",
        "    print(\"\\n✅ 所有操作完成！\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 顯示特徵時發生錯誤: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"基礎統計特徵計算完成！\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Diversity 特徵\n",
        "#### Diversity 特徵的意義：計算不同值的數量（countDistinct）\n",
        "- dst_ip_diversity（目標 IP 多樣性）：\n",
        "    - 高值：短時間內連接多個不同目標 IP，可能表示：掃描行為、DDoS 攻擊\n",
        "- dst_port_diversity（目標 Port 多樣性）：\n",
        "  - 高值：短時間內連接多個不同 Port，可能表示：端口掃描行為\n",
        "- protocol_diversity（協定多樣性）：\n",
        "  - 高值：使用多種不同協定，可能表示：複雜的網路行為\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 功能：資料預處理\n",
        "# 拆分 IP:Port 欄位\n",
        "# 處理只有 IP 沒有 Port 的情況，以及 Port 為非數字的情況\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"拆分 IP:Port 欄位（安全版本 - 使用 try_cast）\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from pyspark.sql.functions import split, size, when, expr\n",
        "\n",
        "print(\"\\n[步驟 1] 拆分 Source IP:Port...\")\n",
        "try:\n",
        "    # 使用 try_cast，如果轉換失敗會返回 NULL\n",
        "    df = df.withColumn(\n",
        "        \"src_ip\", \n",
        "        split(col(\"Src_IP_Port\"), \":\").getItem(0)\n",
        "    ).withColumn(\n",
        "        \"src_port\",\n",
        "        when(\n",
        "            size(split(col(\"Src_IP_Port\"), \":\")) > 1,\n",
        "            expr(\"try_cast(split(Src_IP_Port, ':')[1] as int)\")\n",
        "        ).otherwise(None)\n",
        "    )\n",
        "    print(\"✅ Source IP:Port 拆分完成\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 拆分 Source IP:Port 時發生錯誤: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "print(\"\\n[步驟 2] 拆分 Destination IP:Port...\")\n",
        "try:\n",
        "    df = df.withColumn(\n",
        "        \"dst_ip\",\n",
        "        split(col(\"Dst_IP_Port\"), \":\").getItem(0)\n",
        "    ).withColumn(\n",
        "        \"dst_port\",\n",
        "        when(\n",
        "            size(split(col(\"Dst_IP_Port\"), \":\")) > 1,\n",
        "            expr(\"try_cast(split(Dst_IP_Port, ':')[1] as int)\")\n",
        "        ).otherwise(None)\n",
        "    )\n",
        "    print(\"✅ Destination IP:Port 拆分完成\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 拆分 Destination IP:Port 時發生錯誤: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "print(\"\\n[步驟 3] 檢查拆分結果...\")\n",
        "try:\n",
        "    # 統計 NULL 值\n",
        "    src_port_null = df.filter(col(\"src_port\").isNull()).count()\n",
        "    dst_port_null = df.filter(col(\"dst_port\").isNull()).count()\n",
        "    \n",
        "    print(f\"   src_port NULL 值: {src_port_null:,}\")\n",
        "    print(f\"   dst_port NULL 值: {dst_port_null:,}\")\n",
        "    \n",
        "    # 顯示拆分結果樣本\n",
        "    print(\"\\n   拆分結果樣本（前 10 筆）：\")\n",
        "    samples = df.select(\n",
        "        \"Src_IP_Port\", \"src_ip\", \"src_port\",\n",
        "        \"Dst_IP_Port\", \"dst_ip\", \"dst_port\"\n",
        "    ).head(10)\n",
        "    for i, row in enumerate(samples, 1):\n",
        "        print(f\"   {i}. Src: {row['Src_IP_Port']} -> {row['src_ip']}:{row['src_port']}\")\n",
        "        print(f\"      Dst: {row['Dst_IP_Port']} -> {row['dst_ip']}:{row['dst_port']}\")\n",
        "    \n",
        "    # 顯示有 NULL port 的樣本（包括無法轉換的）\n",
        "    if dst_port_null > 0:\n",
        "        print(f\"\\n   有 NULL port 的樣本（前 5 筆）：\")\n",
        "        null_samples = df.filter(col(\"dst_port\").isNull()).select(\n",
        "            \"Dst_IP_Port\", \"dst_ip\", \"dst_port\"\n",
        "        ).head(5)\n",
        "        for i, row in enumerate(null_samples, 1):\n",
        "            print(f\"   {i}. {row['Dst_IP_Port']} -> {row['dst_ip']}:{row['dst_port']}\")\n",
        "    \n",
        "    print(\"\\n✅ IP:Port 欄位拆分完成！\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 檢查拆分結果時發生錯誤: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"拆分完成\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 計算每個 Source IP 在每個時間窗口內的 Diversity\n",
        "# - Destination IP Diversity: 目標 IP 的多樣性（掃描行為指標）\n",
        "# - Destination Port Diversity: 目標 Port 的多樣性（端口掃描指標）\n",
        "# - Protocol Diversity: 協定的多樣性（協議使用多樣性）\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Diversity 特徵計算\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 步驟 1: 檢查必要欄位\n",
        "print(\"\\n[步驟 1] 檢查必要欄位...\")\n",
        "required_columns = [\"src_ip\", \"timestamp\", \"dst_ip\", \"dst_port\", \"Prot\"]\n",
        "missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "if missing_columns:\n",
        "    print(f\"❌ 缺少必要欄位: {missing_columns}\")\n",
        "    print(f\"   當前欄位: {df.columns}\")\n",
        "    raise ValueError(f\"缺少必要欄位: {missing_columns}\")\n",
        "else:\n",
        "    print(f\"✅ 所有必要欄位都存在: {required_columns}\")\n",
        "\n",
        "# 步驟 2: 檢查並清理資料（過濾掉 NULL 值，避免 countDistinct 問題）\n",
        "print(\"\\n[步驟 2] 檢查並清理資料...\")\n",
        "try:\n",
        "    # 檢查 NULL 值數量\n",
        "    null_counts = df.filter(\n",
        "        (col(\"dst_ip\").isNull()) | \n",
        "        (col(\"dst_port\").isNull()) | \n",
        "        (col(\"Prot\").isNull())\n",
        "    ).count()\n",
        "    \n",
        "    if null_counts > 0:\n",
        "        print(f\"   ⚠️ 發現 {null_counts:,} 筆資料有 NULL 值\")\n",
        "        print(f\"   注意：countDistinct 會自動忽略 NULL 值，不需要過濾\")\n",
        "    else:\n",
        "        print(\"   ✅ 沒有發現 NULL 值\")\n",
        "    \n",
        "    # countDistinct 會自動忽略 NULL 值，所以不需要過濾\n",
        "    df_clean = df\n",
        "    print(f\"   使用資料筆數: {df_clean.count():,}\")\n",
        "except Exception as e:\n",
        "    print(f\"   ⚠️ 檢查資料時發生錯誤: {e}\")\n",
        "    df_clean = df\n",
        "\n",
        "# 步驟 3: 計算 Diversity 特徵\n",
        "print(\"\\n[步驟 3] 計算 Diversity 特徵（這可能需要一些時間）...\")\n",
        "try:\n",
        "    features_diversity = df_clean.groupBy(\n",
        "        \"src_ip\",\n",
        "        window(col(\"timestamp\"), \"1 minute\").alias(\"time_window\")\n",
        "    ).agg(\n",
        "        countDistinct(\"dst_ip\").alias(\"dst_ip_diversity\"),      # 目標 IP 多樣性\n",
        "        countDistinct(\"dst_port\").alias(\"dst_port_diversity\"),  # 目標 Port 多樣性\n",
        "        countDistinct(\"Prot\").alias(\"protocol_diversity\")       # 協定多樣性\n",
        "    )\n",
        "    print(\"✅ groupBy 和聚合操作完成\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 計算 Diversity 特徵時發生錯誤: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# 步驟 4: 計算特徵表筆數\n",
        "print(\"\\n[步驟 4] 計算特徵表筆數（這可能需要一些時間）...\")\n",
        "try:\n",
        "    diversity_count = features_diversity.count()\n",
        "    print(\"✅ Diversity 特徵計算完成\")\n",
        "    print(f\"   特徵表筆數: {diversity_count:,}\")\n",
        "    \n",
        "    # 驗證筆數是否與 features_base 一致\n",
        "    if 'features_base' in dir():\n",
        "        base_count = features_base.count()\n",
        "        if diversity_count == base_count:\n",
        "            print(f\"   ✅ 筆數與 features_base 一致: {base_count:,}\")\n",
        "        else:\n",
        "            print(f\"   ⚠️ 筆數與 features_base 不一致:\")\n",
        "            print(f\"      features_base: {base_count:,}\")\n",
        "            print(f\"      features_diversity: {diversity_count:,}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 計算特徵表筆數時發生錯誤: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# 步驟 5: 顯示前 5 筆特徵\n",
        "print(\"\\n[步驟 5] 顯示前 5 筆特徵...\")\n",
        "try:\n",
        "    print(\"\\n前 5 筆 Diversity 特徵：\")\n",
        "    # 使用 head() 避免可能的顯示問題\n",
        "    diversity_samples = features_diversity.head(5)\n",
        "    for i, row in enumerate(diversity_samples, 1):\n",
        "        print(f\"\\n   {i}. src_ip: {row['src_ip']}\")\n",
        "        print(f\"      time_window: {row['time_window']}\")\n",
        "        print(f\"      dst_ip_diversity: {row['dst_ip_diversity']}\")\n",
        "        print(f\"      dst_port_diversity: {row['dst_port_diversity']}\")\n",
        "        print(f\"      protocol_diversity: {row['protocol_diversity']}\")\n",
        "    \n",
        "    # 顯示統計摘要（使用樣本）\n",
        "    print(\"\\nDiversity 特徵統計摘要（基於樣本）...\")\n",
        "    sample_size = 10000\n",
        "    diversity_sample = features_diversity.head(sample_size)\n",
        "    \n",
        "    if diversity_sample:\n",
        "        import pandas as pd\n",
        "        diversity_pd = pd.DataFrame([row.asDict() for row in diversity_sample])\n",
        "        print(\"\\n統計摘要：\")\n",
        "        print(diversity_pd[[\"dst_ip_diversity\", \"dst_port_diversity\", \"protocol_diversity\"]].describe())\n",
        "    \n",
        "    print(\"\\n✅ 所有操作完成！\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 顯示特徵時發生錯誤: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    # 即使顯示失敗，特徵計算可能已經成功\n",
        "    print(\"\\n⚠️ 注意：雖然顯示失敗，但 features_diversity 可能已經成功建立\")\n",
        "    print(\"   可以嘗試: features_diversity.head(5) 來查看資料\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Diversity 特徵計算完成！\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Ratio 特徵\n",
        "原始資料的問題：\n",
        "- 單一 NetFlow 記錄只是一個時間點的流量\n",
        "- 無法直接看出「平均行為模式」\n",
        "- 無法區分「正常的大量流量」和「異常的大量流量」\n",
        "\n",
        "Ratio 特徵：\n",
        "- 時間窗口聚合：將多個流量記錄聚合為一個特徵\n",
        "- 標準化：除以流量數量，消除「流量多 = 異常」的誤判\n",
        "- 捕捉模式：識別異常的流量模式，而不只是流量大小\n",
        "#### Ratio 特徵意義\n",
        "1. bytes_per_flow（平均每個流量的位元組數）\n",
        "意義：\n",
        "- 衡量每個網路連接的平均資料傳輸量\n",
        "- 反映流量的「大小」特徵\n",
        "異常檢測用途：\n",
        "- 異常高值：可能表示\n",
        "  - 大檔案傳輸（正常但值得關注）\n",
        "  - DDoS 攻擊（大量資料傳輸）\n",
        "  - 資料外洩（異常大量資料傳輸）\n",
        "- 異常低值：可能表示\n",
        "    -掃描行為（很多小連接，每個連接資料量很小）\n",
        "    - 心跳包（維持連接的小封包）\n",
        "1. packets_per_flow（平均每個流量的封包數）\n",
        "意義：\n",
        "- 衡量每個網路連接的平均封包數量\n",
        "- 反映流量的「頻率」特徵\n",
        "異常檢測用途：\n",
        "- 異常高值：可能表示\n",
        "  - 頻繁的資料交換（正常但值得關注）\n",
        "  - 攻擊行為（大量小封包）\n",
        "- 異常低值：可能表示\n",
        "  - 掃描行為（每個連接只有少量封包）\n",
        "  - 連接失敗（只有初始封包，沒有後續資料）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 計算 Bytes Ratio 和 Packets Ratio\n",
        "# 這裡我們計算每個 Source IP 的平均 Bytes/Flow 和 Packets/Flow\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Ratio 特徵計算\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 步驟 1: 檢查必要欄位\n",
        "print(\"\\n[步驟 1] 檢查必要欄位...\")\n",
        "required_columns = [\"src_ip\", \"timestamp\", \"Bytes\", \"Packets\"]\n",
        "missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "if missing_columns:\n",
        "    print(f\"❌ 缺少必要欄位: {missing_columns}\")\n",
        "    print(f\"   當前欄位: {df.columns}\")\n",
        "    raise ValueError(f\"缺少必要欄位: {missing_columns}\")\n",
        "else:\n",
        "    print(f\"✅ 所有必要欄位都存在: {required_columns}\")\n",
        "\n",
        "# 步驟 2: 檢查資料是否有 NULL 值\n",
        "print(\"\\n[步驟 2] 檢查資料品質...\")\n",
        "try:\n",
        "    bytes_null = df.filter(col(\"Bytes\").isNull()).count()\n",
        "    packets_null = df.filter(col(\"Packets\").isNull()).count()\n",
        "    print(f\"   Bytes NULL 值: {bytes_null:,}\")\n",
        "    print(f\"   Packets NULL 值: {packets_null:,}\")\n",
        "    \n",
        "    if bytes_null > 0 or packets_null > 0:\n",
        "        print(\"   ⚠️ 發現 NULL 值，聚合時會自動忽略\")\n",
        "except Exception as e:\n",
        "    print(f\"   ⚠️ 檢查資料品質時發生錯誤: {e}\")\n",
        "\n",
        "# 步驟 3: 計算 Ratio 特徵\n",
        "print(\"\\n[步驟 3] 計算 Ratio 特徵（這可能需要一些時間）...\")\n",
        "print(\"   正在執行 groupBy 和聚合操作...\")\n",
        "try:\n",
        "    features_ratio = df.groupBy(\n",
        "        \"src_ip\",\n",
        "        window(col(\"timestamp\"), \"1 minute\").alias(\"time_window\")\n",
        "    ).agg(\n",
        "        (spark_sum(\"Bytes\") / count(\"*\")).alias(\"bytes_per_flow\"),  # 平均每個流量的位元組數\n",
        "        (spark_sum(\"Packets\") / count(\"*\")).alias(\"packets_per_flow\")  # 平均每個流量的封包數\n",
        "    )\n",
        "    print(\"✅ groupBy 和聚合操作完成\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 計算 Ratio 特徵時發生錯誤: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# 步驟 4: 計算特徵表筆數（這會觸發 action）\n",
        "print(\"\\n[步驟 4] 計算特徵表筆數（這可能需要一些時間）...\")\n",
        "print(\"   正在執行 count() 操作...\")\n",
        "try:\n",
        "    ratio_count = features_ratio.count()\n",
        "    print(\"✅ Ratio 特徵計算完成\")\n",
        "    print(f\"   特徵表筆數: {ratio_count:,}\")\n",
        "    \n",
        "    # 驗證筆數是否與 features_base 一致\n",
        "    if 'features_base' in dir():\n",
        "        base_count = features_base.count()\n",
        "        if ratio_count == base_count:\n",
        "            print(f\"   ✅ 筆數與 features_base 一致: {base_count:,}\")\n",
        "        else:\n",
        "            print(f\"   ⚠️ 筆數與 features_base 不一致:\")\n",
        "            print(f\"      features_base: {base_count:,}\")\n",
        "            print(f\"      features_ratio: {ratio_count:,}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 計算特徵表筆數時發生錯誤: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# 步驟 5: 顯示前 5 筆特徵\n",
        "print(\"\\n[步驟 5] 顯示前 5 筆特徵...\")\n",
        "try:\n",
        "    print(\"\\n前 5 筆 Ratio 特徵：\")\n",
        "    # 使用 head() 避免可能的顯示問題\n",
        "    ratio_samples = features_ratio.head(5)\n",
        "    for i, row in enumerate(ratio_samples, 1):\n",
        "        print(f\"\\n   {i}. src_ip: {row['src_ip']}\")\n",
        "        print(f\"      time_window: {row['time_window']}\")\n",
        "        print(f\"      bytes_per_flow: {row['bytes_per_flow']:.2f}\")\n",
        "        print(f\"      packets_per_flow: {row['packets_per_flow']:.2f}\")\n",
        "    \n",
        "    print(\"\\n✅ 所有操作完成！\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 顯示特徵時發生錯誤: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    # 即使顯示失敗，特徵計算可能已經成功\n",
        "    print(\"\\n⚠️ 注意：雖然顯示失敗，但 features_ratio 可能已經成功建立\")\n",
        "    print(\"   可以嘗試: features_ratio.head(5) 來查看資料\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Ratio 特徵計算完成！\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Entropy 特徵\n",
        "#### Entropy 數學意義\n",
        "Entropy 使用 Shannon Entropy 公式計算：\n",
        "\n",
        "$Entropy = -Σ(p(x) * log₂(p(x)))$\n",
        "\n",
        "其中 p(x) 是某個值出現的機率。Entropy 衡量分布的隨機性或多樣性：\n",
        "- 高 Entropy：分布較均勻、多樣性高\n",
        "- 低 Entropy：分布集中、多樣性低\n",
        "\n",
        "#### 特徵意義\n",
        "1. Port Entropy (port_entropy)：衡量目標 Port 分布的隨機性。\n",
        "   - 高 Port Entropy（接近 log₂(不同 port 數量)）：\n",
        "     - 表示 Port 分布較均勻\n",
        "     - 可能表示：端口掃描、隨機連接嘗試、異常行為\n",
        "   - 低 Port Entropy（接近 0）：\n",
        "     - 表示集中在少數 Port\n",
        "     - 可能表示：正常服務連接（如只連 80、443）\n",
        "2. Protocol Entropy (protocol_entropy)：衡量協定（TCP、UDP 等）分布的隨機性。\n",
        "   - 高 Protocol Entropy：\n",
        "     - 表示使用多種協定且分布均勻\n",
        "     - 可能表示：複雜攻擊、多協定掃描\n",
        "   - 低 Protocol Entropy：\n",
        "     - 表示主要使用單一協定\n",
        "     - 可能表示：正常服務（如只使用 TCP）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 計算 Port Entropy 和 Protocol Entropy\n",
        "# Entropy = -Σ(p(x) * log2(p(x)))\n",
        "# Entropy 衡量分布的隨機性/多樣性\n",
        "\n",
        "# 使用 Spark SQL 內建函數，避免 UDF 和 collect_list 的記憶體問題\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Entropy 特徵計算（優化版本 - 使用 Spark SQL）\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 步驟 1: 檢查必要欄位\n",
        "print(\"\\n[步驟 1] 檢查必要欄位...\")\n",
        "required_columns = [\"src_ip\", \"timestamp\", \"dst_port\", \"Prot\"]\n",
        "missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "if missing_columns:\n",
        "    print(f\"❌ 缺少必要欄位: {missing_columns}\")\n",
        "    raise ValueError(f\"缺少必要欄位: {missing_columns}\")\n",
        "else:\n",
        "    print(f\"✅ 所有必要欄位都存在: {required_columns}\")\n",
        "\n",
        "# 步驟 2: 計算每個值的出現次數和總數\n",
        "print(\"\\n[步驟 2] 計算每個值的出現次數（這可能需要一些時間）...\")\n",
        "try:\n",
        "    # 先計算每個 (src_ip, time_window, dst_port) 的出現次數\n",
        "    port_counts = df.filter(col(\"dst_port\").isNotNull()).groupBy(\n",
        "        \"src_ip\",\n",
        "        window(col(\"timestamp\"), \"1 minute\").alias(\"time_window\"),\n",
        "        \"dst_port\"\n",
        "    ).agg(\n",
        "        count(\"*\").alias(\"port_count\")\n",
        "    )\n",
        "    \n",
        "    # 計算每個窗口的總數\n",
        "    port_totals = port_counts.groupBy(\"src_ip\", \"time_window\").agg(\n",
        "        spark_sum(\"port_count\").alias(\"total_count\")\n",
        "    )\n",
        "    \n",
        "    # 計算每個窗口的 Entropy\n",
        "    # Entropy = -Σ(p * log2(p)) where p = count / total\n",
        "    port_entropy_df = port_counts.join(\n",
        "        port_totals, [\"src_ip\", \"time_window\"], \"inner\"\n",
        "    ).withColumn(\n",
        "        \"p\", col(\"port_count\") / col(\"total_count\")\n",
        "    ).withColumn(\n",
        "        \"entropy_component\", \n",
        "        -col(\"p\") * log2(col(\"p\"))\n",
        "    ).groupBy(\"src_ip\", \"time_window\").agg(\n",
        "        spark_sum(\"entropy_component\").alias(\"port_entropy\")\n",
        "    )\n",
        "    \n",
        "    print(\"✅ Port Entropy 計算完成\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 計算 Port Entropy 時發生錯誤: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# 步驟 3: 計算 Protocol Entropy\n",
        "print(\"\\n[步驟 3] 計算 Protocol Entropy（這可能需要一些時間）...\")\n",
        "try:\n",
        "    protocol_counts = df.filter(col(\"Prot\").isNotNull()).groupBy(\n",
        "        \"src_ip\",\n",
        "        window(col(\"timestamp\"), \"1 minute\").alias(\"time_window\"),\n",
        "        \"Prot\"\n",
        "    ).agg(\n",
        "        count(\"*\").alias(\"protocol_count\")\n",
        "    )\n",
        "    \n",
        "    protocol_totals = protocol_counts.groupBy(\"src_ip\", \"time_window\").agg(\n",
        "        spark_sum(\"protocol_count\").alias(\"total_count\")\n",
        "    )\n",
        "    \n",
        "    protocol_entropy_df = protocol_counts.join(\n",
        "        protocol_totals, [\"src_ip\", \"time_window\"], \"inner\"\n",
        "    ).withColumn(\n",
        "        \"p\", col(\"protocol_count\") / col(\"total_count\")\n",
        "    ).withColumn(\n",
        "        \"entropy_component\",\n",
        "        -col(\"p\") * log2(col(\"p\"))\n",
        "    ).groupBy(\"src_ip\", \"time_window\").agg(\n",
        "        spark_sum(\"entropy_component\").alias(\"protocol_entropy\")\n",
        "    )\n",
        "    \n",
        "    print(\"✅ Protocol Entropy 計算完成\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 計算 Protocol Entropy 時發生錯誤: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# 步驟 4: 合併兩個 Entropy 特徵\n",
        "print(\"\\n[步驟 4] 合併 Entropy 特徵...\")\n",
        "try:\n",
        "    features_entropy = port_entropy_df.join(\n",
        "        protocol_entropy_df, [\"src_ip\", \"time_window\"], \"outer\"\n",
        "    ).fillna(0.0, subset=[\"port_entropy\", \"protocol_entropy\"])\n",
        "    \n",
        "    print(\"✅ Entropy 特徵合併完成\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 合併 Entropy 特徵時發生錯誤: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# 步驟 5: 驗證結果\n",
        "print(\"\\n[步驟 5] 驗證結果...\")\n",
        "try:\n",
        "    entropy_count = features_entropy.count()\n",
        "    print(f\"✅ Entropy 特徵計算完成\")\n",
        "    print(f\"   特徵表筆數: {entropy_count:,}\")\n",
        "    \n",
        "    # 顯示前 5 筆（使用 head 避免可能的顯示問題）\n",
        "    print(\"\\n前 5 筆 Entropy 特徵：\")\n",
        "    entropy_samples = features_entropy.head(5)\n",
        "    for i, row in enumerate(entropy_samples, 1):\n",
        "        print(f\"\\n   {i}. src_ip: {row['src_ip']}\")\n",
        "        print(f\"      time_window: {row['time_window']}\")\n",
        "        print(f\"      port_entropy: {row['port_entropy']:.4f}\")\n",
        "        print(f\"      protocol_entropy: {row['protocol_entropy']:.4f}\")\n",
        "    \n",
        "    print(\"\\n✅ 所有操作完成！\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 驗證結果時發生錯誤: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Entropy 特徵計算完成！\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 合併所有特徵\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 合併所有特徵表\n",
        "# 使用 src_ip 和 time_window 作為 key\n",
        "\n",
        "# ===== 過濾異常的 src_ip =====\n",
        "print(\"=\" * 60)\n",
        "print(\"過濾異常的 src_ip\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 定義有效的 IP 格式（基本驗證：4 個數字用點分隔）\n",
        "# 過濾掉：NULL、空字串、\"0\"、不符合 IP 格式的值\n",
        "\n",
        "# 定義過濾條件（重複使用）\n",
        "ip_filter = (\n",
        "    col(\"src_ip\").isNotNull() &\n",
        "    (col(\"src_ip\") != \"\") &\n",
        "    (col(\"src_ip\") != \"0\") &\n",
        "    col(\"src_ip\").rlike(r\"^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$\")\n",
        ")\n",
        "\n",
        "print(\"\\n[步驟 1] 過濾 features_base...\")\n",
        "try:\n",
        "    features_base_clean = features_base.filter(ip_filter)\n",
        "    base_before = features_base.count()\n",
        "    base_after = features_base_clean.count()\n",
        "    print(f\"   ✅ features_base: {base_before:,} -> {base_after:,} (過濾 {base_before - base_after:,} 筆)\")\n",
        "except Exception as e:\n",
        "    print(f\"   ❌ features_base 過濾失敗: {e}\")\n",
        "    print(\"   ⚠️ 使用原始 features_base\")\n",
        "    features_base_clean = features_base\n",
        "    base_before = base_after = features_base.count()\n",
        "\n",
        "print(\"\\n[步驟 2] 過濾 features_diversity...\")\n",
        "try:\n",
        "    features_diversity_clean = features_diversity.filter(ip_filter)\n",
        "    diversity_before = features_diversity.count()\n",
        "    diversity_after = features_diversity_clean.count()\n",
        "    print(f\"   ✅ features_diversity: {diversity_before:,} -> {diversity_after:,} (過濾 {diversity_before - diversity_after:,} 筆)\")\n",
        "except Exception as e:\n",
        "    print(f\"   ❌ features_diversity 過濾失敗: {e}\")\n",
        "    print(\"   ⚠️ 使用原始 features_diversity\")\n",
        "    features_diversity_clean = features_diversity\n",
        "    diversity_before = diversity_after = features_diversity.count()\n",
        "\n",
        "print(\"\\n[步驟 3] 過濾 features_ratio...\")\n",
        "try:\n",
        "    features_ratio_clean = features_ratio.filter(ip_filter)\n",
        "    ratio_before = features_ratio.count()\n",
        "    ratio_after = features_ratio_clean.count()\n",
        "    print(f\"   ✅ features_ratio: {ratio_before:,} -> {ratio_after:,} (過濾 {ratio_before - ratio_after:,} 筆)\")\n",
        "except Exception as e:\n",
        "    print(f\"   ❌ features_ratio 過濾失敗: {e}\")\n",
        "    print(\"   ⚠️ 使用原始 features_ratio\")\n",
        "    features_ratio_clean = features_ratio\n",
        "    ratio_before = ratio_after = features_ratio.count()\n",
        "\n",
        "print(\"\\n[步驟 4] 過濾 features_entropy...\")\n",
        "try:\n",
        "    features_entropy_clean = features_entropy.filter(ip_filter)\n",
        "    entropy_before = features_entropy.count()\n",
        "    entropy_after = features_entropy_clean.count()\n",
        "    print(f\"   ✅ features_entropy: {entropy_before:,} -> {entropy_after:,} (過濾 {entropy_before - entropy_after:,} 筆)\")\n",
        "except Exception as e:\n",
        "    print(f\"   ❌ features_entropy 過濾失敗: {e}\")\n",
        "    print(\"   ⚠️ 使用原始 features_entropy\")\n",
        "    features_entropy_clean = features_entropy\n",
        "    entropy_before = entropy_after = features_entropy.count()\n",
        "\n",
        "print(\"\\n✅ 異常值過濾完成\")\n",
        "print(\"=\" * 60)\n",
        "# ===== 過濾結束 =====\n",
        "\n",
        "# 使用過濾後的特徵表進行合併\n",
        "print(\"\\n[步驟 5] 合併所有特徵...\")\n",
        "try:\n",
        "    features_final = features_base_clean \\\n",
        "        .join(features_diversity_clean, [\"src_ip\", \"time_window\"], \"outer\") \\\n",
        "        .join(features_ratio_clean, [\"src_ip\", \"time_window\"], \"outer\") \\\n",
        "        .join(features_entropy_clean, [\"src_ip\", \"time_window\"], \"outer\")\n",
        "    \n",
        "    print(\"✅ 所有特徵合併完成\")\n",
        "    print(f\"   最終特徵表筆數: {features_final.count():,}\")\n",
        "    print(f\"   特徵欄位數: {len(features_final.columns)}\")\n",
        "    print(f\"   特徵欄位: {features_final.columns}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 合併特徵時發生錯誤: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# 驗證沒有異常值\n",
        "print(\"\\n[步驟 6] 驗證過濾結果...\")\n",
        "try:\n",
        "    remaining_abnormal = features_final.filter(\n",
        "        (col(\"src_ip\").isNull()) | \n",
        "        (col(\"src_ip\") == \"\") | \n",
        "        (col(\"src_ip\") == \"0\")\n",
        "    ).count()\n",
        "    \n",
        "    if remaining_abnormal == 0:\n",
        "        print(\"   ✅ 驗證：所有異常值已過濾\")\n",
        "    else:\n",
        "        print(f\"   ⚠️ 警告：仍有 {remaining_abnormal:,} 筆異常值\")\n",
        "except Exception as e:\n",
        "    print(f\"   ⚠️ 驗證時發生錯誤: {e}\")\n",
        "\n",
        "# 使用 head() 代替 show() 避免可能的顯示問題\n",
        "print(\"\\n前 5 筆完整特徵：\")\n",
        "try:\n",
        "    samples = features_final.head(5)\n",
        "    for i, row in enumerate(samples, 1):\n",
        "        print(f\"\\n   {i}. src_ip: {row['src_ip']}\")\n",
        "        print(f\"      time_window: {row['time_window']}\")\n",
        "        flow_count = row['flow_count'] if 'flow_count' in row else None\n",
        "        print(f\"      flow_count: {flow_count}\")\n",
        "        port_ent = row['port_entropy'] if 'port_entropy' in row else None\n",
        "        protocol_ent = row['protocol_entropy'] if 'protocol_entropy' in row else None\n",
        "        print(f\"      port_entropy: {port_ent:.4f if port_ent is not None else 'N/A'}\")\n",
        "        print(f\"      protocol_entropy: {protocol_ent:.4f if protocol_ent is not None else 'N/A'}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 顯示特徵時發生錯誤: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 特徵統計摘要與視覺化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 特徵統計摘要\n",
        "print(\"特徵統計摘要：\")\n",
        "features_final.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 儲存特徵表\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import shutil\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 確保 OUTPUT_PATH 是 Path 物件\n",
        "OUTPUT_PATH = Path(r\"C:\\MyVS\\NetworkAnomalyDetection\\data\\processed\\features.parquet\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"正在使用 Pandas 替代方案儲存至: {OUTPUT_PATH}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    # -------------------------------------------------------\n",
        "    # 檢查並刪除舊的衝突路徑\n",
        "    # -------------------------------------------------------\n",
        "    if OUTPUT_PATH.exists():\n",
        "        print(f\"⚠️ 發現舊的輸出路徑: {OUTPUT_PATH}\")\n",
        "        try:\n",
        "            if OUTPUT_PATH.is_dir():\n",
        "                print(\"   正在刪除舊的 Spark 資料夾...\")\n",
        "                shutil.rmtree(OUTPUT_PATH)  # 強制刪除資料夾\n",
        "            else:\n",
        "                print(\"   正在刪除舊的檔案...\")\n",
        "                os.remove(OUTPUT_PATH)      # 刪除檔案\n",
        "            print(\"   ✅ 舊路徑清理完成\")\n",
        "        except PermissionError:\n",
        "            print(\"   ❌ 無法刪除舊檔，可能被佔用。請手動到檔案總管刪除該資料夾，或重啟 Kernel。\")\n",
        "            raise\n",
        "\n",
        "    # 1. 將 Spark DataFrame 轉為 Pandas DataFrame\n",
        "    # (如果你已經轉過了，變數 pandas_df 還在記憶體裡，可以註解掉下面這一行直接跑第 2 步)\n",
        "    if 'pandas_df' not in locals():\n",
        "        print(\"1. 正在將資料從 Spark 轉換為 Pandas...\")\n",
        "        pandas_df = features_final.toPandas()\n",
        "        print(f\"   ✅ 轉換完成，資料形狀: {pandas_df.shape}\")\n",
        "    else:\n",
        "        print(\"1. Pandas DataFrame 已存在記憶體中，跳過轉換。\")\n",
        "\n",
        "    # 2. 儲存為 Parquet\n",
        "    print(\"2. 正在寫入 Parquet 檔案...\")\n",
        "    # 確保父目錄存在\n",
        "    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # 使用 pyarrow 引擎寫入\n",
        "    pandas_df.to_parquet(str(OUTPUT_PATH), engine='pyarrow', index=False)\n",
        "    \n",
        "    print(f\"✅ 特徵表儲存成功！\")\n",
        "    print(f\"   路徑: {OUTPUT_PATH}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ 儲存失敗: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# 確保 Spark 關閉\n",
        "# try:\n",
        "#     spark.stop()\n",
        "#     print(\"\\n✅ SparkSession 已確認關閉\")\n",
        "# except:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.1 檢查特徵表"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# 讀取 Parquet 檔案\n",
        "file_path = Path(r\"C:\\MyVS\\NetworkAnomalyDetection\\data\\processed\\features.parquet\")\n",
        "df = pd.read_parquet(file_path)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"特徵統計摘要：\")\n",
        "print(\"=\" * 60)\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"資料基本資訊：\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"資料筆數: {len(df):,}\")\n",
        "print(f\"欄位數: {len(df.columns)}\")\n",
        "print(f\"\\n欄位清單:\")\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "# 檢查 label 欄位\n",
        "if 'label' in df.columns:\n",
        "    print(f\"\\n✅ label 欄位存在\")\n",
        "    print(f\"\\nlabel 值分布:\")\n",
        "    print(df['label'].value_counts().sort_index())\n",
        "else:\n",
        "    print(f\"\\n❌ label 欄位不存在\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 結論與下一步\n",
        "\n",
        "### 特徵工程總結\n",
        "\n",
        "1. **時間窗口聚合**：以 Source IP × 1 分鐘窗口進行聚合，將原始 NetFlow 記錄轉換為特徵向量\n",
        "2. **基礎統計特徵**：Flow Count, Total Bytes, Total Packets, Average Duration\n",
        "3. **Diversity 特徵**：Destination IP/Port/Protocol 的多樣性\n",
        "4. **Ratio 特徵**：Bytes/Flow, Packets/Flow 等比例特徵\n",
        "5. **Entropy 特徵**：Port 和 Protocol 分布的熵值\n",
        "\n",
        "### 為何 Raw NetFlow 不可直接進模型？\n",
        "\n",
        "- **資料粒度問題**：原始 NetFlow 是單一流量記錄，缺乏時間上下文\n",
        "- **特徵稀疏性**：單一記錄的特徵不足以捕捉異常行為模式\n",
        "- **時間序列特性**：網路異常通常表現在時間序列模式中（如短時間內大量連接）\n",
        "- **聚合特徵的必要性**：透過時間窗口聚合，我們可以捕捉到：\n",
        "  - 異常的流量模式（高 flow_count）\n",
        "  - 掃描行為（高 dst_ip_diversity, dst_port_diversity）\n",
        "  - 異常的流量比例（異常的 bytes_per_flow）\n",
        "\n",
        "### 下一步\n",
        "\n",
        "- 在 `03_Model_Training_IsoForest_vs_XGB.ipynb` 中使用這些特徵進行模型訓練\n",
        "- 將特徵工程邏輯封裝到 `src/features.py` 以便重複使用\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 關閉 SparkSession\n",
        "spark.stop()\n",
        "print(\"✅ SparkSession 已關閉\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
