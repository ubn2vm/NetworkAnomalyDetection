"""
ç›£ç£å­¸ç¿’æ¨¡å‹éæ“¬åˆæª¢æŸ¥å·¥å…·ï¼ˆXGBoostï¼‰

æª¢æŸ¥ XGBoost ç›£ç£å­¸ç¿’æ¨¡å‹æ˜¯å¦å­˜åœ¨éæ“¬åˆå•é¡Œã€‚
æ”¯æ´å…©ç¨®æ¨¡å¼ï¼š
1. çœŸå¯¦è³‡æ–™æ¨¡å¼ï¼ˆé è¨­ï¼‰ï¼šä½¿ç”¨å°ˆæ¡ˆè³‡æ–™é€²è¡Œå®Œæ•´éæ“¬åˆæª¢æŸ¥
2. å¿«é€Ÿæ¸¬è©¦æ¨¡å¼ï¼šä½¿ç”¨æ¨¡æ“¬è³‡æ–™å¿«é€Ÿé©—è­‰éæ“¬åˆæª¢æ¸¬åŠŸèƒ½

å°æ‡‰è¨“ç·´è…³æœ¬ï¼šscripts/training/train_supervised.py

ä½¿ç”¨æ–¹æ³•ï¼š
    # ä½¿ç”¨çœŸå¯¦è³‡æ–™ï¼ˆé è¨­ï¼‰
    python scripts/evaluation/check_supervised_overfitting.py
    
    # å¿«é€Ÿæ¸¬è©¦æ¨¡å¼ï¼ˆä½¿ç”¨æ¨¡æ“¬è³‡æ–™ï¼‰
    python scripts/evaluation/check_supervised_overfitting.py --quick-test
    python scripts/evaluation/check_supervised_overfitting.py -q
    
    # æŒ‡å®šç‰¹å¾µéšæ®µï¼ˆåƒ…çœŸå¯¦è³‡æ–™æ¨¡å¼ï¼‰
    python scripts/evaluation/check_supervised_overfitting.py --feature-stage 4
"""
import sys
import argparse
from pathlib import Path
from typing import Tuple, Optional, Dict, Any

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥ Python è·¯å¾‘
PROJECT_ROOT = Path(__file__).parent.parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src import (
    DataLoaderFactory,
    DataSourceType,
    ModelFactory,
    ModelType,
    extract_features,
    prepare_feature_set
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import pandas as pd
import numpy as np


def create_synthetic_data(
    n_samples: int = 10000,
    n_features: int = 20,
    random_state: int = 42
) -> Tuple[pd.DataFrame, pd.Series]:
    """
    å‰µå»ºæ¨¡æ“¬è³‡æ–™ï¼ˆç”¨æ–¼å¿«é€Ÿæ¸¬è©¦ï¼‰
    
    Args:
        n_samples: æ¨£æœ¬æ•¸é‡
        n_features: ç‰¹å¾µæ•¸é‡
        random_state: éš¨æ©Ÿç¨®å­
    
    Returns:
        (X, y): ç‰¹å¾µè³‡æ–™å’Œæ¨™ç±¤
    
    >>> X, y = create_synthetic_data(n_samples=100, n_features=5)
    >>> len(X), len(y)
    (100, 100)
    >>> X.shape[1]
    5
    """
    np.random.seed(random_state)
    X = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        columns=[f'feature_{i}' for i in range(n_features)]
    )
    # å‰µå»ºä¸€å€‹ç°¡å–®çš„åˆ†é¡ä»»å‹™
    y = pd.Series((X.iloc[:, 0] > 0).astype(int))
    return X, y


def load_real_data(feature_stage: int = 3) -> Tuple[pd.DataFrame, pd.Series]:
    """
    è¼‰å…¥çœŸå¯¦è³‡æ–™ï¼ˆå®Œæ•´æµç¨‹ï¼‰
    
    Args:
        feature_stage: ç‰¹å¾µéšæ®µï¼ˆ1-4ï¼‰
    
    Returns:
        (X, y): ç‰¹å¾µè³‡æ–™å’Œæ¨™ç±¤
    """
    # 1. è¼‰å…¥è³‡æ–™
    print("\n[æ­¥é©Ÿ 1] è¼‰å…¥è³‡æ–™...")
    loader = DataLoaderFactory.create(DataSourceType.BIDIRECTIONAL_BINETFLOW)
    
    parquet_path = Path("data/processed/capture20110817_cleaned_spark.parquet")
    if parquet_path.exists():
        print(f"   ä½¿ç”¨ Parquet: {parquet_path}")
        raw_df = pd.read_parquet(parquet_path, engine='pyarrow')
    else:
        print("   å¾ CSV è¼‰å…¥...")
        raw_df = loader.load()
    
    print(f"âœ… è¼‰å…¥å®Œæˆï¼š{len(raw_df):,} ç­†è³‡æ–™")
    
    # 2. æ¸…æ´—è³‡æ–™
    print("\n[æ­¥é©Ÿ 2] æ¸…æ´—è³‡æ–™...")
    cleaned_df = loader.clean(raw_df)
    print(f"âœ… æ¸…æ´—å®Œæˆï¼š{len(cleaned_df):,} ç­†è³‡æ–™")
    
    # 3. ç‰¹å¾µå·¥ç¨‹
    print("\n[æ­¥é©Ÿ 3] ç‰¹å¾µå·¥ç¨‹...")
    features_df = extract_features(
        cleaned_df,
        include_time_features=True,
        time_feature_stage=feature_stage
    )
    print(f"âœ… ç‰¹å¾µå·¥ç¨‹å®Œæˆï¼š{features_df.shape[1]} å€‹ç‰¹å¾µ")
    
    # 4. æº–å‚™ç‰¹å¾µ
    print("\n[æ­¥é©Ÿ 4] æº–å‚™ç‰¹å¾µ...")
    X = prepare_feature_set(
        features_df,
        include_base_features=True,
        include_time_features=True,
        time_feature_stage=feature_stage
    )
    
    # ç°¡å–®çš„ç‰¹å¾µé¸æ“‡
    constant_features = [col for col in X.columns if X[col].nunique() <= 1]
    if constant_features:
        X = X[[col for col in X.columns if col not in constant_features]]
    
    print(f"âœ… ç‰¹å¾µæº–å‚™å®Œæˆï¼š{len(X.columns)} å€‹ç‰¹å¾µ")
    
    # 5. æº–å‚™æ¨™ç±¤
    print("\n[æ­¥é©Ÿ 5] æº–å‚™æ¨™ç±¤...")
    if 'Label' not in features_df.columns:
        raise ValueError("âŒ éŒ¯èª¤ï¼šç¼ºå°‘ 'Label' æ¬„ä½")
    
    y = (features_df['Label'].str.contains('Botnet', case=False, na=False)).astype(int)
    print(f"   æ­£å¸¸ (0): {(y == 0).sum():,} ({(y == 0).sum()/len(y)*100:.2f}%)")
    print(f"   ç•°å¸¸ (1): {(y == 1).sum():,} ({(y == 1).sum()/len(y)*100:.2f}%)")
    
    return X, y


def diagnose_overfitting(
    train_metrics: Dict[str, Any],
    X_test: Optional[pd.DataFrame] = None,
    y_test: Optional[pd.Series] = None,
    model = None,
    verbose: bool = True
) -> Dict[str, Any]:
    """
    éæ“¬åˆè¨ºæ–·ï¼ˆå…±ç”¨å‡½æ•¸ï¼‰
    
    Args:
        train_metrics: è¨“ç·´æŒ‡æ¨™å­—å…¸
        X_test: æ¸¬è©¦é›†ç‰¹å¾µï¼ˆå¯é¸ï¼Œç”¨æ–¼æœ€çµ‚æ€§èƒ½è©•ä¼°ï¼‰
        y_test: æ¸¬è©¦é›†æ¨™ç±¤ï¼ˆå¯é¸ï¼‰
        model: è¨“ç·´å¥½çš„æ¨¡å‹ï¼ˆå¯é¸ï¼Œç”¨æ–¼é æ¸¬ï¼‰
        verbose: æ˜¯å¦è¼¸å‡ºè©³ç´°è³‡è¨Š
    
    Returns:
        dict: è¨ºæ–·çµæœ
    
    >>> metrics = {'train_accuracy': 0.95, 'test_accuracy': 0.85, 'accuracy_gap': 0.10, 'overfitting_risk': 'high'}
    >>> result = diagnose_overfitting(metrics, verbose=False)
    >>> result['risk']
    'high'
    """
    if verbose:
        print("\n" + "=" * 60)
        print("ğŸ“Š éæ“¬åˆè¨ºæ–·çµæœ")
        print("=" * 60)
    
    if 'train_accuracy' not in train_metrics or 'test_accuracy' not in train_metrics:
        if verbose:
            print("âŒ ç„¡æ³•ç²å–éæ“¬åˆè¨ºæ–·ä¿¡æ¯")
            print(f"   å¯ç”¨çš„æŒ‡æ¨™ï¼š{list(train_metrics.keys())}")
        return {'error': 'Missing required metrics'}
    
    train_acc = train_metrics['train_accuracy']
    test_acc = train_metrics['test_accuracy']
    gap = train_metrics.get('accuracy_gap', train_acc - test_acc)
    risk = train_metrics.get('overfitting_risk', 'unknown')
    best_iter = train_metrics.get('best_iteration', 'N/A')
    
    result = {
        'train_accuracy': train_acc,
        'test_accuracy': test_acc,
        'accuracy_gap': gap,
        'risk': risk,
        'best_iteration': best_iter
    }
    
    if verbose:
        print(f"\nè¨“ç·´é›†æº–ç¢ºç‡ï¼š{train_acc:.6f} ({train_acc*100:.4f}%)")
        print(f"é©—è­‰é›†æº–ç¢ºç‡ï¼š{test_acc:.6f} ({test_acc*100:.4f}%)")
        print(f"æº–ç¢ºç‡å·®ç•°ï¼š{gap:.6f} ({gap*100:.4f}%)")
        print(f"éæ“¬åˆé¢¨éšªï¼š{risk.upper()}")
        print(f"æœ€ä½³è¿­ä»£æ¬¡æ•¸ï¼š{best_iter}")
        
        print("\n" + "-" * 60)
        if risk == 'high':
            print("âš ï¸  è­¦å‘Šï¼šå­˜åœ¨é«˜éæ“¬åˆé¢¨éšªï¼")
            print("   å»ºè­°ï¼š")
            print("   1. é™ä½ max_depthï¼ˆä¾‹å¦‚å¾ 6 é™åˆ° 4ï¼‰")
            print("   2. å¢åŠ  subsample å’Œ colsample_bytree çš„éš¨æ©Ÿæ€§")
            print("   3. é™ä½ learning_rate ä¸¦å¢åŠ  n_estimators")
            print("   4. å¢åŠ  early_stopping_rounds")
        elif risk == 'medium':
            print("âš ï¸  æ³¨æ„ï¼šå­˜åœ¨ä¸­ç­‰éæ“¬åˆé¢¨éšª")
            print("   å»ºè­°ï¼š")
            print("   1. è€ƒæ…®é™ä½æ¨¡å‹è¤‡é›œåº¦")
            print("   2. å¢åŠ æ­£å‰‡åŒ–åƒæ•¸")
        else:
            print("âœ… éæ“¬åˆé¢¨éšªè¼ƒä½ï¼Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›è‰¯å¥½")
        
        # è¨ˆç®—æ¸¬è©¦é›†æœ€çµ‚æ€§èƒ½ï¼ˆå¦‚æœæœ‰æä¾›ï¼‰
        if X_test is not None and y_test is not None and model is not None:
            print("\n" + "-" * 60)
            print("æœ€çµ‚æ¸¬è©¦é›†æ€§èƒ½ï¼š")
            y_pred_final = model.predict(X_test)
            cm = confusion_matrix(y_test, y_pred_final)
            final_accuracy = (cm[0,0] + cm[1,1]) / cm.sum()
            print(f"   æº–ç¢ºç‡ï¼š{final_accuracy:.6f} ({final_accuracy*100:.4f}%)")
            print(f"   TN: {cm[0,0]:,}, FP: {cm[0,1]:,}")
            print(f"   FN: {cm[1,0]:,}, TP: {cm[1,1]:,}")
            result['final_test_accuracy'] = final_accuracy
            result['confusion_matrix'] = cm.tolist()
    
    return result


def run_quick_test_mode() -> int:
    """
    å¿«é€Ÿæ¸¬è©¦æ¨¡å¼ï¼ˆä½¿ç”¨æ¨¡æ“¬è³‡æ–™ï¼‰
    
    Returns:
        exit_code: 0 è¡¨ç¤ºæˆåŠŸï¼Œé 0 è¡¨ç¤ºå¤±æ•—
    """
    print("=" * 60)
    print("XGBoost éæ“¬åˆæª¢æŸ¥ï¼ˆå¿«é€Ÿæ¸¬è©¦æ¨¡å¼ï¼‰")
    print("=" * 60)
    
    # å‰µå»ºæ¨¡æ“¬è³‡æ–™
    print("\n[æ­¥é©Ÿ 1] å‰µå»ºæ¨¡æ“¬è³‡æ–™...")
    X, y = create_synthetic_data()
    print(f"âœ… è³‡æ–™å‰µå»ºå®Œæˆï¼š{len(X)} ç­†ï¼Œ{len(X.columns)} å€‹ç‰¹å¾µ")
    print(f"   æ¨™ç±¤åˆ†å¸ƒï¼šæ­£å¸¸ {(y == 0).sum()}, ç•°å¸¸ {(y == 1).sum()}")
    
    # åˆ†å‰²è³‡æ–™
    print("\n[æ­¥é©Ÿ 2] åˆ†å‰²è³‡æ–™...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    print(f"âœ… è¨“ç·´é›†ï¼š{len(X_train)} ç­†ï¼Œæ¸¬è©¦é›†ï¼š{len(X_test)} ç­†")
    
    # è¨“ç·´æ¨¡å‹
    print("\n[æ­¥é©Ÿ 3] è¨“ç·´ XGBoost æ¨¡å‹...")
    model = ModelFactory.create(ModelType.XGBOOST)
    
    trained_model, metrics = model.train(
        X_train,
        y_train,
        test_size=0.2,
        random_state=42,
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        early_stopping_rounds=10
    )
    
    # è¨ºæ–·
    diagnose_overfitting(metrics, X_test, y_test, model, verbose=True)
    print("\nâœ… éæ“¬åˆæª¢æ¸¬åŠŸèƒ½æ­£å¸¸é‹ä½œï¼")
    
    return 0


def run_real_data_mode(feature_stage: int = 3) -> int:
    """
    çœŸå¯¦è³‡æ–™æ¨¡å¼ï¼ˆä½¿ç”¨å°ˆæ¡ˆè³‡æ–™ï¼‰
    
    Args:
        feature_stage: ç‰¹å¾µéšæ®µï¼ˆ1-4ï¼‰
    
    Returns:
        exit_code: 0 è¡¨ç¤ºæˆåŠŸï¼Œé 0 è¡¨ç¤ºå¤±æ•—
    """
    print("=" * 60)
    print("XGBoost éæ“¬åˆæª¢æŸ¥ï¼ˆçœŸå¯¦è³‡æ–™æ¨¡å¼ï¼‰")
    print("=" * 60)
    
    try:
        # è¼‰å…¥çœŸå¯¦è³‡æ–™
        X, y = load_real_data(feature_stage=feature_stage)
        
        # 6. åˆ†å‰²è³‡æ–™é›†
        print("\n[æ­¥é©Ÿ 6] åˆ†å‰²è³‡æ–™é›†...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y,
            test_size=0.2,
            random_state=42,
            stratify=y
        )
        print(f"âœ… è¨“ç·´é›†ï¼š{len(X_train):,} ç­†")
        print(f"âœ… æ¸¬è©¦é›†ï¼š{len(X_test):,} ç­†")
        
        # 7. è¨“ç·´æ¨¡å‹ï¼ˆå¸¶éæ“¬åˆè¨ºæ–·ï¼‰
        print("\n[æ­¥é©Ÿ 7] è¨“ç·´ XGBoost æ¨¡å‹ï¼ˆå¸¶éæ“¬åˆè¨ºæ–·ï¼‰...")
        model = ModelFactory.create(ModelType.XGBOOST)
        
        neg_count = (y_train == 0).sum()
        pos_count = (y_train == 1).sum()
        scale_pos_weight = neg_count / pos_count if pos_count > 0 else 1.0
        
        trained_model, train_metrics = model.train(
            X_train,
            y_train,
            test_size=0.2,
            random_state=42,
            scale_pos_weight=scale_pos_weight,
            max_depth=6,
            learning_rate=0.1,
            n_estimators=200,
            subsample=0.8,
            colsample_bytree=0.8,
            early_stopping_rounds=10
        )
        
        # 8. éæ“¬åˆè¨ºæ–·
        diagnose_overfitting(train_metrics, X_test, y_test, model, verbose=True)
        
        print("\n" + "=" * 60)
        return 0
        
    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤ï¼š{e}")
        import traceback
        traceback.print_exc()
        return 1


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(
        description='ç›£ç£å­¸ç¿’æ¨¡å‹éæ“¬åˆæª¢æŸ¥å·¥å…·ï¼ˆXGBoostï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  # ä½¿ç”¨çœŸå¯¦è³‡æ–™ï¼ˆé è¨­ï¼‰
  python scripts/evaluation/check_supervised_overfitting.py
  
  # å¿«é€Ÿæ¸¬è©¦æ¨¡å¼ï¼ˆä½¿ç”¨æ¨¡æ“¬è³‡æ–™ï¼‰
  python scripts/evaluation/check_supervised_overfitting.py --quick-test
  python scripts/evaluation/check_supervised_overfitting.py -q
  
  # æŒ‡å®šç‰¹å¾µéšæ®µï¼ˆåƒ…çœŸå¯¦è³‡æ–™æ¨¡å¼ï¼‰
  python scripts/evaluation/check_supervised_overfitting.py --feature-stage 4
        """
    )
    parser.add_argument(
        '--quick-test',
        '-q',
        action='store_true',
        help='å¿«é€Ÿæ¸¬è©¦æ¨¡å¼ï¼ˆä½¿ç”¨æ¨¡æ“¬è³‡æ–™ï¼‰'
    )
    parser.add_argument(
        '--feature-stage',
        type=int,
        default=3,
        choices=[1, 2, 3, 4],
        help='ç‰¹å¾µéšæ®µï¼ˆåƒ…çœŸå¯¦è³‡æ–™æ¨¡å¼ï¼Œé è¨­ï¼š3ï¼‰'
    )
    
    args = parser.parse_args()
    
    if args.quick_test:
        return run_quick_test_mode()
    else:
        return run_real_data_mode(feature_stage=args.feature_stage)


if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æœªé æœŸçš„éŒ¯èª¤ï¼š{e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

