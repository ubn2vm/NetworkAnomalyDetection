"""
ç‰¹å¾µè½‰æ›æ–¹å¼æ¯”è¼ƒè…³æœ¬

æ¯”è¼ƒå¤šç¨®ç‰¹å¾µè½‰æ›æ–¹å¼ï¼ˆåŸå§‹ã€log1pã€sqrtã€boxcoxã€yeo-johnsonï¼‰ï¼Œ
è‡ªå‹•é¸æ“‡æœ€ä½³è½‰æ›æ–¹å¼ï¼Œç„¶å¾Œæ‡‰ç”¨ RobustScaler æ¨™æº–åŒ–ã€‚

è©•ä¼°æŒ‡æ¨™ï¼š
1. åˆ†é›¢åº¦ï¼ˆCohen's dï¼‰ï¼šæ­£å¸¸èˆ‡ç•°å¸¸æ¨£æœ¬çš„åˆ†é›¢ç¨‹åº¦
2. åˆ†ä½ˆå½¢ç‹€ï¼ˆååº¦ã€å³°åº¦ï¼‰ï¼šæ˜¯å¦æ¥è¿‘æ­£æ…‹åˆ†ä½ˆ
3. ç¶œåˆè©•åˆ†ï¼šè‡ªå‹•é¸æ“‡æœ€ä½³è½‰æ›æ–¹å¼

ä½¿ç”¨æ–¹æ³•ï¼š
# æ¯”è¼ƒæ‰€æœ‰é•·å°¾åˆ†ä½ˆç‰¹å¾µï¼ˆé è¨­ï¼Œè‡ªå‹•ä½¿ç”¨å¤šé€²ç¨‹ä¸¦è¡Œè™•ç†ï¼‰
python scripts/compare_feature_transformations.py

# æ¯”è¼ƒæ‰€æœ‰ç‰¹å¾µä¸¦æ‡‰ç”¨æœ€ä½³è½‰æ›
python scripts/compare_feature_transformations.py --apply-best

# æ¯”è¼ƒå–®å€‹ç‰¹å¾µ
python scripts/compare_feature_transformations.py --feature DstBytes

# æ¯”è¼ƒå–®å€‹ç‰¹å¾µä¸¦æ‡‰ç”¨æœ€ä½³è½‰æ›
python scripts/compare_feature_transformations.py --feature DstBytes --apply-best

# è·³éç”Ÿæˆæ¯”è¼ƒåœ–ï¼ˆåŠ é€Ÿè™•ç†ï¼‰
python scripts/compare_feature_transformations.py --no-plot

# æŒ‡å®šä¸¦è¡Œè™•ç†çš„é€²ç¨‹æ•¸
python scripts/compare_feature_transformations.py --n-jobs 4
"""
import sys
import time
from pathlib import Path
import argparse
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import warnings
import logging
import base64
import platform
from typing import Dict, List, Tuple, Optional
from datetime import datetime
from scipy import stats
from scipy.stats import gaussian_kde
from multiprocessing import Pool, cpu_count
from functools import partial

# è¨­ç½®ä¸­æ–‡å­—é«”
try:
    if platform.system() == 'Windows':
        chinese_fonts = ['Microsoft YaHei', 'SimHei', 'SimSun', 'KaiTi', 'FangSong']
        plt.rcParams['font.sans-serif'] = chinese_fonts + ['DejaVu Sans']
    else:
        plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'Noto Sans CJK SC', 'DejaVu Sans']
except Exception:
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']

plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore', category=UserWarning, message=r'.*Glyph.*missing from font.*')
warnings.filterwarnings('ignore', category=UserWarning, message=r'.*glyph.*U\+.*')
logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)

PROJECT_ROOT = Path(__file__).parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src import (
    StandardFeatureProcessor,
    convert_label_to_binary,
    DEFAULT_SKEWED_FEATURES,
    apply_log_transformation,
    apply_sqrt_transformation,
    apply_boxcox_transformation,
    calculate_transformation_metrics,
    apply_robust_scaling
)


def calculate_cohens_d(normal_values: pd.Series, anomaly_values: pd.Series) -> float:
    """è¨ˆç®— Cohen's dï¼ˆåˆ†é›¢åº¦æŒ‡æ¨™ï¼‰"""
    normal_clean = normal_values.replace([np.inf, -np.inf], np.nan).dropna()
    anomaly_clean = anomaly_values.replace([np.inf, -np.inf], np.nan).dropna()
    
    if len(normal_clean) == 0 or len(anomaly_clean) == 0:
        return 0.0
    
    normal_mean = normal_clean.mean()
    anomaly_mean = anomaly_clean.mean()
    normal_std = normal_clean.std()
    anomaly_std = anomaly_clean.std()
    
    if pd.isna(normal_mean) or pd.isna(anomaly_mean):
        return 0.0
    
    pooled_std = np.sqrt((normal_std**2 + anomaly_std**2) / 2)
    
    if pooled_std == 0:
        return 0.0
    
    cohens_d = abs(normal_mean - anomaly_mean) / pooled_std
    return cohens_d


def sample_data_for_plotting(
    values: pd.Series,
    y_labels: pd.Series,
    max_samples: int = 50000,
    random_state: int = 42
) -> Tuple[pd.Series, pd.Series]:
    """
    å°è³‡æ–™é€²è¡Œåˆ†å±¤æ¡æ¨£ï¼Œç”¨æ–¼ç¹ªåœ–å’Œè¨ˆç®—å„ªåŒ–
    
    Args:
        values: ç‰¹å¾µå€¼
        y_labels: æ¨™ç±¤ï¼ˆ0=æ­£å¸¸, 1=ç•°å¸¸ï¼‰
        max_samples: æœ€å¤§æ¡æ¨£æ•¸é‡
        random_state: éš¨æ©Ÿç¨®å­
    
    Returns:
        (æ¡æ¨£å¾Œçš„å€¼, æ¡æ¨£å¾Œçš„æ¨™ç±¤)
    """
    if len(values) <= max_samples:
        return values, y_labels
    
    # åˆ†å±¤æ¡æ¨£ï¼šä¿æŒæ­£å¸¸å’Œç•°å¸¸æ¨£æœ¬çš„æ¯”ä¾‹
    normal_mask = y_labels == 0
    anomaly_mask = y_labels == 1
    
    normal_values = values[normal_mask]
    anomaly_values = values[anomaly_mask]
    
    normal_count = len(normal_values)
    anomaly_count = len(anomaly_values)
    total_count = normal_count + anomaly_count
    
    # è¨ˆç®—æ¡æ¨£æ¯”ä¾‹
    normal_ratio = normal_count / total_count
    anomaly_ratio = anomaly_count / total_count
    
    normal_samples = min(int(max_samples * normal_ratio), normal_count)
    anomaly_samples = min(int(max_samples * anomaly_ratio), anomaly_count)
    
    # å¦‚æœæ¡æ¨£å¾Œç¸½æ•¸ä¸è¶³ï¼Œèª¿æ•´
    if normal_samples + anomaly_samples < max_samples:
        remaining = max_samples - normal_samples - anomaly_samples
        if normal_count > normal_samples:
            normal_samples += min(remaining, normal_count - normal_samples)
        remaining = max_samples - normal_samples - anomaly_samples
        if anomaly_count > anomaly_samples:
            anomaly_samples += min(remaining, anomaly_count - anomaly_samples)
    
    # éš¨æ©Ÿæ¡æ¨£
    if normal_samples < normal_count:
        normal_indices = normal_values.sample(n=normal_samples, random_state=random_state).index
    else:
        normal_indices = normal_values.index
    
    if anomaly_samples < anomaly_count:
        anomaly_indices = anomaly_values.sample(n=anomaly_samples, random_state=random_state).index
    else:
        anomaly_indices = anomaly_values.index
    
    # åˆä½µæ¡æ¨£çµæœ
    sampled_indices = normal_indices.union(anomaly_indices)
    return values.loc[sampled_indices], y_labels.loc[sampled_indices]


def calculate_density_overlap(normal_values: pd.Series, anomaly_values: pd.Series) -> float:
    """è¨ˆç®—å¯†åº¦åœ–é‡ç–Šåº¦"""
    normal_clean = normal_values.replace([np.inf, -np.inf], np.nan).dropna()
    anomaly_clean = anomaly_values.replace([np.inf, -np.inf], np.nan).dropna()
    
    if len(normal_clean) == 0 or len(anomaly_clean) == 0:
        return 1.0
    
    try:
        normal_kde = gaussian_kde(normal_clean)
        anomaly_kde = gaussian_kde(anomaly_clean)
        
        min_val = min(normal_clean.min(), anomaly_clean.min())
        max_val = max(normal_clean.max(), anomaly_clean.max())
        x_range = np.linspace(min_val, max_val, 1000)
        
        normal_density = normal_kde(x_range)
        anomaly_density = anomaly_kde(x_range)
        
        overlap = np.minimum(normal_density, anomaly_density)
        total_area = np.trapezoid(normal_density, x_range) + np.trapezoid(anomaly_density, x_range)
        overlap_area = np.trapezoid(overlap, x_range)
        
        overlap_ratio = overlap_area / total_area if total_area > 0 else 1.0
        return overlap_ratio
    except:
        return 1.0


def evaluate_transformation(
    feature_name: str,
    values: pd.Series,
    y_labels: pd.Series,
    transformation_name: str
) -> Dict[str, float]:
    """è©•ä¼°å–®å€‹è½‰æ›æ–¹å¼"""
    normal_values = values[y_labels == 0]
    anomaly_values = values[y_labels == 1]
    
    if len(normal_values) == 0 or len(anomaly_values) == 0:
        return {
            'transformation': transformation_name,
            'cohens_d': 0.0,
            'skewness': 0.0,
            'kurtosis': 0.0,
            'is_normal_like': False,
            'density_overlap': 1.0,
            'score': 0.0
        }
    
    # è¨ˆç®—æŒ‡æ¨™
    cohens_d = calculate_cohens_d(normal_values, anomaly_values)
    metrics = calculate_transformation_metrics(values, y_labels)
    density_overlap = calculate_density_overlap(normal_values, anomaly_values)
    
    # ç¶œåˆè©•åˆ†ï¼ˆ0-100ï¼‰
    # åˆ†é›¢åº¦ï¼ˆ50åˆ†ï¼‰+ åˆ†ä½ˆå½¢ç‹€ï¼ˆ30åˆ†ï¼‰+ é‡ç–Šåº¦ï¼ˆ20åˆ†ï¼‰
    separation_score = min(cohens_d / 2.0 * 50, 50)  # Cohen's d > 2.0 å¾—æ»¿åˆ†
    distribution_score = (metrics['is_normal_like'] * 30) + (abs(metrics['skewness']) < 1.0) * 10
    overlap_score = (1.0 - density_overlap) * 20
    
    total_score = separation_score + distribution_score + overlap_score
    
    return {
        'transformation': transformation_name,
        'cohens_d': cohens_d,
        'skewness': metrics['skewness'],
        'kurtosis': metrics['kurtosis'],
        'is_normal_like': metrics['is_normal_like'],
        'density_overlap': density_overlap,
        'separation_score': separation_score,
        'distribution_score': distribution_score,
        'overlap_score': overlap_score,
        'total_score': total_score
    }


def compare_transformations_for_feature(
    feature_name: str,
    original_values: pd.Series,
    y_labels: pd.Series,
    output_dir: Path,
    verbose: bool = True,
    max_samples: int = 50000
) -> Tuple[Dict[str, Dict], Optional[Path], str]:
    """
    æ¯”è¼ƒå–®å€‹ç‰¹å¾µçš„å¤šç¨®è½‰æ›æ–¹å¼
    
    Returns:
        (è©•ä¼°çµæœå­—å…¸, æ¯”è¼ƒåœ–è·¯å¾‘, ç‰¹å¾µåç¨±)
    """
    if verbose:
        print(f"      æ¯”è¼ƒç‰¹å¾µï¼š{feature_name}")
    start_time = time.time()
    
    # æº–å‚™æ•¸æ“š
    df_temp = pd.DataFrame({feature_name: original_values})
    
    # 1. åŸå§‹å€¼
    t0 = time.time()
    original_eval = evaluate_transformation(
        feature_name, original_values, y_labels, 'original'
    )
    if verbose:
        print(f"         åŸå§‹å€¼è©•ä¼°ï¼š{time.time() - t0:.2f} ç§’")
    
    # 2. Log1p è½‰æ›
    t0 = time.time()
    df_log = apply_log_transformation(df_temp, [feature_name], prefix='log_')
    log_values = df_log[f'log_{feature_name}']
    log_eval = evaluate_transformation(
        feature_name, log_values, y_labels, 'log1p'
    )
    if verbose:
        print(f"         Log1p è½‰æ›è©•ä¼°ï¼š{time.time() - t0:.2f} ç§’")
    
    # 3. å¹³æ–¹æ ¹è½‰æ›
    t0 = time.time()
    df_sqrt = apply_sqrt_transformation(df_temp, [feature_name], prefix='sqrt_')
    sqrt_values = df_sqrt[f'sqrt_{feature_name}']
    sqrt_eval = evaluate_transformation(
        feature_name, sqrt_values, y_labels, 'sqrt'
    )
    if verbose:
        print(f"         å¹³æ–¹æ ¹è½‰æ›è©•ä¼°ï¼š{time.time() - t0:.2f} ç§’")
    
    # 4. Box-Cox è½‰æ›ï¼ˆå„ªåŒ–ï¼šä½¿ç”¨æ¡æ¨£ä¾†æ‰¾ lambdaï¼‰
    boxcox_eval = None
    boxcox_values = None
    t0 = time.time()
    try:
        # å°å¤§é‡è³‡æ–™é€²è¡Œæ¡æ¨£ä»¥åŠ é€Ÿ lambda å°‹æ‰¾
        if len(df_temp) > max_samples:
            sampled_df, sampled_labels = sample_data_for_plotting(
                df_temp[feature_name], y_labels, max_samples=max_samples
            )
            df_for_lambda = pd.DataFrame({feature_name: sampled_df})
        else:
            df_for_lambda = df_temp
        
        # åªåœ¨æ¡æ¨£è³‡æ–™ä¸Šæ‰¾ lambda
        _, lambdas = apply_boxcox_transformation(
            df_for_lambda, [feature_name], prefix='boxcox_', method='box-cox'
        )
        
        # ç”¨æ‰¾åˆ°çš„ lambda å°å…¨éƒ¨è³‡æ–™é€²è¡Œè½‰æ›
        if feature_name in lambdas:
            lambda_param = lambdas[feature_name]
            col_data = df_temp[feature_name]
            
            # Box-Cox éœ€è¦æ­£æ•¸
            if col_data.min() <= 0:
                shift = -col_data.min() + 1
                full_shifted = col_data + shift
                full_shifted = full_shifted.clip(lower=1e-10)
            else:
                full_shifted = col_data
                shift = 0
            
            # ä½¿ç”¨æ‰¾åˆ°çš„ lambda å°å…¨éƒ¨è³‡æ–™é€²è¡Œè½‰æ›
            valid_mask = ~col_data.isna()
            boxcox_values = col_data.copy()
            if valid_mask.sum() > 0:
                # ä½¿ç”¨ scipy.stats.boxcox çš„å…§éƒ¨é‚è¼¯æ‰‹å‹•è¨ˆç®—ï¼ˆå› ç‚ºæˆ‘å€‘å·²ç¶“çŸ¥é“ lambdaï¼‰
                shifted_valid = full_shifted[valid_mask]
                if abs(lambda_param) < 1e-10:
                    # lambda = 0 æ™‚ä½¿ç”¨ log
                    boxcox_values.loc[valid_mask] = np.log(shifted_valid)
                else:
                    # æ¨™æº– Box-Cox è½‰æ›å…¬å¼
                    boxcox_values.loc[valid_mask] = (shifted_valid ** lambda_param - 1) / lambda_param
            
            boxcox_eval = evaluate_transformation(
                feature_name, boxcox_values, y_labels, f'boxcox(lambda={lambda_param:.3f})'
            )
        
        if verbose:
            print(f"         Box-Cox è½‰æ›è©•ä¼°ï¼š{time.time() - t0:.2f} ç§’")
    except Exception as e:
        if verbose:
            print(f"         âš ï¸  Box-Cox è½‰æ›å¤±æ•—ï¼š{e} ({time.time() - t0:.2f} ç§’)")
    
    # 5. Yeo-Johnson è½‰æ›ï¼ˆå„ªåŒ–ï¼šä½¿ç”¨æ¡æ¨£ä¾†æ‰¾ lambdaï¼‰
    yeojohnson_eval = None
    yeojohnson_values = None
    t0 = time.time()
    try:
        # å°å¤§é‡è³‡æ–™é€²è¡Œæ¡æ¨£ä»¥åŠ é€Ÿ lambda å°‹æ‰¾
        if len(df_temp) > max_samples:
            sampled_df, sampled_labels = sample_data_for_plotting(
                df_temp[feature_name], y_labels, max_samples=max_samples
            )
            df_for_lambda = pd.DataFrame({feature_name: sampled_df})
        else:
            df_for_lambda = df_temp
        
        # åªåœ¨æ¡æ¨£è³‡æ–™ä¸Šæ‰¾ lambda
        _, lambdas_yj = apply_boxcox_transformation(
            df_for_lambda, [feature_name], prefix='yeoj_', method='yeo-johnson'
        )
        
        # ç”¨æ‰¾åˆ°çš„ lambda å°å…¨éƒ¨è³‡æ–™é€²è¡Œè½‰æ›
        if feature_name in lambdas_yj:
            from sklearn.preprocessing import PowerTransformer
            lambda_param = lambdas_yj[feature_name]
            pt = PowerTransformer(method='yeo-johnson', standardize=False)
            # åœ¨æ¡æ¨£è³‡æ–™ä¸Š fit ä»¥ç²å– transformer
            pt.fit(df_for_lambda[[feature_name]].values)
            # å°å…¨éƒ¨è³‡æ–™é€²è¡Œè½‰æ›
            full_col_2d = df_temp[[feature_name]].values
            yeojohnson_values = pd.Series(
                pt.transform(full_col_2d).flatten(),
                index=df_temp.index
            )
            
            yeojohnson_eval = evaluate_transformation(
                feature_name, yeojohnson_values, y_labels, f'yeo-johnson(lambda={lambda_param:.3f})'
            )
        
        if verbose:
            print(f"         Yeo-Johnson è½‰æ›è©•ä¼°ï¼š{time.time() - t0:.2f} ç§’")
    except Exception as e:
        if verbose:
            print(f"         âš ï¸  Yeo-Johnson è½‰æ›å¤±æ•—ï¼š{e} ({time.time() - t0:.2f} ç§’)")
    
    # æ”¶é›†æ‰€æœ‰è©•ä¼°çµæœ
    all_evaluations = {
        'original': original_eval,
        'log1p': log_eval,
        'sqrt': sqrt_eval
    }
    
    if boxcox_eval:
        all_evaluations['boxcox'] = boxcox_eval
    if yeojohnson_eval:
        all_evaluations['yeo-johnson'] = yeojohnson_eval
    
    # é¸æ“‡æœ€ä½³è½‰æ›æ–¹å¼
    best_transformation = max(all_evaluations.items(), key=lambda x: x[1]['total_score'])
    if verbose:
        print(f"         âœ… æœ€ä½³è½‰æ›ï¼š{best_transformation[0]} (åˆ†æ•¸ï¼š{best_transformation[1]['total_score']:.1f}/100)")
    
    # ç”Ÿæˆæ¯”è¼ƒåœ–ï¼ˆå¦‚æœæ²’æœ‰ç¦ç”¨ï¼‰
    image_path = None
    no_plot = getattr(compare_transformations_for_feature, '_no_plot', False)
    if not no_plot:
        t0 = time.time()
        image_path = plot_transformation_comparison(
            feature_name,
            original_values,
            log_values,
            sqrt_values,
            boxcox_values,
            yeojohnson_values,
            y_labels,
            all_evaluations,
            output_dir,
            max_plot_samples=max_samples
        )
        if verbose:
            print(f"         ç”Ÿæˆæ¯”è¼ƒåœ–ï¼š{time.time() - t0:.2f} ç§’")
    
    if verbose:
        print(f"       âœ… å®Œæˆï¼š{feature_name} (ç¸½è¨ˆï¼š{time.time() - start_time:.2f} ç§’)")
    
    return all_evaluations, image_path, feature_name


def plot_transformation_comparison(
    feature_name: str,
    original_values: pd.Series,
    log_values: pd.Series,
    sqrt_values: pd.Series,
    boxcox_values: Optional[pd.Series],
    yeojohnson_values: Optional[pd.Series],
    y_labels: pd.Series,
    evaluations: Dict[str, Dict],
    output_dir: Path,
    max_plot_samples: int = 50000
) -> Optional[Path]:
    """
    ç”Ÿæˆè½‰æ›æ–¹å¼æ¯”è¼ƒåœ–
    
    Args:
        max_plot_samples: ç¹ªåœ–æ™‚çš„æœ€å¤§æ¡æ¨£æ•¸é‡ï¼ˆé è¨­ 50000ï¼‰
    """
    try:
        # å°è³‡æ–™é€²è¡Œæ¡æ¨£ä»¥åŠ é€Ÿç¹ªåœ–
        sampled_original, sampled_labels = sample_data_for_plotting(
            original_values, y_labels, max_samples=max_plot_samples
        )
        
        # å°å…¶ä»–è½‰æ›å€¼ä¹Ÿé€²è¡Œå°æ‡‰çš„æ¡æ¨£
        sampled_indices = sampled_original.index
        sampled_log = log_values.loc[sampled_indices] if log_values is not None else None
        sampled_sqrt = sqrt_values.loc[sampled_indices] if sqrt_values is not None else None
        sampled_boxcox = boxcox_values.loc[sampled_indices] if boxcox_values is not None else None
        sampled_yeoj = yeojohnson_values.loc[sampled_indices] if yeojohnson_values is not None else None
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        transformations = [
            ('original', sampled_original, 'åŸå§‹ç‰¹å¾µ'),
            ('log1p', sampled_log, 'Log1p è½‰æ›'),
            ('sqrt', sampled_sqrt, 'å¹³æ–¹æ ¹è½‰æ›'),
            ('boxcox', sampled_boxcox, 'Box-Cox è½‰æ›'),
            ('yeo-johnson', sampled_yeoj, 'Yeo-Johnson è½‰æ›'),
            (None, None, 'çµ±è¨ˆæ‘˜è¦')
        ]
        
        for idx, (trans_name, values, title) in enumerate(transformations):
            ax = axes[idx]
            
            if trans_name is None:
                # çµ±è¨ˆæ‘˜è¦
                ax.axis('off')
                summary_text = "è©•ä¼°çµæœæ‘˜è¦\n\n"
                for name, eval_result in sorted(evaluations.items(), key=lambda x: x[1]['total_score'], reverse=True):
                    summary_text += f"{name}:\n"
                    summary_text += f"  Cohen's d: {eval_result['cohens_d']:.3f}\n"
                    summary_text += f"  ååº¦: {eval_result['skewness']:.3f}\n"
                    summary_text += f"  å³°åº¦: {eval_result['kurtosis']:.3f}\n"
                    summary_text += f"  ç¸½åˆ†: {eval_result['total_score']:.1f}/100\n\n"
                
                best = max(evaluations.items(), key=lambda x: x[1]['total_score'])
                summary_text += f"æ¨è–¦ï¼š{best[0]}"
                ax.text(0.1, 0.5, summary_text, transform=ax.transAxes, fontsize=9,
                       verticalalignment='center', family='monospace')
                continue
            
            if values is None:
                ax.text(0.5, 0.5, 'è½‰æ›å¤±æ•—', ha='center', va='center', transform=ax.transAxes)
                ax.set_title(title, fontsize=10)
                continue
            
            # ç¹ªè£½å¯†åº¦åœ–ï¼ˆä½¿ç”¨æ¡æ¨£å¾Œçš„è³‡æ–™ï¼‰
            normal_values = values[sampled_labels == 0].dropna()
            anomaly_values = values[sampled_labels == 1].dropna()
            
            if len(normal_values) > 0 and len(anomaly_values) > 0:
                try:
                    normal_kde = gaussian_kde(normal_values)
                    anomaly_kde = gaussian_kde(anomaly_values)
                    
                    x_range = np.linspace(
                        min(normal_values.min(), anomaly_values.min()),
                        max(normal_values.max(), anomaly_values.max()),
                        1000
                    )
                    
                    ax.plot(x_range, normal_kde(x_range), label='æ­£å¸¸', color='blue', linewidth=2)
                    ax.plot(x_range, anomaly_kde(x_range), label='ç•°å¸¸', color='red', linewidth=2)
                    ax.fill_between(x_range, normal_kde(x_range), alpha=0.3, color='blue')
                    ax.fill_between(x_range, anomaly_kde(x_range), alpha=0.3, color='red')
                except:
                    # å¦‚æœ KDE å¤±æ•—ï¼Œä½¿ç”¨ç›´æ–¹åœ–
                    ax.hist(normal_values, bins=50, alpha=0.6, label='æ­£å¸¸', color='blue', density=True)
                    ax.hist(anomaly_values, bins=50, alpha=0.6, label='ç•°å¸¸', color='red', density=True)
            
            # æ·»åŠ è©•ä¼°æŒ‡æ¨™åˆ°æ¨™é¡Œ
            if trans_name in evaluations:
                eval_result = evaluations[trans_name]
                title += f"\nCohen's d={eval_result['cohens_d']:.3f}, åˆ†æ•¸={eval_result['total_score']:.1f}"
            
            ax.set_title(title, fontsize=10)
            ax.set_xlabel('å€¼', fontsize=9)
            ax.set_ylabel('å¯†åº¦', fontsize=9)
            ax.legend(fontsize=8)
            ax.grid(True, alpha=0.3)
        
        plt.suptitle(f'ç‰¹å¾µè½‰æ›æ–¹å¼æ¯”è¼ƒï¼š{feature_name} (æ¡æ¨£ï¼š{len(sampled_original):,}/{len(original_values):,})', 
                    fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        safe_feature_name = feature_name.replace('/', '_').replace('\\', '_')
        image_path = output_dir / f"comparison_{safe_feature_name}.png"
        plt.savefig(image_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        return image_path
    except Exception as e:
        print(f"         âš ï¸  ç”Ÿæˆæ¯”è¼ƒåœ–å¤±æ•—ï¼š{e}")
        return None


def apply_best_transformation_and_scale(
    features_df: pd.DataFrame,
    best_transformations: Dict[str, str],
    y_labels: pd.Series
) -> Tuple[pd.DataFrame, Dict[str, any], List[str]]:
    """
    æ‡‰ç”¨æœ€ä½³è½‰æ›æ–¹å¼ä¸¦é€²è¡Œ RobustScaler æ¨™æº–åŒ–
    
    Returns:
        (è½‰æ›ä¸¦æ¨™æº–åŒ–å¾Œçš„ DataFrame, scaler å­—å…¸, è½‰æ›å¾Œçš„ç‰¹å¾µåˆ—å)
    """
    result_df = features_df.copy()
    scalers = {}
    transformed_columns = []
    
    for feature_name, best_trans in best_transformations.items():
        if feature_name not in result_df.columns:
            continue
        
        original_values = result_df[feature_name]
        
        # æ‡‰ç”¨æœ€ä½³è½‰æ›
        if best_trans == 'original':
            transformed_values = original_values
            new_col_name = feature_name
        elif best_trans == 'log1p':
            df_temp = pd.DataFrame({feature_name: original_values})
            df_log = apply_log_transformation(df_temp, [feature_name], prefix='')
            transformed_values = df_log[feature_name]
            new_col_name = feature_name  # æ›¿æ›åŸæ¬„ä½
        elif best_trans == 'sqrt':
            df_temp = pd.DataFrame({feature_name: original_values})
            df_sqrt = apply_sqrt_transformation(df_temp, [feature_name], prefix='')
            transformed_values = df_sqrt[feature_name]
            new_col_name = feature_name
        elif best_trans.startswith('boxcox'):
            df_temp = pd.DataFrame({feature_name: original_values})
            df_boxcox, _ = apply_boxcox_transformation(df_temp, [feature_name], prefix='', method='box-cox')
            if feature_name in df_boxcox.columns:
                transformed_values = df_boxcox[feature_name]
            else:
                transformed_values = original_values
            new_col_name = feature_name
        elif best_trans.startswith('yeo-johnson'):
            df_temp = pd.DataFrame({feature_name: original_values})
            df_yeoj, _ = apply_boxcox_transformation(df_temp, [feature_name], prefix='', method='yeo-johnson')
            if feature_name in df_yeoj.columns:
                transformed_values = df_yeoj[feature_name]
            else:
                transformed_values = original_values
            new_col_name = feature_name
        else:
            transformed_values = original_values
            new_col_name = feature_name
        
        # æ›´æ–° DataFrame
        result_df[new_col_name] = transformed_values
        transformed_columns.append(new_col_name)
    
    # æ‡‰ç”¨ RobustScaler æ¨™æº–åŒ–
    if transformed_columns:
        result_df, robust_scaler = apply_robust_scaling(result_df, transformed_columns)
        scalers['robust'] = robust_scaler
    
    return result_df, scalers, transformed_columns


def main():
    parser = argparse.ArgumentParser(
        description='æ¯”è¼ƒå¤šç¨®ç‰¹å¾µè½‰æ›æ–¹å¼ä¸¦é¸æ“‡æœ€ä½³æ–¹å¼',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        '--feature',
        type=str,
        help='è¦æ¯”è¼ƒçš„å–®å€‹ç‰¹å¾µåç¨±ï¼ˆä¾‹å¦‚ï¼šDstBytesï¼‰ã€‚å¦‚æœä¸æŒ‡å®šï¼Œå‰‡æ¯”è¼ƒæ‰€æœ‰é•·å°¾åˆ†ä½ˆç‰¹å¾µ'
    )
    parser.add_argument(
        '--time-feature-stage',
        type=int,
        default=4,
        choices=[1, 2, 3, 4],
        help='ä½¿ç”¨çš„æ™‚é–“ç‰¹å¾µéšæ®µï¼ˆé è¨­ï¼š4ï¼‰'
    )
    parser.add_argument(
        '--apply-best',
        action='store_true',
        help='æ‡‰ç”¨æœ€ä½³è½‰æ›æ–¹å¼ä¸¦ä¿å­˜çµæœ'
    )
    parser.add_argument(
        '--n-jobs',
        type=int,
        default=None,
        help=f'ä¸¦è¡Œè™•ç†çš„é€²ç¨‹æ•¸ï¼ˆé è¨­ï¼šCPU æ ¸å¿ƒæ•¸ï¼Œç•¶å‰ï¼š{cpu_count()}ï¼‰'
    )
    parser.add_argument(
        '--no-plot',
        action='store_true',
        help='è·³éç”Ÿæˆæ¯”è¼ƒåœ–ï¼ˆåŠ é€Ÿè™•ç†ï¼‰'
    )
    parser.add_argument(
        '--max-samples',
        type=int,
        default=50000,
        help='ç¹ªåœ–å’Œ Box-Cox lambda å°‹æ‰¾æ™‚çš„æœ€å¤§æ¡æ¨£æ•¸é‡ï¼ˆé è¨­ï¼š50000ï¼‰'
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ç‰¹å¾µè½‰æ›æ–¹å¼æ¯”è¼ƒ")
    print("=" * 60)
    
    # è¼‰å…¥æ•¸æ“š
    print("\n[æ­¥é©Ÿ 1] è¼‰å…¥ç‰¹å¾µæ•¸æ“š...")
    processor = StandardFeatureProcessor(time_feature_stage=args.time_feature_stage)
    
    try:
        X_original = processor.load_features()
        print(f"   âœ… åŸå§‹ç‰¹å¾µï¼š{len(X_original):,} ç­†ï¼Œ{len(X_original.columns)} å€‹ç‰¹å¾µ")
        
        # æª¢æŸ¥æ¨™ç±¤
        if 'Label' not in X_original.columns:
            print("   âŒ éŒ¯èª¤ï¼šæ²’æœ‰æ¨™ç±¤ï¼Œç„¡æ³•è¨ˆç®—åˆ†é›¢åº¦æŒ‡æ¨™")
            return 1
        
        X_original = convert_label_to_binary(X_original, verbose=False)
        y_labels = X_original['label_binary']
        print(f"   âœ… æ¨™ç±¤ï¼šæ­£å¸¸ {len(y_labels[y_labels==0]):,} ç­†ï¼Œç•°å¸¸ {len(y_labels[y_labels==1]):,} ç­†")
    except Exception as e:
        print(f"   âŒ è¼‰å…¥åŸå§‹ç‰¹å¾µå¤±æ•—ï¼š{e}")
        return 1
    
    # æ±ºå®šè¦æ¯”è¼ƒçš„ç‰¹å¾µï¼ˆé è¨­æ¯”è¼ƒæ‰€æœ‰é•·å°¾åˆ†ä½ˆç‰¹å¾µï¼‰
    if args.feature:
        features_to_compare = [args.feature]
        print(f"\n   å°‡æ¯”è¼ƒå–®å€‹ç‰¹å¾µï¼š{args.feature}")
    else:
        features_to_compare = [f for f in DEFAULT_SKEWED_FEATURES if f in X_original.columns]
        print(f"\n   å°‡æ¯”è¼ƒæ‰€æœ‰ {len(features_to_compare)} å€‹é•·å°¾åˆ†ä½ˆç‰¹å¾µ")
    
    # è¼¸å‡ºç›®éŒ„
    output_dir = Path("output/visualizations/transformation_comparison")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # éæ¿¾å­˜åœ¨çš„ç‰¹å¾µ
    valid_features = [f for f in features_to_compare if f in X_original.columns]
    if len(valid_features) == 0:
        print("   âŒ æ²’æœ‰å¯ç”¨çš„ç‰¹å¾µé€²è¡Œæ¯”è¼ƒ")
        return 1
    
    # æ±ºå®šæ˜¯å¦ä½¿ç”¨å¤šé€²ç¨‹
    n_jobs = args.n_jobs if args.n_jobs is not None else cpu_count()
    use_parallel = len(valid_features) > 1 and n_jobs > 1
    
    # å¦‚æœç¦ç”¨ç¹ªåœ–ï¼Œè¨­ç½®æ¨™è¨˜
    if args.no_plot:
        compare_transformations_for_feature._no_plot = True
    
    # æ¯”è¼ƒçµæœ
    all_results = []
    best_transformations = {}
    
    print("\n[æ­¥é©Ÿ 2] æ¯”è¼ƒç‰¹å¾µè½‰æ›æ–¹å¼...")
    if use_parallel:
        print(f"   ä½¿ç”¨ {n_jobs} å€‹é€²ç¨‹ä¸¦è¡Œè™•ç† {len(valid_features)} å€‹ç‰¹å¾µ...")
        start_time = time.time()
        
        # æº–å‚™æ•¸æ“šï¼ˆåªå‚³ééœ€è¦çš„éƒ¨åˆ†ï¼Œé¿å…å‚³éæ•´å€‹ DataFrameï¼‰
        feature_data = []
        for feature_name in valid_features:
            feature_data.append({
                'feature_name': feature_name,
                'values': X_original[feature_name].values,
                'y_labels': y_labels.values,
                'output_dir': str(output_dir),
                'verbose': False  # å¤šé€²ç¨‹æ™‚ä¸è¼¸å‡ºè©³ç´°ä¿¡æ¯
            })
        
        # ä¸¦è¡Œè™•ç†
        def process_feature_wrapper(data):
            """åŒ…è£å‡½æ•¸ç”¨æ–¼å¤šé€²ç¨‹"""
            feature_name = data['feature_name']
            original_values = pd.Series(data['values'])
            y_labels = pd.Series(data['y_labels'])
            output_dir = Path(data['output_dir'])
            verbose = data['verbose']
            no_plot = data.get('no_plot', False)
            
            try:
                # è‡¨æ™‚è¨­ç½®å…¨å±€æ¨™è¨˜ï¼ˆç”¨æ–¼è·³éç¹ªåœ–ï¼‰
                if no_plot:
                    compare_transformations_for_feature._no_plot = True
                
                max_samples = data.get('max_samples', 50000)
                evaluations, image_path, _ = compare_transformations_for_feature(
                    feature_name, original_values, y_labels, output_dir, verbose=verbose, max_samples=max_samples
                )
                
                if no_plot:
                    compare_transformations_for_feature._no_plot = False
                
                return (feature_name, evaluations, image_path, None)
            except Exception as e:
                import traceback
                error_msg = f"{str(e)}\n{traceback.format_exc()}"
                return (feature_name, None, None, error_msg)
        
        # æ·»åŠ  no_plot å’Œ max_samples æ¨™è¨˜åˆ°æ•¸æ“šä¸­
        for data in feature_data:
            data['no_plot'] = args.no_plot
            data['max_samples'] = args.max_samples
        
        # ä¸¦è¡Œè™•ç†ï¼ˆWindows å’Œ Linux éƒ½ä½¿ç”¨ç›¸åŒçš„æ–¹å¼ï¼‰
        with Pool(processes=n_jobs) as pool:
            # ä½¿ç”¨ imap ä»¥ä¾¿é¡¯ç¤ºé€²åº¦
            results = []
            for idx, result in enumerate(pool.imap(process_feature_wrapper, feature_data), 1):
                results.append(result)
                feature_name = result[0]
                if result[3] is None:  # æ²’æœ‰éŒ¯èª¤
                    print(f"   [{idx}/{len(valid_features)}] âœ… {feature_name} å®Œæˆ")
                else:
                    print(f"   [{idx}/{len(valid_features)}] âš ï¸  {feature_name} å¤±æ•—")
        
        # æ”¶é›†çµæœ
        for feature_name, evaluations, image_path, error in results:
            if error:
                print(f"   âš ï¸  {feature_name} è™•ç†å¤±æ•—ï¼š{error}")
                continue
            
            if evaluations is None:
                continue
            
            # é¸æ“‡æœ€ä½³è½‰æ›æ–¹å¼
            best_trans = max(evaluations.items(), key=lambda x: x[1]['total_score'])
            best_transformations[feature_name] = best_trans[0]
            
            # ä¿å­˜çµæœ
            for trans_name, eval_result in evaluations.items():
                # å‰µå»ºçµæœå­—å…¸ï¼Œç¢ºä¿ä½¿ç”¨ç°¡çŸ­åç¨±ä½œç‚º transformation
                # å…ˆè¤‡è£½ eval_resultï¼Œç§»é™¤å…¶ä¸­çš„ transformation æ¬„ä½ï¼ˆå› ç‚ºå®ƒå¯èƒ½æ˜¯å®Œæ•´å­—ä¸²ï¼‰
                eval_result_clean = {k: v for k, v in eval_result.items() if k != 'transformation'}
                result_dict = {
                    'feature': feature_name,
                    'transformation': trans_name,  # ä½¿ç”¨ç°¡çŸ­åç¨±ï¼ˆå­—å…¸çš„ keyï¼‰
                    **eval_result_clean
                }
                all_results.append(result_dict)
        
        elapsed_time = time.time() - start_time
        print(f"   âœ… ä¸¦è¡Œè™•ç†å®Œæˆï¼ˆè€—æ™‚ï¼š{elapsed_time:.2f} ç§’ï¼Œå¹³å‡æ¯å€‹ç‰¹å¾µï¼š{elapsed_time/len(valid_features):.2f} ç§’ï¼‰")
    else:
        # å–®é€²ç¨‹è™•ç†ï¼ˆç”¨æ–¼å–®å€‹ç‰¹å¾µæˆ–ç¦ç”¨ä¸¦è¡Œæ™‚ï¼‰
        for idx, feature_name in enumerate(valid_features, 1):
            print(f"\n   [{idx}/{len(valid_features)}] è™•ç†ç‰¹å¾µï¼š{feature_name}")
            original_values = X_original[feature_name]
            
            # æ¯”è¼ƒè½‰æ›æ–¹å¼
            evaluations, image_path, _ = compare_transformations_for_feature(
                feature_name, original_values, y_labels, output_dir, verbose=True, max_samples=args.max_samples
            )
            
            # é¸æ“‡æœ€ä½³è½‰æ›æ–¹å¼
            best_trans = max(evaluations.items(), key=lambda x: x[1]['total_score'])
            best_transformations[feature_name] = best_trans[0]
            
            # ä¿å­˜çµæœ
            for trans_name, eval_result in evaluations.items():
                # å‰µå»ºçµæœå­—å…¸ï¼Œç¢ºä¿ä½¿ç”¨ç°¡çŸ­åç¨±ä½œç‚º transformation
                # å…ˆè¤‡è£½ eval_resultï¼Œç§»é™¤å…¶ä¸­çš„ transformation æ¬„ä½ï¼ˆå› ç‚ºå®ƒå¯èƒ½æ˜¯å®Œæ•´å­—ä¸²ï¼‰
                eval_result_clean = {k: v for k, v in eval_result.items() if k != 'transformation'}
                result_dict = {
                    'feature': feature_name,
                    'transformation': trans_name,  # ä½¿ç”¨ç°¡çŸ­åç¨±ï¼ˆå­—å…¸çš„ keyï¼‰
                    **eval_result_clean
                }
                all_results.append(result_dict)
    
    # ç”Ÿæˆæ‘˜è¦å ±å‘Š
    print("\n[æ­¥é©Ÿ 3] ç”Ÿæˆæ‘˜è¦å ±å‘Š...")
    results_df = pd.DataFrame(all_results)
    
    csv_path = output_dir / "comparison_results.csv"
    results_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    print(f"   âœ… çµæœå·²ä¿å­˜ï¼š{csv_path}")
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 60)
    print("æ¯”è¼ƒçµæœæ‘˜è¦")
    print("=" * 60)
    print("\næ¨è–¦è½‰æ›æ–¹å¼ï¼š")
    for feature_name, best_trans in best_transformations.items():
        feature_results = results_df[results_df['feature'] == feature_name]
        matching_results = feature_results[feature_results['transformation'] == best_trans]
        
        if len(matching_results) == 0:
            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå˜—è©¦ä½¿ç”¨å­—ä¸²åŒ¹é…ï¼ˆè™•ç†å¯èƒ½çš„æ ¼å¼ä¸ä¸€è‡´ï¼‰
            matching_results = feature_results[
                feature_results['transformation'].str.startswith(best_trans, na=False)
            ]
        
        if len(matching_results) > 0:
            best_result = matching_results.iloc[0]
            print(f"  {feature_name:30s} -> {best_trans:15s} "
                  f"(Cohen's d={best_result['cohens_d']:.3f}, "
                  f"ååº¦={best_result['skewness']:.3f}, "
                  f"ç¸½åˆ†={best_result['total_score']:.1f}/100)")
        else:
            print(f"  {feature_name:30s} -> {best_trans:15s} (âš ï¸ ç„¡æ³•æ‰¾åˆ°å°æ‡‰çµæœ)")
    
    # æ‡‰ç”¨æœ€ä½³è½‰æ›æ–¹å¼
    if args.apply_best:
        print("\n[æ­¥é©Ÿ 4] æ‡‰ç”¨æœ€ä½³è½‰æ›æ–¹å¼ä¸¦æ¨™æº–åŒ–...")
        
        # éæ¿¾éç‰¹å¾µæ¬„ä½
        non_feature_columns = ['Label', 'label_binary', 'StartTime', 'SrcAddr', 'DstAddr', 
                              'Sport', 'Dport', 'State', 'Proto']
        feature_columns = [col for col in X_original.columns 
                          if col not in non_feature_columns 
                          and pd.api.types.is_numeric_dtype(X_original[col])]
        
        # åªå°è¦æ¯”è¼ƒçš„ç‰¹å¾µæ‡‰ç”¨æœ€ä½³è½‰æ›ï¼Œå…¶ä»–ç‰¹å¾µä¿æŒåŸæ¨£
        features_to_transform = {k: v for k, v in best_transformations.items() 
                                if k in feature_columns}
        
        # å‰µå»ºè½‰æ›å¾Œçš„ DataFrame
        X_transformed = X_original[feature_columns].copy()
        
        for feature_name, best_trans in features_to_transform.items():
            original_values = X_transformed[feature_name]
            
            if best_trans == 'original':
                continue  # ä¿æŒåŸæ¨£
            elif best_trans == 'log1p':
                df_temp = pd.DataFrame({feature_name: original_values})
                df_log = apply_log_transformation(df_temp, [feature_name], prefix='')
                X_transformed[feature_name] = df_log[feature_name]
            elif best_trans == 'sqrt':
                df_temp = pd.DataFrame({feature_name: original_values})
                df_sqrt = apply_sqrt_transformation(df_temp, [feature_name], prefix='')
                X_transformed[feature_name] = df_sqrt[feature_name]
            elif best_trans.startswith('boxcox'):
                df_temp = pd.DataFrame({feature_name: original_values})
                df_boxcox, _ = apply_boxcox_transformation(df_temp, [feature_name], prefix='', method='box-cox')
                X_transformed[feature_name] = df_boxcox[feature_name]
            elif best_trans.startswith('yeo-johnson'):
                df_temp = pd.DataFrame({feature_name: original_values})
                df_yeoj, _ = apply_boxcox_transformation(df_temp, [feature_name], prefix='', method='yeo-johnson')
                X_transformed[feature_name] = df_yeoj[feature_name]
        
        # æ‡‰ç”¨ RobustScaler æ¨™æº–åŒ–
        X_scaled, robust_scaler = apply_robust_scaling(X_transformed, list(X_transformed.columns))
        
        # ä¿å­˜çµæœ
        scaled_path = output_dir / "best_transformed_features.parquet"
        X_scaled.to_parquet(scaled_path, engine='pyarrow')
        print(f"   âœ… è½‰æ›ä¸¦æ¨™æº–åŒ–å¾Œçš„ç‰¹å¾µå·²ä¿å­˜ï¼š{scaled_path}")
        
        # ä¿å­˜ scaler
        import pickle
        scaler_path = output_dir / "best_transformation_scaler.pkl"
        with open(scaler_path, 'wb') as f:
            pickle.dump(robust_scaler, f)
        print(f"   âœ… Scaler å·²ä¿å­˜ï¼š{scaler_path}")
        
        # ä¿å­˜æœ€ä½³è½‰æ›æ–¹å¼é…ç½®
        import json
        config_path = output_dir / "best_transformations.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(best_transformations, f, indent=2, ensure_ascii=False)
        print(f"   âœ… æœ€ä½³è½‰æ›é…ç½®å·²ä¿å­˜ï¼š{config_path}")
    
    print("\nâœ… å®Œæˆï¼")
    print(f"   çµæœæ–‡ä»¶ï¼š{output_dir}")
    if not args.apply_best:
        print(f"\nğŸ’¡ æç¤ºï¼šä½¿ç”¨ --apply-best åƒæ•¸æ‡‰ç”¨æœ€ä½³è½‰æ›æ–¹å¼ä¸¦ä¿å­˜çµæœ")
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

