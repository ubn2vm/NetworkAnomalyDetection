"""
é¦–æ¬¡è¼‰å…¥è³‡æ–™ä¸¦ç”Ÿæˆ Parquet æª”æ¡ˆ

æ­¤è…³æœ¬ç”¨æ–¼é¦–æ¬¡è¼‰å…¥è³‡æ–™ï¼Œå¾ CSV è®€å–ä¸¦è‡ªå‹•ç”Ÿæˆ Parquet æª”æ¡ˆä»¥ä¾›å¾ŒçºŒå¿«é€Ÿè¼‰å…¥ã€‚
é©åˆåœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ™‚åŸ·è¡Œï¼Œæˆ–éœ€è¦é‡æ–°ç”Ÿæˆ Parquet æª”æ¡ˆæ™‚ä½¿ç”¨ã€‚
"""
import time
import sys
from pathlib import Path

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥ Python è·¯å¾‘
PROJECT_ROOT = Path(__file__).parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src.data_loader import DataLoaderFactory, DataSourceType, get_project_root


def format_time(seconds):
    """æ ¼å¼åŒ–æ™‚é–“é¡¯ç¤º"""
    if seconds < 60:
        return f"{seconds:.2f} ç§’"
    elif seconds < 3600:
        minutes = int(seconds // 60)
        secs = seconds % 60
        return f"{minutes} åˆ† {secs:.2f} ç§’"
    else:
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = seconds % 60
        return f"{hours} å°æ™‚ {minutes} åˆ† {secs:.2f} ç§’"


def format_size(bytes_size):
    """æ ¼å¼åŒ–æª”æ¡ˆå¤§å°é¡¯ç¤º"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if bytes_size < 1024.0:
            return f"{bytes_size:.2f} {unit}"
        bytes_size /= 1024.0
    return f"{bytes_size:.2f} TB"


def main():
    print("=" * 70)
    print("é¦–æ¬¡è¼‰å…¥è³‡æ–™ä¸¦ç”Ÿæˆ Parquet æª”æ¡ˆ")
    print("=" * 70)
    
    project_root = get_project_root()
    csv_path = project_root / "data" / "raw" / "capture20110817.binetflow"
    parquet_path = project_root / "data" / "processed" / "capture20110817_cleaned_spark.parquet"
    
    # 1. æª¢æŸ¥ CSV æª”æ¡ˆ
    print("\n[æ­¥é©Ÿ 1] æª¢æŸ¥åŸå§‹ CSV æª”æ¡ˆ...")
    if not csv_path.exists():
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° CSV æª”æ¡ˆ: {csv_path}")
        print("   è«‹ç¢ºèªæª”æ¡ˆæ˜¯å¦å­˜åœ¨")
        return 1
    
    csv_size = csv_path.stat().st_size
    print(f"âœ… CSV æª”æ¡ˆå­˜åœ¨: {csv_path}")
    print(f"   æª”æ¡ˆå¤§å°: {format_size(csv_size)}")
    
    # 2. æª¢æŸ¥ Parquet æª”æ¡ˆ
    print("\n[æ­¥é©Ÿ 2] æª¢æŸ¥ Parquet æª”æ¡ˆ...")
    if parquet_path.exists():
        parquet_size = parquet_path.stat().st_size
        print(f"âš ï¸  Parquet æª”æ¡ˆå·²å­˜åœ¨: {parquet_path}")
        print(f"   æª”æ¡ˆå¤§å°: {format_size(parquet_size)}")
        
        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ç‚ºç©ºæˆ–æå£
        if parquet_size == 0:
            print(f"   âš ï¸  æª”æ¡ˆå¤§å°ç‚º 0ï¼Œå¯èƒ½æ˜¯æå£çš„æª”æ¡ˆ")
            print(f"   å°‡è‡ªå‹•åˆªé™¤ä¸¦é‡æ–°ç”Ÿæˆ")
            try:
                if parquet_path.is_dir():
                    import shutil
                    shutil.rmtree(parquet_path)
                else:
                    parquet_path.unlink()
                print("âœ… å·²åˆªé™¤æå£çš„ Parquet æª”æ¡ˆ")
            except Exception as e:
                print(f"âŒ åˆªé™¤å¤±æ•—: {e}")
                return 1
        else:
            # åªæœ‰ç•¶æª”æ¡ˆå¤§å°ä¸ç‚º 0 æ™‚æ‰è¨ˆç®—å£“ç¸®æ¯”
            compression_ratio = csv_size / parquet_size if parquet_size > 0 else 0
            print(f"   å£“ç¸®æ¯”: {compression_ratio:.2f}:1")
            
            response = input("\næ˜¯å¦è¦é‡æ–°ç”Ÿæˆ Parquet æª”æ¡ˆï¼Ÿ(y/N): ").strip().lower()
            if response != 'y':
                print("âœ… å–æ¶ˆæ“ä½œï¼Œä½¿ç”¨ç¾æœ‰çš„ Parquet æª”æ¡ˆ")
                print("   å¦‚éœ€è¼‰å…¥è³‡æ–™ï¼Œè«‹ä½¿ç”¨å…¶ä»–è…³æœ¬æˆ– notebook")
                return 0
            
            # åˆªé™¤ç¾æœ‰çš„ Parquet æª”æ¡ˆ
            print("\nğŸ—‘ï¸  åˆªé™¤ç¾æœ‰çš„ Parquet æª”æ¡ˆ...")
            try:
                if parquet_path.is_dir():
                    import shutil
                    shutil.rmtree(parquet_path)
                else:
                    parquet_path.unlink()
                print("âœ… å·²åˆªé™¤èˆŠçš„ Parquet æª”æ¡ˆ")
            except Exception as e:
                print(f"âŒ åˆªé™¤å¤±æ•—: {e}")
                return 1
    else:
        print(f"â„¹ï¸  Parquet æª”æ¡ˆä¸å­˜åœ¨ï¼Œå°‡å¾ CSV ç”Ÿæˆ")
    
    # 3. æª¢æŸ¥ä¾è³´
    print("\n[æ­¥é©Ÿ 3] æª¢æŸ¥ä¾è³´...")
    try:
        import pyarrow
        print(f"âœ… pyarrow å·²å®‰è£ï¼Œç‰ˆæœ¬: {pyarrow.__version__}")
    except ImportError:
        print("âŒ pyarrow æœªå®‰è£")
        print("   è«‹åŸ·è¡Œ: pip install pyarrow")
        return 1
    
    try:
        import pyspark
        print("âœ… pyspark å·²å®‰è£")
    except ImportError:
        print("âŒ pyspark æœªå®‰è£")
        print("   è«‹åŸ·è¡Œ: pip install pyspark")
        return 1
    
    # 4. ç¢ºä¿ç›®éŒ„å­˜åœ¨
    print("\n[æ­¥é©Ÿ 4] æº–å‚™ç›®éŒ„...")
    parquet_path.parent.mkdir(parents=True, exist_ok=True)
    print(f"âœ… ç›®éŒ„å·²æº–å‚™: {parquet_path.parent}")
    
    # 5. è¼‰å…¥è³‡æ–™ï¼ˆå¼·åˆ¶å¾ CSV è®€å–ï¼‰
    print("\n[æ­¥é©Ÿ 5] è¼‰å…¥è³‡æ–™ï¼ˆå¾ CSV è®€å–ï¼‰...")
    print("   é€™å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“ï¼Œè«‹è€å¿ƒç­‰å¾…...")
    print("   ä½¿ç”¨ PySpark å¤šæ ¸å¿ƒåŠ é€Ÿè™•ç†...")
    
    start_time = time.time()
    
    try:
        # å‰µå»º Spark è¼‰å…¥å™¨
        loader = DataLoaderFactory.create(DataSourceType.BIDIRECTIONAL_BINETFLOW_SPARK)
        
        # å¼·åˆ¶å¾ CSV è®€å–ï¼ˆå³ä½¿ Parquet å­˜åœ¨ä¹Ÿæœƒé‡æ–°ç”Ÿæˆï¼‰
        # ä½¿ç”¨ use_parquet=False å¯ä»¥è·³é Parquet æª¢æŸ¥ï¼Œä½†æˆ‘å€‘å·²ç¶“åˆªé™¤äº†
        # æ‰€ä»¥é€™è£¡ä½¿ç”¨ use_parquet=Trueï¼Œå®ƒæœƒè‡ªå‹•å¾ CSV è®€å–ä¸¦ç”Ÿæˆ Parquet
        raw_df = loader.load(file_path=csv_path, use_parquet=True)
        
        load_time = time.time() - start_time
        
        print(f"\nâœ… è³‡æ–™è¼‰å…¥å®Œæˆ")
        print(f"   è³‡æ–™ç­†æ•¸: {len(raw_df):,}")
        print(f"   è³‡æ–™æ¬„ä½: {len(raw_df.columns)}")
        print(f"   è¼‰å…¥æ™‚é–“: {format_time(load_time)}")
        
        # é¡¯ç¤ºè³‡æ–™è³‡è¨Š
        print(f"\n   è³‡æ–™æ¬„ä½åˆ—è¡¨:")
        for i, col in enumerate(raw_df.columns[:10], 1):
            print(f"     {i:2d}. {col}")
        if len(raw_df.columns) > 10:
            print(f"     ... é‚„æœ‰ {len(raw_df.columns) - 10} å€‹æ¬„ä½")
        
    except Exception as e:
        print(f"\nâŒ è¼‰å…¥å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    # 6. é©—è­‰ Parquet æª”æ¡ˆ
    print("\n[æ­¥é©Ÿ 6] é©—è­‰ Parquet æª”æ¡ˆ...")
    if parquet_path.exists():
        parquet_size = parquet_path.stat().st_size
        
        if parquet_size == 0:
            print(f"âš ï¸  Parquet æª”æ¡ˆå¤§å°ç‚º 0ï¼Œå¯èƒ½ç”Ÿæˆå¤±æ•—")
            print(f"   è«‹æª¢æŸ¥éŒ¯èª¤è¨Šæ¯æˆ–é‡æ–°åŸ·è¡Œè…³æœ¬")
        else:
            compression_ratio = csv_size / parquet_size if parquet_size > 0 else 0
            space_saved = csv_size - parquet_size
            space_saved_pct = (1 - parquet_size/csv_size)*100 if csv_size > 0 else 0
            
            print(f"âœ… Parquet æª”æ¡ˆå·²ç”Ÿæˆ: {parquet_path}")
            print(f"   æª”æ¡ˆå¤§å°: {format_size(parquet_size)}")
            if compression_ratio > 0:
                print(f"   å£“ç¸®æ¯”: {compression_ratio:.2f}:1")
                print(f"   ç¯€çœç©ºé–“: {format_size(space_saved)} ({space_saved_pct:.1f}%)")
            
            # æ¸¬è©¦è®€å–
            try:
                import pandas as pd
                test_df = pd.read_parquet(parquet_path, engine='pyarrow')
                print(f"   é©—è­‰è®€å–: âœ… æˆåŠŸ ({len(test_df):,} ç­†è³‡æ–™)")
            except Exception as e:
                print(f"   é©—è­‰è®€å–: âŒ å¤±æ•— - {e}")
                print(f"   æª”æ¡ˆå¯èƒ½æå£ï¼Œå»ºè­°åˆªé™¤å¾Œé‡æ–°ç”Ÿæˆ")
    else:
        print(f"âš ï¸  Parquet æª”æ¡ˆæœªç”Ÿæˆï¼Œå¯èƒ½å„²å­˜å¤±æ•—")
    
    # 7. æ¸…æ´—è³‡æ–™ï¼ˆå¯é¸ï¼‰
    print("\n[æ­¥é©Ÿ 7] æ¸…æ´—è³‡æ–™ï¼ˆå¯é¸ï¼‰...")
    response = input("æ˜¯å¦è¦æ¸…æ´—è³‡æ–™ï¼Ÿ(Y/n): ").strip().lower()
    if response != 'n':
        print("   æ­£åœ¨æ¸…æ´—è³‡æ–™...")
        clean_start = time.time()
        cleaned_df = loader.clean(raw_df)
        clean_time = time.time() - clean_start
        
        print(f"âœ… æ¸…æ´—å®Œæˆ")
        print(f"   è³‡æ–™ç­†æ•¸: {len(cleaned_df):,}")
        print(f"   æ¸…æ´—æ™‚é–“: {format_time(clean_time)}")
        
        # æª¢æŸ¥ StartTime æ˜¯å¦å·²è½‰æ›ç‚º datetime
        if 'StartTime' in cleaned_df.columns:
            if cleaned_df['StartTime'].dtype.name.startswith('datetime'):
                print(f"   âœ… StartTime å·²è½‰æ›ç‚º datetime é¡å‹")
            else:
                print(f"   âš ï¸  StartTime é¡å‹: {cleaned_df['StartTime'].dtype}")
    else:
        print("   è·³éæ¸…æ´—æ­¥é©Ÿ")
    
    # ç¸½çµ
    total_time = time.time() - start_time
    print("\n" + "=" * 70)
    print("âœ… é¦–æ¬¡è¼‰å…¥å®Œæˆï¼")
    print("=" * 70)
    print(f"ç¸½è€—æ™‚: {format_time(total_time)}")
    print(f"CSV æª”æ¡ˆ: {csv_path}")
    print(f"Parquet æª”æ¡ˆ: {parquet_path}")
    print("\nğŸ’¡ æç¤ºï¼š")
    print("   - ä¸‹æ¬¡è¼‰å…¥æ™‚ï¼ŒParquet æª”æ¡ˆæœƒè‡ªå‹•ä½¿ç”¨ï¼Œé€Ÿåº¦æœƒå¿«å¾ˆå¤šï¼ˆç´„ 5-10 ç§’ï¼‰")
    print("   - å¯ä»¥åœ¨ notebook æˆ–å…¶ä»–è…³æœ¬ä¸­ä½¿ç”¨ Spark è¼‰å…¥å™¨è¼‰å…¥è³‡æ–™")
    print("=" * 70)
    
    return 0


if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ“ä½œå·²å–æ¶ˆ")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

