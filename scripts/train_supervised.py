"""
ç›£ç£å­¸ç¿’æ¨¡å‹è¨“ç·´ï¼šXGBoost

ä½¿ç”¨ Pandas è¼‰å…¥å™¨è¼‰å…¥è³‡æ–™ï¼Œä¸¦ä½¿ç”¨ XGBoost é€²è¡Œç›£ç£å­¸ç¿’ç•°å¸¸æª¢æ¸¬ã€‚
åƒ…æ”¯æ´å¾ Parquet æª”æ¡ˆè¼‰å…¥ã€‚
ä½¿ç”¨æ¨™ç±¤é€²è¡Œè¨“ç·´ï¼Œé€šå¸¸æ¯”ç„¡ç›£ç£å­¸ç¿’è¡¨ç¾æ›´å¥½ã€‚
"""
import sys
import time
import json
from pathlib import Path
from typing import Tuple, Dict, Optional

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥ Python è·¯å¾‘ï¼ˆå¿…é ˆåœ¨åŒ¯å…¥ src æ¨¡çµ„ä¹‹å‰ï¼‰
PROJECT_ROOT = Path(__file__).parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src import (
    DataLoaderFactory,
    DataSourceType,
    ModelFactory,
    ModelType,
    extract_features,
    prepare_feature_set,
    FeatureSelector,
    FeatureSelectionStrategy,
    convert_label_to_binary,
    StandardFeatureProcessor,
    evaluate_and_print
)
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd


def load_and_prepare_features(
    parquet_path: Path,
    processor: StandardFeatureProcessor
) -> pd.DataFrame:
    """
    è¼‰å…¥å’Œæº–å‚™ç‰¹å¾µæ•¸æ“šï¼ˆæ”¯æ´å¿«å–ï¼‰
    
    Args:
        parquet_path: Parquet æ–‡ä»¶è·¯å¾‘
        processor: ç‰¹å¾µè™•ç†å™¨
    
    Returns:
        åŒ…å«æ‰€æœ‰ç‰¹å¾µçš„ DataFrame
    """
    features_stage4_path = Path("data/processed/features_stage4.parquet")
    
    if features_stage4_path.exists():
        print(f"\n   ğŸ’¾ ç™¼ç¾å·²è™•ç†çš„ç‰¹å¾µå¿«å–ï¼Œç›´æ¥è¼‰å…¥...")
        cache_start_time = time.time()
        
        # è¼‰å…¥åŸå§‹ç‰¹å¾µ
        features_df = processor.load_features()
        
        # æª¢æŸ¥ä¸¦æ·»åŠ  bidirectional_window_flow_ratioï¼ˆå¦‚æœä¸å­˜åœ¨ä½†æ‡‰è©²å­˜åœ¨ï¼‰
        if (processor.time_feature_stage == 4 and 
            'bidirectional_window_flow_ratio' not in features_df.columns and
            'bidirectional_total_src_bytes' in features_df.columns and
            'bidirectional_total_dst_bytes' in features_df.columns):
            print("   ğŸ”§ æª¢æ¸¬åˆ°ç¼ºå°‘ bidirectional_window_flow_ratioï¼Œæ­£åœ¨è¨ˆç®—ä¸¦æ·»åŠ ...")
            features_df['bidirectional_window_flow_ratio'] = (
                features_df['bidirectional_total_src_bytes'].astype(float) / 
                (features_df['bidirectional_total_dst_bytes'].astype(float) + 1)
            ).fillna(0.0).replace([np.inf, -np.inf], 0.0)
            print("   âœ… bidirectional_window_flow_ratio å·²æ·»åŠ ")
        
        cache_load_time = time.time() - cache_start_time
        print(f"   âœ… å¿«å–è¼‰å…¥å®Œæˆï¼ˆè€—æ™‚ {cache_load_time:.2f} ç§’ï¼‰")
        print(f"   ğŸ“Š åŸå§‹ç‰¹å¾µæ•¸ï¼š{features_df.shape[1]} å€‹")
    else:
        print("   âš ï¸  æœªç™¼ç¾ç‰¹å¾µå¿«å–ï¼ŒåŸ·è¡Œå®Œæ•´ç‰¹å¾µå·¥ç¨‹æµç¨‹...")
        print("   âš ï¸  éšæ®µ4éœ€è¦ PySparkï¼Œè¨ˆç®—æˆæœ¬è¼ƒé«˜ï¼Œè«‹è€å¿ƒç­‰å¾…...")
        features_start_time = time.time()
        
        # è®€å– Parquet æ–‡ä»¶
        raw_df = pd.read_parquet(parquet_path, engine='pyarrow')
        
        # å‰µå»ºè¼‰å…¥å™¨ç”¨æ–¼æ¸…æ´—è³‡æ–™
        loader = DataLoaderFactory.create(DataSourceType.BIDIRECTIONAL_BINETFLOW)
        cleaned_df = loader.clean(raw_df)
        
        # åŸ·è¡Œå®Œæ•´ç‰¹å¾µå·¥ç¨‹
        features_df = extract_features(
            cleaned_df,
            include_time_features=True,
            time_feature_stage=4  # éšæ®µ4ï¼šåŒ…å«æ‰€æœ‰éšæ®µç‰¹å¾µï¼ˆæœ€å®Œæ•´ï¼‰
        )
        
        features_time = time.time() - features_start_time
        print(f"   âœ… ç‰¹å¾µå·¥ç¨‹å®Œæˆï¼ˆè€—æ™‚ {features_time:.2f} ç§’ï¼‰")
        print(f"   ğŸ“Š åŸå§‹ç‰¹å¾µæ•¸ï¼š{features_df.shape[1]} å€‹")
    
    return features_df


def perform_feature_selection(
    X: pd.DataFrame,
    features_df: pd.DataFrame,
    initial_feature_count: int
) -> pd.DataFrame:
    """
    åŸ·è¡Œå®Œæ•´çš„ç‰¹å¾µé¸æ“‡æµç¨‹
    
    Args:
        X: ç‰¹å¾µ DataFrame
        features_df: åŒ…å«æ¨™ç±¤çš„å®Œæ•´ DataFrame
        initial_feature_count: åˆå§‹ç‰¹å¾µæ•¸é‡
    
    Returns:
        é¸æ“‡å¾Œçš„ç‰¹å¾µ DataFrame
    """
    print("\n[æ­¥é©Ÿ 4.5] ç‰¹å¾µé¸æ“‡ï¼ˆå“è³ªæª¢æŸ¥å’Œç›¸é—œæ€§åˆ†æï¼‰...")
    
    # ä½¿ç”¨ FeatureSelector é€²è¡Œå“è³ªæª¢æŸ¥å’Œç›¸é—œæ€§åˆ†æ
    selector = FeatureSelector(
        remove_constant=True,
        remove_low_variance=True,
        variance_threshold=1e-6,
        remove_inf=True,
        inf_ratio_threshold=0.1,
        remove_high_missing=True,
        missing_ratio_threshold=0.5,
        remove_high_correlation=True,
        correlation_threshold=0.98
    )
    
    X_selected, removed_features = selector.select_features(
        X,
        features_df=None,  # å“è³ªæª¢æŸ¥å’Œç›¸é—œæ€§åˆ†æä¸éœ€è¦æ¨™ç±¤
        strategies=[FeatureSelectionStrategy.QUALITY_CHECK, FeatureSelectionStrategy.CORRELATION],
        verbose=True
    )
    
    print(f"\nâœ… åŸºæœ¬ç‰¹å¾µé¸æ“‡å®Œæˆï¼šå¾ {initial_feature_count} å€‹ç‰¹å¾µæ¸›å°‘åˆ° {len(X_selected.columns)} å€‹ç‰¹å¾µ")
    
    # åŸºæ–¼é‡è¦æ€§çš„ç‰¹å¾µé¸æ“‡ï¼ˆéœ€è¦æ¨™ç±¤ï¼‰
    print("\n[æ­¥é©Ÿ 4.6] åŸºæ–¼ XGBoost ç‰¹å¾µé‡è¦æ€§çš„ç‰¹å¾µé¸æ“‡...")
    if 'Label' in features_df.columns:
        try:
            # æº–å‚™æ¨™ç±¤
            if 'label_binary' not in features_df.columns:
                features_df_temp = convert_label_to_binary(features_df, verbose=False)
            else:
                features_df_temp = features_df.copy()
            
            # ä½¿ç”¨ FeatureSelector çš„å…§éƒ¨æ–¹æ³•é€²è¡Œé‡è¦æ€§é¸æ“‡ï¼ˆéœ€è¦è‡ªå®šç¾©åƒæ•¸ï¼‰
            # æ³¨æ„ï¼šç”±æ–¼ FeatureSelector._importance_selection çš„åƒæ•¸èˆ‡æˆ‘å€‘çš„éœ€æ±‚ç•¥æœ‰ä¸åŒï¼Œ
            # æˆ‘å€‘ç›´æ¥èª¿ç”¨å®ƒä¸¦å‚³å…¥è‡ªå®šç¾©åƒæ•¸ã€‚é€™æ˜¯åˆç†çš„ï¼Œå› ç‚ºæˆ‘å€‘éœ€è¦ç‰¹å®šçš„åƒæ•¸å€¼ã€‚
            # æœªä¾†å¯ä»¥è€ƒæ…®åœ¨ FeatureSelector ä¸­æ·»åŠ æ”¯æŒè‡ªå®šç¾©åƒæ•¸çš„å…¬å…±æ–¹æ³•ã€‚
            try:
                X_selected, removed_importance = selector._importance_selection(
                    X_selected,
                    features_df_temp,
                    verbose=True,
                    min_features=15,
                    max_features=30,  # train_supervised.py ä½¿ç”¨ 30ï¼Œè€Œä¸æ˜¯ FeatureSelector é è¨­çš„ 25
                    importance_threshold=0.95  # train_supervised.py ä½¿ç”¨ 0.95ï¼Œè€Œä¸æ˜¯ FeatureSelector é è¨­çš„ 0.98
                )
            except TypeError:
                # å¦‚æœ _importance_selection ä¸æ”¯æŒé€™äº›åƒæ•¸ï¼Œå›é€€åˆ°ä½¿ç”¨ select_features
                # ä½†é€™æœƒä½¿ç”¨é è¨­åƒæ•¸ï¼ˆmin_features=15, max_features=25, importance_threshold=0.98ï¼‰
                print("   âš ï¸  ä½¿ç”¨ FeatureSelector é è¨­åƒæ•¸é€²è¡Œé‡è¦æ€§é¸æ“‡...")
                X_selected, removed_features_dict = selector.select_features(
                    X_selected,
                    features_df=features_df_temp,
                    strategies=[FeatureSelectionStrategy.IMPORTANCE],
                    verbose=True
                )
            
            print(f"\n   âœ… åŸºæ–¼é‡è¦æ€§é¸æ“‡å®Œæˆï¼šä¿ç•™ {len(X_selected.columns)} å€‹æœ€é‡è¦ç‰¹å¾µ")
            print(f"   æœ€çµ‚ç‰¹å¾µåˆ—è¡¨ï¼š{list(X_selected.columns)}")
            
        except Exception as e:
            print(f"   âš ï¸  åŸºæ–¼é‡è¦æ€§çš„ç‰¹å¾µé¸æ“‡å¤±æ•—ï¼š{e}")
            print(f"   å°‡ä½¿ç”¨åŸºæœ¬ç‰¹å¾µé¸æ“‡çš„çµæœç¹¼çºŒåŸ·è¡Œ")
            import traceback
            traceback.print_exc()
    else:
        print("   âš ï¸  ç¼ºå°‘æ¨™ç±¤ï¼Œè·³éåŸºæ–¼é‡è¦æ€§çš„ç‰¹å¾µé¸æ“‡")
    
    return X_selected




def train_model_with_overfitting_check(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    scale_pos_weight: float
) -> Tuple[any, Dict]:
    """
    è¨“ç·´æ¨¡å‹ä¸¦é€²è¡Œéæ“¬åˆæª¢æŸ¥å’Œå‹•æ…‹åƒæ•¸èª¿æ•´
    
    Args:
        X_train: è¨“ç·´ç‰¹å¾µ
        y_train: è¨“ç·´æ¨™ç±¤
        scale_pos_weight: ä¸å¹³è¡¡æ¬Šé‡
    
    Returns:
        (è¨“ç·´å¥½çš„æ¨¡å‹, è¨“ç·´æŒ‡æ¨™)
    """
    print("\n[æ­¥é©Ÿ 7] è¨“ç·´ XGBoost æ¨¡å‹ï¼ˆåˆå§‹åƒæ•¸ï¼‰...")
    model = ModelFactory.create(ModelType.XGBOOST)
    
    # XGBoost åˆå§‹åƒæ•¸ï¼ˆä¿å®ˆè¨­ç½®ï¼Œé˜²æ­¢éæ“¬åˆï¼‰
    initial_params = {
        'scale_pos_weight': scale_pos_weight,
        'max_depth': 6,
        'learning_rate': 0.1,
        'n_estimators': 200,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'eval_metric': 'logloss',
        'early_stopping_rounds': 10
    }
    
    print(f"   åˆå§‹åƒæ•¸ï¼šmax_depth={initial_params['max_depth']}, learning_rate={initial_params['learning_rate']}, "
          f"n_estimators={initial_params['n_estimators']}, subsample={initial_params['subsample']}, "
          f"colsample_bytree={initial_params['colsample_bytree']}")
    
    trained_model, train_metrics = model.train(
        X_train,
        y_train,
        test_size=0.2,
        random_state=42,
        **initial_params
    )
    
    print("âœ… åˆå§‹æ¨¡å‹è¨“ç·´å®Œæˆ")
    
    # éæ“¬åˆè¨ºæ–·
    print(f"\n   ğŸ“Š éæ“¬åˆè¨ºæ–·ï¼š")
    if 'train_accuracy' in train_metrics and 'test_accuracy' in train_metrics:
        train_acc = train_metrics['train_accuracy']
        test_acc = train_metrics['test_accuracy']
        gap = train_metrics.get('accuracy_gap', train_acc - test_acc)
        risk = train_metrics.get('overfitting_risk', 'unknown')
        best_iter = train_metrics.get('best_iteration', 'N/A')
        
        print(f"     è¨“ç·´é›†æº–ç¢ºç‡ï¼š{train_acc:.4f} ({train_acc*100:.2f}%)")
        print(f"     é©—è­‰é›†æº–ç¢ºç‡ï¼š{test_acc:.4f} ({test_acc*100:.2f}%)")
        print(f"     æº–ç¢ºç‡å·®ç•°ï¼š{gap:.4f} ({gap*100:.2f}%)")
        print(f"     éæ“¬åˆé¢¨éšªï¼š{risk.upper()}")
        print(f"     æœ€ä½³è¿­ä»£æ¬¡æ•¸ï¼š{best_iter}")
        
        # æ ¹æ“šéæ“¬åˆé¢¨éšªå‹•æ…‹èª¿æ•´åƒæ•¸
        if risk == 'high':
            print(f"\n     âš ï¸  è­¦å‘Šï¼šå­˜åœ¨é«˜éæ“¬åˆé¢¨éšªï¼å°‡èª¿æ•´åƒæ•¸é™ä½æ¨¡å‹è¤‡é›œåº¦...")
            adjusted_params = {
                'max_depth': 4,
                'learning_rate': 0.05,
                'n_estimators': 300,
                'subsample': 0.7,
                'colsample_bytree': 0.7,
                'early_stopping_rounds': 20
            }
            print(f"     èª¿æ•´å¾Œåƒæ•¸ï¼šmax_depth={adjusted_params['max_depth']}, learning_rate={adjusted_params['learning_rate']}, "
                  f"n_estimators={adjusted_params['n_estimators']}, subsample={adjusted_params['subsample']}, "
                  f"colsample_bytree={adjusted_params['colsample_bytree']}")
            
            # ä½¿ç”¨èª¿æ•´å¾Œçš„åƒæ•¸é‡æ–°è¨“ç·´
            print(f"\n   ğŸ”„ ä½¿ç”¨èª¿æ•´å¾Œçš„åƒæ•¸é‡æ–°è¨“ç·´æ¨¡å‹...")
            model_adjusted = ModelFactory.create(ModelType.XGBOOST)
            trained_model, train_metrics = model_adjusted.train(
                X_train,
                y_train,
                test_size=0.2,
                random_state=42,
                scale_pos_weight=scale_pos_weight,
                eval_metric='logloss',
                **adjusted_params
            )
            
            # é‡æ–°è¨ºæ–·
            train_acc = train_metrics['train_accuracy']
            test_acc = train_metrics['test_accuracy']
            gap = train_metrics.get('accuracy_gap', train_acc - test_acc)
            risk = train_metrics.get('overfitting_risk', 'unknown')
            best_iter = train_metrics.get('best_iteration', 'N/A')
            
            print(f"   âœ… èª¿æ•´å¾Œæ¨¡å‹è¨“ç·´å®Œæˆ")
            print(f"     è¨“ç·´é›†æº–ç¢ºç‡ï¼š{train_acc:.4f} ({train_acc*100:.2f}%)")
            print(f"     é©—è­‰é›†æº–ç¢ºç‡ï¼š{test_acc:.4f} ({test_acc*100:.2f}%)")
            print(f"     æº–ç¢ºç‡å·®ç•°ï¼š{gap:.4f} ({gap*100:.2f}%)")
            print(f"     éæ“¬åˆé¢¨éšªï¼š{risk.upper()}")
            print(f"     æœ€ä½³è¿­ä»£æ¬¡æ•¸ï¼š{best_iter}")
            
            if risk == 'high':
                print(f"     âš ï¸  è­¦å‘Šï¼šèª¿æ•´å¾Œä»å­˜åœ¨é«˜éæ“¬åˆé¢¨éšªï¼")
            elif risk == 'medium':
                print(f"     âš ï¸  æ³¨æ„ï¼šèª¿æ•´å¾Œä»å­˜åœ¨ä¸­ç­‰éæ“¬åˆé¢¨éšª")
            else:
                print(f"     âœ… èª¿æ•´å¾Œéæ“¬åˆé¢¨éšªé™ä½ï¼Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›æ”¹å–„")
            
            # æ›´æ–° model å°è±¡
            model = model_adjusted
            
        elif risk == 'medium':
            print(f"\n     âš ï¸  æ³¨æ„ï¼šå­˜åœ¨ä¸­ç­‰éæ“¬åˆé¢¨éšªï¼Œå»ºè­°èª¿æ•´æ¨¡å‹åƒæ•¸")
            print(f"     å¯ä»¥è€ƒæ…®ï¼šé™ä½ max_depth æˆ–å¢åŠ  subsample/colsample_bytree çš„éš¨æ©Ÿæ€§")
        else:
            print(f"     âœ… éæ“¬åˆé¢¨éšªè¼ƒä½ï¼Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›è‰¯å¥½")
    else:
        print(f"     æº–ç¢ºç‡ï¼š{train_metrics.get('accuracy', 'N/A'):.4f}")
    
    print(f"\n   å…§éƒ¨é©—è­‰é›†æ€§èƒ½ï¼š")
    if 'classification_report' in train_metrics:
        report = train_metrics['classification_report']
        if isinstance(report, dict):
            print(f"     æ­£å¸¸é¡åˆ¥ - ç²¾ç¢ºç‡ï¼š{report.get('0', {}).get('precision', 'N/A'):.4f}, å¬å›ç‡ï¼š{report.get('0', {}).get('recall', 'N/A'):.4f}, F1ï¼š{report.get('0', {}).get('f1-score', 'N/A'):.4f}")
            print(f"     ç•°å¸¸é¡åˆ¥ - ç²¾ç¢ºç‡ï¼š{report.get('1', {}).get('precision', 'N/A'):.4f}, å¬å›ç‡ï¼š{report.get('1', {}).get('recall', 'N/A'):.4f}, F1ï¼š{report.get('1', {}).get('f1-score', 'N/A'):.4f}")
        else:
            print(f"     åˆ†é¡å ±å‘Šï¼š\n{report}")
    
    return model, train_metrics


def main():
    print("=" * 60)
    print("ç›£ç£å­¸ç¿’æ¨¡å‹è¨“ç·´ï¼šXGBoost")
    print("=" * 60)
    start_time = time.time()
    
    # 1. è¼‰å…¥è³‡æ–™ï¼ˆåƒ…æ”¯æ´ Parquet æª”æ¡ˆï¼‰
    print("\n[æ­¥é©Ÿ 1] è¼‰å…¥è³‡æ–™...")
    parquet_path = Path("data/processed/capture20110817_cleaned_spark.parquet")
    if not parquet_path.exists():
        raise FileNotFoundError(
            f"æ‰¾ä¸åˆ° Parquet æª”æ¡ˆ: {parquet_path}\n"
            f"è«‹å…ˆåŸ·è¡Œè³‡æ–™è™•ç†è…³æœ¬ç”Ÿæˆ Parquet æª”æ¡ˆã€‚"
        )
    
    print(f"   ä½¿ç”¨ Pandas è®€å– Parquet: {parquet_path}")
    load_time = time.time() - start_time
    
    # 2-3. ç‰¹å¾µè™•ç†ï¼ˆä½¿ç”¨ FeatureProcessorï¼Œæ”¯æ´å¿«å–ï¼‰
    print("\n[æ­¥é©Ÿ 2-3] ç‰¹å¾µè™•ç†...")
    print("   ä½¿ç”¨éšæ®µ4æ™‚é–“ç‰¹å¾µï¼ˆæœ€å®Œæ•´ï¼šåŒ…å«æ‰€æœ‰éšæ®µç‰¹å¾µï¼‰")
    print("   - éšæ®µ1ï¼šåŸºæœ¬æ™‚é–“ç‰¹å¾µ")
    print("   - éšæ®µ2ï¼šæ™‚é–“é–“éš”ç‰¹å¾µ")
    print("   - éšæ®µ3ï¼šæ™‚é–“çª—å£èšåˆç‰¹å¾µï¼ˆæŒ‰ SrcAddrï¼‰")
    print("   - éšæ®µ4ï¼šé›™å‘æµ Pair èšåˆç‰¹å¾µï¼ˆæŒ‰ IP Pairï¼Œéœ€è¦ PySparkï¼‰")
    
    processor = StandardFeatureProcessor(time_feature_stage=4)
    features_df = load_and_prepare_features(parquet_path, processor)
    
    # 4. æº–å‚™è¨“ç·´è³‡æ–™
    print("\n[æ­¥é©Ÿ 4] æº–å‚™è¨“ç·´è³‡æ–™...")
    X = prepare_feature_set(
        features_df,
        include_base_features=True,
        include_time_features=True,
        time_feature_stage=4
    )
    initial_feature_count = len(X.columns)
    print(f"âœ… åˆå§‹ç‰¹å¾µæ¬„ä½ï¼ˆå…± {initial_feature_count} å€‹ï¼‰")
    
    # 4.5-4.6 ç‰¹å¾µé¸æ“‡
    X = perform_feature_selection(X, features_df, initial_feature_count)
    
    # 4.7 æ™‚é–“ç‰¹å¾µæª¢æŸ¥ï¼ˆä½¿ç”¨ FeatureSelector çµ±ä¸€æ–¹æ³•ï¼‰
    print("\n[æ­¥é©Ÿ 4.7] æª¢æŸ¥æ™‚é–“ç‰¹å¾µé‡è¦æ€§ï¼ˆé¿å…æ™‚é–“åå·®ï¼‰...")
    selector = FeatureSelector()
    X, time_importance_dict = selector.check_time_feature_bias(
        X,
        features_df,
        time_features=['hour', 'cos_hour', 'sin_hour'],
        importance_threshold=0.05,  # æ™‚é–“ç‰¹å¾µç¸½é‡è¦æ€§é–¾å€¼ 5%
        sample_size=10000,
        verbose=True
    )
    
    print(f"\nâœ… ç‰¹å¾µé¸æ“‡å®Œæˆï¼šå¾ {initial_feature_count} å€‹ç‰¹å¾µæ¸›å°‘åˆ° {len(X.columns)} å€‹ç‰¹å¾µ")
    
    # 5. æº–å‚™æ¨™ç±¤
    print("\n[æ­¥é©Ÿ 5] æº–å‚™æ¨™ç±¤...")
    if 'Label' not in features_df.columns:
        print("âŒ éŒ¯èª¤ï¼šç¼ºå°‘ 'Label' æ¬„ä½ï¼Œç„¡æ³•é€²è¡Œç›£ç£å­¸ç¿’")
        print("   è«‹ä½¿ç”¨åŒ…å«æ¨™ç±¤çš„è³‡æ–™é›†")
        return 1
    
    features_df = convert_label_to_binary(features_df, verbose=True)
    y = features_df['label_binary']
    
    # é¡¯ç¤ºæ¨™ç±¤åˆ†å¸ƒ
    print(f"\n   æ¨™ç±¤åˆ†å¸ƒçµ±è¨ˆï¼š")
    print(f"     æ­£å¸¸ (0): {(y == 0).sum():,} ({(y == 0).sum()/len(y)*100:.2f}%)")
    print(f"     ç•°å¸¸ (1): {(y == 1).sum():,} ({(y == 1).sum()/len(y)*100:.2f}%)")
    
    # è¨ˆç®—ä¸å¹³è¡¡æ¯”ä¾‹
    negative_count = (y == 0).sum()
    positive_count = (y == 1).sum()
    scale_pos_weight = negative_count / positive_count if positive_count > 0 else 1.0
    print(f"   ä¸å¹³è¡¡æ¯”ä¾‹ï¼š{scale_pos_weight:.2f}:1ï¼ˆæ­£å¸¸:ç•°å¸¸ï¼‰")
    print(f"   å°‡ä½¿ç”¨ scale_pos_weight={scale_pos_weight:.2f} ä¾†è™•ç†ä¸å¹³è¡¡è³‡æ–™")
    
    # 6. åˆ†å‰²è³‡æ–™é›†
    print("\n[æ­¥é©Ÿ 6] åˆ†å‰²è³‡æ–™é›†...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=0.2, 
        random_state=42, 
        stratify=y
    )
    print(f"âœ… è¨“ç·´é›†ï¼š{len(X_train):,} ç­†ï¼ˆ{len(X_train)/len(X)*100:.1f}%ï¼‰")
    print(f"âœ… æ¸¬è©¦é›†ï¼š{len(X_test):,} ç­†ï¼ˆ{len(X_test)/len(X)*100:.1f}%ï¼‰")
    print(f"   è¨“ç·´é›†æ¨™ç±¤åˆ†å¸ƒï¼šæ­£å¸¸ {(y_train == 0).sum():,}ï¼Œç•°å¸¸ {(y_train == 1).sum():,}")
    print(f"   æ¸¬è©¦é›†æ¨™ç±¤åˆ†å¸ƒï¼šæ­£å¸¸ {(y_test == 0).sum():,}ï¼Œç•°å¸¸ {(y_test == 1).sum():,}")
    
    # 7. è¨“ç·´æ¨¡å‹ï¼ˆåŒ…å«éæ“¬åˆæª¢æŸ¥å’Œå‹•æ…‹åƒæ•¸èª¿æ•´ï¼‰
    model, train_metrics = train_model_with_overfitting_check(
        X_train, y_train, scale_pos_weight
    )
    
    # 8. ç‰¹å¾µé‡è¦æ€§åˆ†æ
    print("\n[æ­¥é©Ÿ 8] ç‰¹å¾µé‡è¦æ€§åˆ†æ...")
    feature_importance = model.get_feature_importance()
    sorted_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
    
    print(f"\n   Top 10 æœ€é‡è¦ç‰¹å¾µï¼š")
    for i, (feature, importance) in enumerate(sorted_importance[:10], 1):
        print(f"     {i:2d}. {feature:30s}: {importance:.4f}")
    
    # ä¿å­˜ç‰¹å¾µé‡è¦æ€§åˆ°æ–‡ä»¶ï¼ˆä¾›å ±å‘Šç”Ÿæˆå™¨ä½¿ç”¨ï¼‰
    output_dir = Path("output/evaluations")
    output_dir.mkdir(parents=True, exist_ok=True)
    xgb_feature_importance_path = output_dir / "xgb_feature_importance.json"
    with open(xgb_feature_importance_path, 'w', encoding='utf-8') as f:
        json.dump(feature_importance, f, indent=2, ensure_ascii=False)
    print(f"\n   âœ… ç‰¹å¾µé‡è¦æ€§å·²ä¿å­˜è‡³: {xgb_feature_importance_path}")
    
    # 9. é æ¸¬
    print("\n[æ­¥é©Ÿ 9] é€²è¡Œé æ¸¬...")
    y_pred_labels = model.predict(X_test)
    print(f"   é æ¸¬ç•°å¸¸æ•¸é‡ï¼š{y_pred_labels.sum():,} ({y_pred_labels.sum()/len(y_pred_labels)*100:.2f}%)")
    
    # 10. è©•ä¼°
    print("\n[æ­¥é©Ÿ 10] æ¨¡å‹è©•ä¼°ï¼ˆæ¸¬è©¦é›†ï¼‰...")
    evaluate_and_print(
        y_test,
        y_pred_labels,
        show_confusion_matrix=True,
        show_detailed=True,
        show_classification_report=True,
        indent="  "
    )
    
    total_time = time.time() - start_time
    print("\n" + "=" * 60)
    print(f"âœ… åŸ·è¡Œå®Œæˆï¼ˆç¸½è€—æ™‚ï¼š{total_time:.2f} ç§’ï¼‰")
    print(f"   è³‡æ–™è¼‰å…¥ï¼š{load_time:.2f} ç§’")
    print("=" * 60)
    
    return 0

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

