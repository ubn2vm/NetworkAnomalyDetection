"""
ç„¡ç›£ç£å­¸ç¿’æ¨¡å‹è¨“ç·´ï¼šIsolation Forest

ä½¿ç”¨ Pandas è¼‰å…¥å™¨è¼‰å…¥è³‡æ–™ï¼Œä¸¦ä½¿ç”¨ sklearn çš„ Isolation Forest é€²è¡Œç•°å¸¸æª¢æ¸¬ã€‚
åƒ…æ”¯æ´å¾ Parquet æª”æ¡ˆè¼‰å…¥ã€‚

æ­¤è…³æœ¬å°ˆæ³¨æ–¼æ¨¡å‹è¨“ç·´ï¼Œéµå¾ªå–®ä¸€è·è²¬åŸå‰‡ã€‚
ç™½åå–®å¾Œè™•ç†è«‹ä½¿ç”¨ postprocess_with_whitelist.pyã€‚
"""
import sys
import time
import pickle
import json
from pathlib import Path
from typing import Optional, Tuple, Dict, Any

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥ Python è·¯å¾‘ï¼ˆå¿…é ˆåœ¨åŒ¯å…¥ src æ¨¡çµ„ä¹‹å‰ï¼‰
PROJECT_ROOT = Path(__file__).parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src import (
    DataLoaderFactory,
    DataSourceType,
    extract_features,
    transform_features_for_unsupervised,
    DEFAULT_SKEWED_FEATURES,
    prepare_feature_set,
    FeatureSelector,
    FeatureSelectionStrategy,
    StandardFeatureProcessor,
    calculate_contamination,
    train_single_model,
    train_protocol_grouped_models,
    evaluate_and_print
)
from sklearn.metrics import precision_recall_curve
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd

def load_and_prepare_features(
    processor: StandardFeatureProcessor,
    cleaned_df: Optional[pd.DataFrame] = None
) -> pd.DataFrame:
    """
    è¼‰å…¥å’Œæº–å‚™ç‰¹å¾µæ•¸æ“šï¼ˆæ”¯æ´å¿«å–ï¼Œå„ªå…ˆä½¿ç”¨éšæ®µ4ï¼‰
    
    Args:
        processor: ç‰¹å¾µè™•ç†å™¨
        cleaned_df: æ¸…æ´—å¾Œçš„ DataFrameï¼ˆå¦‚æœå¿«å–ä¸å­˜åœ¨ï¼Œå‰‡éœ€è¦æ­¤åƒæ•¸ï¼‰
    
    Returns:
        åŒ…å«æ‰€æœ‰ç‰¹å¾µçš„ DataFrame
    """
    features_stage3_path = Path("data/processed/features_stage3.parquet")
    features_stage4_path = Path("data/processed/features_stage4.parquet")
    
    # å„ªå…ˆæª¢æŸ¥éšæ®µ4å¿«å–
    if features_stage4_path.exists():
        print(f"\n   ğŸ’¾ ç™¼ç¾éšæ®µ4ç‰¹å¾µå¿«å–ï¼Œç›´æ¥è¼‰å…¥...")
        cache_start_time = time.time()
        
        # è¼‰å…¥åŸå§‹ç‰¹å¾µï¼ˆä¸å‚³ stage åƒæ•¸ï¼Œæœƒè‡ªå‹•å„ªå…ˆè¼‰å…¥éšæ®µ4ï¼‰
        features_df = processor.load_features()
        
        cache_load_time = time.time() - cache_start_time
        print(f"   âœ… éšæ®µ4å¿«å–è¼‰å…¥å®Œæˆï¼ˆè€—æ™‚ {cache_load_time:.2f} ç§’ï¼‰")
        print(f"   ğŸ“Š åŸå§‹ç‰¹å¾µæ•¸ï¼š{features_df.shape[1]} å€‹")
    elif features_stage3_path.exists():
        # æœ‰éšæ®µ3ä½†æ²’æœ‰éšæ®µ4ï¼Œå¯ä»¥å¢é‡åŸ·è¡Œéšæ®µ4
        print(f"\n   ğŸ“‚ ç™¼ç¾éšæ®µ3å¿«å–ï¼Œä½†æœªç™¼ç¾éšæ®µ4å¿«å–...")
        print(f"   ğŸ’¡ æç¤ºï¼šéšæ®µ4åŒ…å«é›™å‘æµ Pair èšåˆç‰¹å¾µï¼Œèƒ½æå‡æ¨¡å‹æ•ˆæœ")
        print(f"   â±ï¸  éšæ®µ4éœ€è¦ PySparkï¼Œé è¨ˆéœ€è¦ 30-60 åˆ†é˜")
        
        if cleaned_df is None:
            print(f"   âš ï¸  æœªæä¾› cleaned_dfï¼Œç„¡æ³•åŸ·è¡Œéšæ®µ4ï¼Œå°‡ä½¿ç”¨éšæ®µ3ç‰¹å¾µ")
            features_df = processor.load_features(stage=3)
        else:
            # è‡ªå‹•åŸ·è¡Œéšæ®µ4ï¼ˆå¯ä»¥æ”¹ç‚ºäº’å‹•å¼ï¼‰
            execute_stage4 = True  # é è¨­åŸ·è¡Œéšæ®µ4
            
            if execute_stage4:
                print(f"\n   ğŸ”„ é–‹å§‹åŸ·è¡Œéšæ®µ4ç‰¹å¾µå·¥ç¨‹ï¼ˆPySparkï¼‰...")
                features_start_time = time.time()
                
                # å¢é‡åŸ·è¡Œéšæ®µ4ï¼ˆå¾éšæ®µ3åˆ°éšæ®µ4ï¼‰
                features_df, _, _, _ = processor.process(
                    cleaned_df,
                    save_features=True,
                    save_transformed=False,  # ä¸ä¿å­˜è½‰æ›å¾Œçš„ç‰¹å¾µï¼ˆå› ç‚ºå¾ŒçºŒæœƒé‡æ–°è½‰æ›ï¼‰
                    incremental=True  # å¢é‡æ¨¡å¼ï¼šå¾éšæ®µ3åˆ°éšæ®µ4
                )
                
                features_time = time.time() - features_start_time
                print(f"   âœ… éšæ®µ4ç‰¹å¾µè™•ç†å®Œæˆï¼ˆè€—æ™‚ {features_time:.2f} ç§’ï¼‰")
                print(f"   ğŸ’¾ éšæ®µ4ç‰¹å¾µå·²å„²å­˜ï¼Œä¸‹æ¬¡åŸ·è¡Œå°‡ç›´æ¥è¼‰å…¥")
                print(f"   ğŸ“Š åŸå§‹ç‰¹å¾µæ•¸ï¼š{features_df.shape[1]} å€‹")
            else:
                print(f"   â¸ï¸  å·²è·³ééšæ®µ4ï¼Œä½¿ç”¨éšæ®µ3ç‰¹å¾µ")
                features_df = processor.load_features(stage=3)
    else:
        # éƒ½æ²’æœ‰å¿«å–ï¼ŒåŸ·è¡Œå®Œæ•´ç‰¹å¾µå·¥ç¨‹
        if cleaned_df is None:
            raise ValueError(
                "å¿«å–ä¸å­˜åœ¨ä¸”æœªæä¾› cleaned_dfï¼Œç„¡æ³•åŸ·è¡Œç‰¹å¾µå·¥ç¨‹ã€‚"
                "è«‹å…ˆåŸ·è¡Œè³‡æ–™æ¸…æ´—æ­¥é©Ÿã€‚"
            )
        
        print("   âš ï¸  æœªç™¼ç¾ç‰¹å¾µå¿«å–ï¼ŒåŸ·è¡Œå®Œæ•´ç‰¹å¾µå·¥ç¨‹æµç¨‹...")
        print("   âš ï¸  éšæ®µ4éœ€è¦ PySparkï¼Œè¨ˆç®—æˆæœ¬è¼ƒé«˜ï¼Œè«‹è€å¿ƒç­‰å¾…...")
        features_start_time = time.time()
        
        # åŸ·è¡Œå®Œæ•´ç‰¹å¾µå·¥ç¨‹ï¼ˆéšæ®µ4ï¼‰
        features_df = extract_features(
            cleaned_df,
            include_time_features=True,
            time_feature_stage=4  # éšæ®µ4ï¼šåŒ…å«æ‰€æœ‰éšæ®µç‰¹å¾µï¼ˆæœ€å®Œæ•´ï¼‰
        )
        
        # ä¿å­˜å¿«å–ä¾›å¾ŒçºŒä½¿ç”¨
        print("   ğŸ’¾ ä¿å­˜éšæ®µ4ç‰¹å¾µå¿«å–...")
        processor.save_features(features_df, stage=4)
        print("   âœ… éšæ®µ4ç‰¹å¾µå¿«å–å·²ä¿å­˜")
        
        features_time = time.time() - features_start_time
        print(f"   âœ… ç‰¹å¾µå·¥ç¨‹å®Œæˆï¼ˆè€—æ™‚ {features_time:.2f} ç§’ï¼‰")
        print(f"   ğŸ“Š åŸå§‹ç‰¹å¾µæ•¸ï¼š{features_df.shape[1]} å€‹")
    
    return features_df


def save_training_results(
    output_dir: Path,
    model: Any,
    scaler: Any,
    protocol_models: Optional[Dict[str, Any]] = None,
    protocol_scalers: Optional[Dict[str, Any]] = None,
    X_train: Optional[pd.DataFrame] = None,
    X_val: Optional[pd.DataFrame] = None,
    X_test: Optional[pd.DataFrame] = None,
    features_df_train: Optional[pd.DataFrame] = None,
    features_df_val: Optional[pd.DataFrame] = None,
    features_df_test: Optional[pd.DataFrame] = None,
    y_train: Optional[np.ndarray] = None,
    y_val: Optional[np.ndarray] = None,
    y_test: Optional[np.ndarray] = None,
    train_anomaly_scores: Optional[np.ndarray] = None,
    val_anomaly_scores: Optional[np.ndarray] = None,
    test_anomaly_scores: Optional[np.ndarray] = None,
    best_threshold: Optional[float] = None,
    contamination: Optional[float] = None,
    use_protocol_grouping: bool = False,
    feature_robust_scaler: Any = None,
    transformed_feature_cols: Optional[list] = None,
    final_feature_cols: Optional[list] = None
):
    """
    ä¿å­˜è¨“ç·´çµæœä¾›å¾ŒçºŒç™½åå–®å¾Œè™•ç†ä½¿ç”¨
    
    Args:
        output_dir: è¼¸å‡ºç›®éŒ„
        model: è¨“ç·´å¥½çš„æ¨¡å‹ï¼ˆå–®ä¸€æ¨¡å‹ï¼‰
        scaler: ç‰¹å¾µæ¨™æº–åŒ–å™¨ï¼ˆå–®ä¸€æ¨¡å‹ï¼‰
        protocol_models: å”è­°åˆ†çµ„æ¨¡å‹å­—å…¸ï¼ˆå¯é¸ï¼‰
        protocol_scalers: å”è­°åˆ†çµ„æ¨™æº–åŒ–å™¨å­—å…¸ï¼ˆå¯é¸ï¼‰
        X_train, X_val, X_test: è¨“ç·´/é©—è­‰/æ¸¬è©¦é›†ç‰¹å¾µ
        features_df_train, features_df_val, features_df_test: è¨“ç·´/é©—è­‰/æ¸¬è©¦é›†å®Œæ•´ç‰¹å¾µ
        y_train, y_val, y_test: è¨“ç·´/é©—è­‰/æ¸¬è©¦é›†æ¨™ç±¤
        train_anomaly_scores, val_anomaly_scores, test_anomaly_scores: ç•°å¸¸åˆ†æ•¸
        best_threshold: æœ€ä½³é–¾å€¼
        contamination: contamination åƒæ•¸
        use_protocol_grouping: æ˜¯å¦ä½¿ç”¨å”è­°åˆ†çµ„
        feature_robust_scaler: ç‰¹å¾µ RobustScaler
        transformed_feature_cols: è½‰æ›å¾Œçš„ç‰¹å¾µæ¬„ä½åˆ—è¡¨ï¼ˆ28å€‹ï¼šç”¨æ–¼é‡è¦æ€§é¸æ“‡ï¼‰
        final_feature_cols: æœ€çµ‚ç”¨æ–¼æ¨¡å‹è¨“ç·´çš„ç‰¹å¾µæ¬„ä½åˆ—è¡¨ï¼ˆ15å€‹ï¼šç¶“éé‡è¦æ€§é¸æ“‡å¾Œï¼‰
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹
    if use_protocol_grouping and protocol_models:
        with open(output_dir / "protocol_models.pkl", "wb") as f:
            pickle.dump(protocol_models, f)
        if protocol_scalers:
            with open(output_dir / "protocol_scalers.pkl", "wb") as f:
                pickle.dump(protocol_scalers, f)
    else:
        with open(output_dir / "model.pkl", "wb") as f:
            pickle.dump(model, f)
        if scaler:
            with open(output_dir / "scaler.pkl", "wb") as f:
                pickle.dump(scaler, f)
    
    # ä¿å­˜ç‰¹å¾µè³‡æ–™
    if X_train is not None:
        X_train.to_parquet(output_dir / "X_train.parquet")
    if X_val is not None:
        X_val.to_parquet(output_dir / "X_val.parquet")
    if X_test is not None:
        X_test.to_parquet(output_dir / "X_test.parquet")
    
    if features_df_train is not None:
        features_df_train.to_parquet(output_dir / "features_df_train.parquet")
    if features_df_val is not None:
        features_df_val.to_parquet(output_dir / "features_df_val.parquet")
    if features_df_test is not None:
        features_df_test.to_parquet(output_dir / "features_df_test.parquet")
    
    # ä¿å­˜æ¨™ç±¤å’Œç•°å¸¸åˆ†æ•¸
    if y_train is not None:
        np.save(output_dir / "y_train.npy", y_train)
    if y_val is not None:
        np.save(output_dir / "y_val.npy", y_val)
    if y_test is not None:
        np.save(output_dir / "y_test.npy", y_test)
    
    if train_anomaly_scores is not None:
        np.save(output_dir / "train_anomaly_scores.npy", train_anomaly_scores)
    if val_anomaly_scores is not None:
        np.save(output_dir / "val_anomaly_scores.npy", val_anomaly_scores)
    if test_anomaly_scores is not None:
        np.save(output_dir / "test_anomaly_scores.npy", test_anomaly_scores)
    
    # ä¿å­˜é…ç½®
    config = {
        "best_threshold": best_threshold,
        "contamination": contamination,
        "use_protocol_grouping": use_protocol_grouping,
        "transformed_feature_cols": transformed_feature_cols,  # 28å€‹ï¼šè½‰æ›å¾Œçš„ç‰¹å¾µ
        "final_feature_cols": final_feature_cols  # 15å€‹ï¼šæœ€çµ‚ç”¨æ–¼æ¨¡å‹è¨“ç·´çš„ç‰¹å¾µ
    }
    
    if feature_robust_scaler is not None:
        with open(output_dir / "feature_robust_scaler.pkl", "wb") as f:
            pickle.dump(feature_robust_scaler, f)
    
    with open(output_dir / "config.json", "w") as f:
        json.dump(config, f, indent=2, default=str)
    
    print(f"\nâœ… è¨“ç·´çµæœå·²ä¿å­˜è‡³ï¼š{output_dir}")


def main():
    print("=" * 60)
    print("ç„¡ç›£ç£å­¸ç¿’æ¨¡å‹è¨“ç·´ï¼šIsolation Forest")
    print("=" * 60)
    
    # è¼¸å‡ºç›®éŒ„
    output_dir = Path("data/models/unsupervised_training")
    
    # 1. è¼‰å…¥è³‡æ–™ï¼ˆåƒ…æ”¯æ´ Parquet æª”æ¡ˆï¼‰
    print("\n[æ­¥é©Ÿ 1] è¼‰å…¥è³‡æ–™...")
    start_time = time.time()
    
    # ç›´æ¥è®€å– Parquet æª”æ¡ˆ
    parquet_path = Path("data/processed/capture20110817_cleaned_spark.parquet")
    if not parquet_path.exists():
        raise FileNotFoundError(
            f"æ‰¾ä¸åˆ° Parquet æª”æ¡ˆ: {parquet_path}\n"
            f"è«‹å…ˆåŸ·è¡Œè³‡æ–™è™•ç†è…³æœ¬ç”Ÿæˆ Parquet æª”æ¡ˆã€‚"
        )
    
    print(f"   ä½¿ç”¨ Pandas è®€å– Parquet: {parquet_path}")
    raw_df = pd.read_parquet(parquet_path, engine='pyarrow')
    
    # å‰µå»ºè¼‰å…¥å™¨ç”¨æ–¼æ¸…æ´—è³‡æ–™
    loader = DataLoaderFactory.create(DataSourceType.BIDIRECTIONAL_BINETFLOW)
    
    load_time = time.time() - start_time
    print(f"âœ… è¼‰å…¥å®Œæˆï¼š{len(raw_df):,} ç­†è³‡æ–™ï¼ˆè€—æ™‚ {load_time:.2f} ç§’ï¼‰")
    
    # 2. æ¸…æ´—è³‡æ–™
    print("\n[æ­¥é©Ÿ 2] æ¸…æ´—è³‡æ–™...")
    cleaned_df = loader.clean(raw_df)
    print(f"âœ… æ¸…æ´—å®Œæˆï¼š{len(cleaned_df):,} ç­†è³‡æ–™")
    
    # 3. ç‰¹å¾µå·¥ç¨‹ï¼ˆä½¿ç”¨å¿«å–æ©Ÿåˆ¶ï¼‰
    print("\n[æ­¥é©Ÿ 3] ç‰¹å¾µå·¥ç¨‹...")
    # ä½¿ç”¨æ™‚é–“ç‰¹å¾µï¼ˆéšæ®µ4ï¼šæœ€å®Œæ•´ï¼ŒåŒ…å«æ‰€æœ‰éšæ®µç‰¹å¾µï¼‰
    # éšæ®µ4åŒ…å«é›™å‘æµ Pair èšåˆç‰¹å¾µï¼Œèƒ½è­˜åˆ¥æ›´è¤‡é›œçš„ç•°å¸¸æ¨¡å¼
    # - éšæ®µ1ï¼šåŸºæœ¬æ™‚é–“ç‰¹å¾µ
    # - éšæ®µ2ï¼šæ™‚é–“é–“éš”ç‰¹å¾µ
    # - éšæ®µ3ï¼šæ™‚é–“çª—å£èšåˆç‰¹å¾µï¼ˆæŒ‰ SrcAddrï¼‰
    # - éšæ®µ4ï¼šé›™å‘æµ Pair èšåˆç‰¹å¾µï¼ˆæŒ‰ IP Pairï¼Œéœ€è¦ PySparkï¼‰
    print("   âš ï¸  ä½¿ç”¨éšæ®µ4æ™‚é–“ç‰¹å¾µï¼ˆæœ€å®Œæ•´ï¼ŒåŒ…å«æ‰€æœ‰éšæ®µç‰¹å¾µï¼‰...")
    
    # å‰µå»ºç‰¹å¾µè™•ç†å™¨ï¼ˆä½¿ç”¨éšæ®µ4ï¼‰
    processor = StandardFeatureProcessor(time_feature_stage=4)
    
    # ä½¿ç”¨å¿«å–æ©Ÿåˆ¶è¼‰å…¥æˆ–ç”Ÿæˆç‰¹å¾µ
    features_df = load_and_prepare_features(processor, cleaned_df)
    
    print(f"âœ… ç‰¹å¾µå·¥ç¨‹å®Œæˆï¼š{features_df.shape[1]} å€‹ç‰¹å¾µ")
    
    # 4. æº–å‚™è¨“ç·´è³‡æ–™
    print("\n[æ­¥é©Ÿ 4] æº–å‚™è¨“ç·´è³‡æ–™...")
    # ä½¿ç”¨çµ±ä¸€çš„ç‰¹å¾µæº–å‚™æ¥å£
    X = prepare_feature_set(
        features_df,
        include_base_features=True,
        include_time_features=True,
        time_feature_stage=4  # èˆ‡ extract_features çš„éšæ®µä¿æŒä¸€è‡´ï¼ˆéšæ®µ4ï¼‰
    )
    print(f"âœ… åˆå§‹ç‰¹å¾µæ¬„ä½ï¼ˆå…± {len(X.columns)} å€‹ï¼‰ï¼š{list(X.columns)}")
    
    # 4.5 ç„¡ç›£ç£ç‰¹å¾µé¸æ“‡
    print("\n[æ­¥é©Ÿ 4.5] ç„¡ç›£ç£ç‰¹å¾µé¸æ“‡...")
    initial_feature_count = len(X.columns)
    
    # ä½¿ç”¨çµ±ä¸€çš„ç‰¹å¾µé¸æ“‡å™¨
    selector = FeatureSelector(
        remove_constant=True,
        remove_low_variance=True,
        variance_threshold=1e-6,
        remove_inf=True,
        inf_ratio_threshold=0.1,
        remove_high_missing=True,
        missing_ratio_threshold=0.5,
        remove_high_correlation=True,
        correlation_threshold=0.98
    )
    
    # åŸ·è¡Œå“è³ªæª¢æŸ¥å’Œç›¸é—œæ€§åˆ†æ
    X, removed_features = selector.select_features(
        X,
        strategies=[
            FeatureSelectionStrategy.QUALITY_CHECK,
            FeatureSelectionStrategy.CORRELATION
        ],
        verbose=True
    )
    
    print(f"\nâœ… ç‰¹å¾µé¸æ“‡å®Œæˆï¼šå¾ {initial_feature_count} å€‹ç‰¹å¾µæ¸›å°‘åˆ° {len(X.columns)} å€‹ç‰¹å¾µ")
    print(f"   æœ€çµ‚ç‰¹å¾µåˆ—è¡¨ï¼š{list(X.columns)}")
    
    # 4.5.3 ç‰¹å¾µè½‰æ›ï¼šLog-Transformation + RobustScalerï¼ˆå„ªåŒ– Unsupervised Modelï¼‰
    print("\n  [4.5.3] ç‰¹å¾µè½‰æ›ï¼ˆLog-Transformation + RobustScalerï¼‰...")
    print("   ä½¿ç”¨æ–°çš„ç‰¹å¾µè½‰æ›æ¨¡çµ„å„ªåŒ–é•·å°¾åˆ†ä½ˆç‰¹å¾µ...")
    
    # ä½¿ç”¨æ–°çš„ç‰¹å¾µè½‰æ›æ¨¡çµ„
    # é€™æœƒè‡ªå‹•ï¼š
    # 1. å°é•·å°¾åˆ†ä½ˆç‰¹å¾µé€²è¡Œ log1p è½‰æ›
    # 2. ä½¿ç”¨ RobustScaler é€²è¡Œæ¨™æº–åŒ–ï¼ˆå°æ¥µç«¯å€¼æ›´ç©©å¥ï¼‰
    skewed_features = [col for col in DEFAULT_SKEWED_FEATURES if col in X.columns]
    
    if skewed_features:
        print(f"   å° {len(skewed_features)} å€‹é•·å°¾åˆ†ä½ˆç‰¹å¾µé€²è¡Œè½‰æ›ï¼š{skewed_features[:5]}...")
        
        # æ‡‰ç”¨ Log-Transformation + RobustScaler
        # æ³¨æ„ï¼šé€™è£¡æˆ‘å€‘éœ€è¦å…ˆå°‡ X è½‰æ›å›å®Œæ•´çš„ DataFrameï¼ˆåŒ…å«æ‰€æœ‰æ¬„ä½ï¼‰
        # ç„¶å¾Œå†æå–ç‰¹å¾µæ¬„ä½
        features_df_temp = features_df.copy()
        features_df_temp[X.columns] = X
        
        transformed_df, robust_scaler, transformed_feature_cols = transform_features_for_unsupervised(
            features_df_temp,
            skewed_features=skewed_features,
            feature_columns=list(X.columns),  # ä½¿ç”¨ç•¶å‰é¸æ“‡çš„ç‰¹å¾µ
            replace_original=False  # å‰µå»º log_ å‰ç¶´çš„æ–°æ¬„ä½
        )
        
        # æ›´æ–° X ç‚ºè½‰æ›å¾Œçš„ç‰¹å¾µï¼ˆåªåŒ…å«è¢«æ¨™æº–åŒ–çš„æ¬„ä½ï¼‰
        X = transformed_df[transformed_feature_cols].copy()
        
        print(f"   âœ… ç‰¹å¾µè½‰æ›å®Œæˆ")
        print(f"      - å°æ•¸è½‰æ›ï¼š{len(skewed_features)} å€‹ç‰¹å¾µ")
        print(f"      - RobustScaler æ¨™æº–åŒ–ï¼š{len(transformed_feature_cols)} å€‹ç‰¹å¾µ")
        print(f"      - ä½¿ç”¨ RobustScalerï¼ˆä¸­ä½æ•¸ + IQRï¼‰è€Œé StandardScalerï¼Œå°æ¥µç«¯å€¼æ›´ç©©å¥")
        # ä¿å­˜ robust_scaler ä¾›å¾ŒçºŒä½¿ç”¨
        feature_robust_scaler = robust_scaler
    else:
        print("   âš ï¸  æœªæ‰¾åˆ°éœ€è¦è½‰æ›çš„é•·å°¾åˆ†ä½ˆç‰¹å¾µï¼Œè·³éè½‰æ›")
        feature_robust_scaler = None
        transformed_feature_cols = list(X.columns)
    
    # 4.6 åŸºæ–¼ XGBoost ç‰¹å¾µé‡è¦æ€§é€²ä¸€æ­¥å„ªåŒ–ï¼ˆå¯é¸ï¼‰
    # å¦‚æœè³‡æ–™æœ‰æ¨™ç±¤ï¼Œå¯ä»¥ä½¿ç”¨ XGBoost ä¾†è­˜åˆ¥æœ€é‡è¦çš„ç‰¹å¾µ
    if 'Label' in features_df.columns:
        print("\n[æ­¥é©Ÿ 4.6] åŸºæ–¼ XGBoost ç‰¹å¾µé‡è¦æ€§å„ªåŒ–ç‰¹å¾µé¸æ“‡...")
        try:
            # ä½¿ç”¨çµ±ä¸€çš„ç‰¹å¾µé¸æ“‡å™¨é€²è¡Œé‡è¦æ€§é¸æ“‡
            X, removed = selector.select_features(
                X,
                features_df=features_df,
                strategies=[FeatureSelectionStrategy.IMPORTANCE],
                verbose=True
            )
            print(f"   å„ªåŒ–å¾Œç‰¹å¾µåˆ—è¡¨ï¼š{list(X.columns)}")
        except Exception as e:
            print(f"   âš ï¸  ç‰¹å¾µé‡è¦æ€§åˆ†æå¤±æ•—ï¼š{e}")
            print("   ç¹¼çºŒä½¿ç”¨æ‰€æœ‰ç‰¹å¾µ")
    
    # 4.7 æ¢ä»¶ç§»é™¤æ™‚é–“ç‰¹å¾µï¼ˆé …ç›® 5ï¼šæ¢ä»¶ç§»é™¤ hour ç‰¹å¾µï¼‰
    print("\n[æ­¥é©Ÿ 4.7] æª¢æŸ¥æ™‚é–“ç‰¹å¾µé‡è¦æ€§ï¼ˆé¿å…æ™‚é–“åå·®ï¼‰...")
    X, time_importance_dict = selector.check_time_feature_bias(
        X,
        features_df,
        time_features=['hour', 'cos_hour', 'sin_hour'],
        importance_threshold=0.05,  # æ™‚é–“ç‰¹å¾µç¸½é‡è¦æ€§é–¾å€¼ 5%
        sample_size=10000,
        verbose=True
    )
    
    # ä¿å­˜æœ€çµ‚ç”¨æ–¼æ¨¡å‹è¨“ç·´çš„ç‰¹å¾µåˆ—è¡¨ï¼ˆåœ¨ç‰¹å¾µé¸æ“‡å®Œæˆå¾Œï¼‰
    final_feature_cols = list(X.columns)  # æœ€çµ‚ç”¨æ–¼æ¨¡å‹è¨“ç·´çš„ç‰¹å¾µï¼ˆ15å€‹ï¼‰
    print(f"\nâœ… æœ€çµ‚ç‰¹å¾µé¸æ“‡å®Œæˆï¼šä½¿ç”¨ {len(final_feature_cols)} å€‹ç‰¹å¾µé€²è¡Œæ¨¡å‹è¨“ç·´")
    print(f"   è½‰æ›å¾Œç‰¹å¾µæ•¸ï¼š{len(transformed_feature_cols)} å€‹")
    print(f"   æœ€çµ‚è¨“ç·´ç‰¹å¾µæ•¸ï¼š{len(final_feature_cols)} å€‹")
    print(f"   ç§»é™¤ç‰¹å¾µæ•¸ï¼š{len(transformed_feature_cols) - len(final_feature_cols)} å€‹")
    
    # è¨ˆç®— contamination åƒæ•¸ï¼ˆä½¿ç”¨çµ±ä¸€å‡½æ•¸ï¼‰
    contamination, y_true = calculate_contamination(
        features_df,
        multiplier=1.3,
        max_contamination=0.2,
        high_threshold=0.15,
        min_contamination=0.01,
        default=0.1,
        verbose=True
    )
    
    # 4.8 è³‡æ–™åˆ†å‰²ï¼ˆé¿å… data leakageï¼‰
    print("\n[æ­¥é©Ÿ 4.8] è³‡æ–™åˆ†å‰²ï¼ˆé¿å…å·çœ‹ç­”æ¡ˆï¼‰...")
    use_data_split = y_true is not None
    
    if use_data_split:
        # å…ˆåˆ†å‰² train/test (80/20)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_true, 
            test_size=0.2, 
            random_state=42, 
            stratify=y_true if y_true is not None else None
        )
        # å†å¾è¨“ç·´é›†ä¸­åˆ†å‰²å‡ºé©—è­‰é›† (80/20 of train = 64/16/20)
        X_train_final, X_val, y_train_final, y_val = train_test_split(
            X_train, y_train, 
            test_size=0.2, 
            random_state=42, 
            stratify=y_train
        )
        
        # åŒæ™‚éœ€è¦åˆ†å‰² features_df ä»¥ä¿æŒç´¢å¼•å°é½Š
        train_idx = X_train_final.index
        val_idx = X_val.index
        test_idx = X_test.index
        
        features_df_train = features_df.loc[train_idx]
        features_df_val = features_df.loc[val_idx]
        features_df_test = features_df.loc[test_idx]
        
        print(f"   âœ… è³‡æ–™åˆ†å‰²å®Œæˆï¼š")
        print(f"      è¨“ç·´é›†ï¼š{len(X_train_final):,} ç­† ({len(X_train_final)/len(X)*100:.1f}%)")
        print(f"      é©—è­‰é›†ï¼š{len(X_val):,} ç­† ({len(X_val)/len(X)*100:.1f}%)")
        print(f"      æ¸¬è©¦é›†ï¼š{len(X_test):,} ç­† ({len(X_test)/len(X)*100:.1f}%)")
        print(f"      è¨“ç·´é›†ç•°å¸¸æ¯”ä¾‹ï¼š{(y_train_final == 1).sum()/len(y_train_final)*100:.2f}%")
        print(f"      é©—è­‰é›†ç•°å¸¸æ¯”ä¾‹ï¼š{(y_val == 1).sum()/len(y_val)*100:.2f}%")
        print(f"      æ¸¬è©¦é›†ç•°å¸¸æ¯”ä¾‹ï¼š{(y_test == 1).sum()/len(y_test)*100:.2f}%")
        print(f"   ğŸ’¡ å°‡åœ¨è¨“ç·´é›†ä¸Šè¨“ç·´æ¨¡å‹ï¼Œåœ¨é©—è­‰é›†ä¸Šå„ªåŒ–é–¾å€¼ï¼Œåœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°")
    else:
        # ç„¡æ¨™ç±¤æ™‚ï¼Œä½¿ç”¨å…¨éƒ¨è³‡æ–™
        print(f"   âš ï¸  ç„¡æ¨™ç±¤è³‡æ–™ï¼Œä½¿ç”¨å…¨éƒ¨è³‡æ–™é€²è¡Œè¨“ç·´å’Œé æ¸¬")
        X_train_final = X
        X_val = None
        X_test = X
        y_train_final = None
        y_val = None
        y_test = None
        features_df_train = features_df
        features_df_val = None
        features_df_test = features_df
        train_idx = X.index
        val_idx = None
        test_idx = X.index
    
    # 5. è¨“ç·´æ¨¡å‹ï¼ˆé …ç›® 4ï¼šæŒ‰å”è­°åˆ†çµ„è¨“ç·´ï¼‰
    print("\n[æ­¥é©Ÿ 5] è¨“ç·´ Isolation Forest æ¨¡å‹...")
    
    # æª¢æŸ¥æ˜¯å¦æœ‰å”è­°æ¬„ä½
    use_protocol_grouping = 'Proto' in features_df_train.columns
    
    if use_protocol_grouping:
        print("\n[æ­¥é©Ÿ 5.1] æŒ‰å”è­°åˆ†çµ„è¨“ç·´ï¼ˆåƒ…ä¸»è¦å”è­°ï¼šTCP/UDPï¼‰...")
        print("   ğŸ’¡ ä½¿ç”¨è¨“ç·´é›†é€²è¡Œæ¨¡å‹è¨“ç·´...")
        
        # ä½¿ç”¨çµ±ä¸€çš„å”è­°åˆ†çµ„è¨“ç·´å‡½æ•¸ï¼ˆåœ¨è¨“ç·´é›†ä¸Šï¼‰
        protocol_models, protocol_scalers, protocol_contaminations, success = train_protocol_grouped_models(
            X_train_final,
            features_df_train,
            contamination=contamination,
            feature_robust_scaler=feature_robust_scaler,
            main_protocols=['tcp', 'udp'],
            min_samples=1000,
            random_state=42,
            n_estimators=300,
            max_samples='auto',
            bootstrap=True,
            verbose=True
        )
        
        if not success:
            use_protocol_grouping = False
            protocol_models = None
            protocol_scalers = None
            model = None
            scaler = None
        else:
            model = None
            scaler = None
    else:
        use_protocol_grouping = False
        protocol_models = None
        protocol_scalers = None
    
    # å¦‚æœæ²’æœ‰å”è­°åˆ†çµ„æˆ–å”è­°åˆ†çµ„å¤±æ•—ï¼Œä½¿ç”¨å–®ä¸€æ¨¡å‹
    if not use_protocol_grouping:
        print("\n[æ­¥é©Ÿ 5.2] å–®ä¸€æ¨¡å‹è¨“ç·´ï¼ˆæœªä½¿ç”¨å”è­°åˆ†çµ„ï¼‰...")
        print("   ğŸ’¡ ä½¿ç”¨è¨“ç·´é›†é€²è¡Œæ¨¡å‹è¨“ç·´...")
        
        # ä½¿ç”¨çµ±ä¸€çš„å–®ä¸€æ¨¡å‹è¨“ç·´å‡½æ•¸ï¼ˆåœ¨è¨“ç·´é›†ä¸Šï¼‰
        model, scaler = train_single_model(
            X_train_final,
            contamination=contamination,
            feature_robust_scaler=feature_robust_scaler,
            random_state=42,
            n_estimators=300,
            max_samples='auto',
            bootstrap=True,
            verbose=True
        )
    
    # 6. åœ¨è¨“ç·´é›†ä¸Šé æ¸¬ï¼ˆç”¨æ–¼å¾ŒçºŒç™½åå–®åˆ†æï¼‰
    print("\n[æ­¥é©Ÿ 6] åœ¨è¨“ç·´é›†ä¸Šé æ¸¬...")
    
    if use_protocol_grouping:
        train_anomaly_scores = np.zeros(len(X_train_final))
        
        for protocol, model_proto in protocol_models.items():
            if protocol == 'other':
                proto_mask = ~features_df_train['Proto'].str.lower().isin(['tcp', 'udp'])
            else:
                proto_mask = features_df_train['Proto'].str.lower() == protocol.lower()
            
            X_train_proto = X_train_final[proto_mask]
            if len(X_train_proto) == 0:
                continue
            
            anomaly_scores_proto = model_proto.predict(X_train_proto)
            anomaly_scores_proto_normalized = -anomaly_scores_proto
            
            if len(anomaly_scores_proto_normalized) > 0:
                score_mean = anomaly_scores_proto_normalized.mean()
                score_std = anomaly_scores_proto_normalized.std()
                if score_std > 1e-6:
                    scores_normalized = (anomaly_scores_proto_normalized - score_mean) / score_std
                    scores_normalized = 1 / (1 + np.exp(-scores_normalized))
                else:
                    scores_normalized = np.zeros_like(anomaly_scores_proto_normalized)
                train_anomaly_scores[proto_mask] = scores_normalized
        
        train_anomaly_scores_normalized = -train_anomaly_scores
    else:
        train_anomaly_scores = model.predict(X_train_final)
        train_anomaly_scores_normalized = -train_anomaly_scores
    
    print(f"   è¨“ç·´é›†ç•°å¸¸åˆ†æ•¸ç¯„åœï¼š[{train_anomaly_scores_normalized.min():.4f}, {train_anomaly_scores_normalized.max():.4f}]")
    
    # 7. åœ¨é©—è­‰é›†ä¸Šå„ªåŒ–é–¾å€¼ï¼ˆå¦‚æœæœ‰æ¨™ç±¤ï¼‰
    best_threshold = None
    val_anomaly_scores_normalized = None
    
    if use_data_split and y_val is not None:
        print("\n[æ­¥é©Ÿ 7] åœ¨é©—è­‰é›†ä¸Šå„ªåŒ–é–¾å€¼...")
        print("   ğŸ’¡ ä½¿ç”¨é©—è­‰é›†å„ªåŒ–é–¾å€¼ï¼ˆé¿å…åœ¨æ¸¬è©¦é›†ä¸Šå·çœ‹ç­”æ¡ˆï¼‰...")
        
        # åœ¨é©—è­‰é›†ä¸Šé æ¸¬
        if use_protocol_grouping:
            # å”è­°åˆ†çµ„é æ¸¬ï¼ˆé©—è­‰é›†ï¼‰
            val_anomaly_scores = np.zeros(len(X_val))
            
            for protocol, model_proto in protocol_models.items():
                if protocol == 'other':
                    proto_mask = ~features_df_val['Proto'].str.lower().isin(['tcp', 'udp'])
                else:
                    proto_mask = features_df_val['Proto'].str.lower() == protocol.lower()
                
                X_val_proto = X_val[proto_mask]
                if len(X_val_proto) == 0:
                    continue
                
                anomaly_scores_proto = model_proto.predict(X_val_proto)
                anomaly_scores_proto_normalized = -anomaly_scores_proto
                
                # æ¨™æº–åŒ–ï¼ˆä½¿ç”¨è¨“ç·´é›†çš„çµ±è¨ˆé‡ï¼‰
                if len(anomaly_scores_proto_normalized) > 0:
                    # ç²å–è¨“ç·´é›†ä¸Šè©²å”è­°çš„çµ±è¨ˆé‡ï¼ˆéœ€è¦å¾è¨“ç·´æ™‚ä¿å­˜ï¼‰
                    # ç°¡åŒ–è™•ç†ï¼šä½¿ç”¨é©—è­‰é›†è‡ªå·±çš„çµ±è¨ˆé‡
                    score_mean = anomaly_scores_proto_normalized.mean()
                    score_std = anomaly_scores_proto_normalized.std()
                    if score_std > 1e-6:
                        scores_normalized = (anomaly_scores_proto_normalized - score_mean) / score_std
                        scores_normalized = 1 / (1 + np.exp(-scores_normalized))
                    else:
                        scores_normalized = np.zeros_like(anomaly_scores_proto_normalized)
                    val_anomaly_scores[proto_mask] = scores_normalized
            
            val_anomaly_scores_normalized = -val_anomaly_scores
        else:
            # å–®ä¸€æ¨¡å‹é æ¸¬ï¼ˆé©—è­‰é›†ï¼‰
            val_anomaly_scores = model.predict(X_val)
            val_anomaly_scores_normalized = -val_anomaly_scores
        
        # ä½¿ç”¨ precision_recall_curve åœ¨é©—è­‰é›†ä¸Šæ‰¾æœ€ä½³é–¾å€¼
        precision, recall, thresholds = precision_recall_curve(y_val, val_anomaly_scores_normalized)
        f1_scores = 2 * recall * precision / (recall + precision + 1e-10)
        
        valid_f1_scores = f1_scores[:len(thresholds)]
        best_idx = np.nanargmax(valid_f1_scores)
        best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else None
        
        if best_threshold is not None:
            # æª¢æŸ¥æœ€ä½³é–¾å€¼å°æ‡‰çš„ç•°å¸¸æ¯”ä¾‹
            y_pred_val = (val_anomaly_scores_normalized >= best_threshold).astype(int)
            anomaly_ratio_val = y_pred_val.sum() / len(y_pred_val)
            
            # è¨ˆç®—é©—è­‰é›†ä¸Šçš„æŒ‡æ¨™
            tp_val = ((y_pred_val == 1) & (y_val == 1)).sum()
            fp_val = ((y_pred_val == 1) & (y_val == 0)).sum()
            fn_val = ((y_pred_val == 0) & (y_val == 1)).sum()
            precision_val = tp_val / (tp_val + fp_val) if (tp_val + fp_val) > 0 else 0
            recall_val = tp_val / (tp_val + fn_val) if (tp_val + fn_val) > 0 else 0
            f1_val = 2 * (precision_val * recall_val) / (precision_val + recall_val + 1e-10)
            
            print(f"   âœ… åœ¨é©—è­‰é›†ä¸Šæ‰¾åˆ°æœ€ä½³é–¾å€¼ï¼š{best_threshold:.4f}")
            print(f"      é©—è­‰é›†æŒ‡æ¨™ï¼šPrecision={precision_val:.4f}, Recall={recall_val:.4f}, F1={f1_val:.4f}")
            print(f"      é©—è­‰é›†ç•°å¸¸æ¯”ä¾‹ï¼š{anomaly_ratio_val*100:.2f}%")
        else:
            # å›é€€åˆ°ä½¿ç”¨ contamination çš„ç™¾åˆ†ä½æ•¸
            best_threshold = np.percentile(val_anomaly_scores_normalized, 100 * (1 - contamination))
            print(f"   âš ï¸  ç„¡æ³•å¾ PR æ›²ç·šæ‰¾åˆ°æœ€ä½³é–¾å€¼ï¼Œä½¿ç”¨ contamination ç™¾åˆ†ä½æ•¸ï¼š{best_threshold:.4f}")
    
    # 8. åœ¨æ¸¬è©¦é›†ä¸Šé æ¸¬
    print("\n[æ­¥é©Ÿ 8] åœ¨æ¸¬è©¦é›†ä¸Šé æ¸¬...")
    print("   ğŸ’¡ ä½¿ç”¨é©—è­‰é›†æ‰¾åˆ°çš„æœ€ä½³é–¾å€¼é€²è¡Œé æ¸¬ï¼ˆé¿å…åœ¨æ¸¬è©¦é›†ä¸Šå·çœ‹ç­”æ¡ˆï¼‰...")
    
    # åœ¨æ¸¬è©¦é›†ä¸Šé æ¸¬
    if use_protocol_grouping:
        # å”è­°åˆ†çµ„é æ¸¬ï¼ˆæ¸¬è©¦é›†ï¼‰
        test_anomaly_scores = np.zeros(len(X_test))
        
        for protocol, model_proto in protocol_models.items():
            if protocol == 'other':
                proto_mask = ~features_df_test['Proto'].str.lower().isin(['tcp', 'udp'])
            else:
                proto_mask = features_df_test['Proto'].str.lower() == protocol.lower()
            
            X_test_proto = X_test[proto_mask]
            if len(X_test_proto) == 0:
                continue
            
            anomaly_scores_proto = model_proto.predict(X_test_proto)
            anomaly_scores_proto_normalized = -anomaly_scores_proto
            
            # æ¨™æº–åŒ–ï¼ˆä½¿ç”¨è¨“ç·´é›†çš„çµ±è¨ˆé‡ï¼Œä½†é€™è£¡ç°¡åŒ–ä½¿ç”¨æ¸¬è©¦é›†è‡ªå·±çš„çµ±è¨ˆé‡ï¼‰
            # æ³¨æ„ï¼šç†æƒ³æƒ…æ³ä¸‹æ‡‰è©²ä½¿ç”¨è¨“ç·´é›†çš„çµ±è¨ˆé‡ï¼Œä½†ç‚ºäº†ç°¡åŒ–ï¼Œé€™è£¡ä½¿ç”¨æ¸¬è©¦é›†è‡ªå·±çš„
            if len(anomaly_scores_proto_normalized) > 0:
                score_mean = anomaly_scores_proto_normalized.mean()
                score_std = anomaly_scores_proto_normalized.std()
                if score_std > 1e-6:
                    scores_normalized = (anomaly_scores_proto_normalized - score_mean) / score_std
                    scores_normalized = 1 / (1 + np.exp(-scores_normalized))
                else:
                    scores_normalized = np.zeros_like(anomaly_scores_proto_normalized)
                test_anomaly_scores[proto_mask] = scores_normalized
        
        test_anomaly_scores_normalized = -test_anomaly_scores
    else:
        # å–®ä¸€æ¨¡å‹é æ¸¬ï¼ˆæ¸¬è©¦é›†ï¼‰
        test_anomaly_scores = model.predict(X_test)
        test_anomaly_scores_normalized = -test_anomaly_scores
    
    print(f"   æ¸¬è©¦é›†ç•°å¸¸åˆ†æ•¸ç¯„åœï¼š[{test_anomaly_scores_normalized.min():.4f}, {test_anomaly_scores_normalized.max():.4f}]")
    print(f"   æ¸¬è©¦é›†å¹³å‡ç•°å¸¸åˆ†æ•¸ï¼š{test_anomaly_scores_normalized.mean():.4f}")
    
    # ä½¿ç”¨é©—è­‰é›†æ‰¾åˆ°çš„æœ€ä½³é–¾å€¼ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå¦å‰‡ä½¿ç”¨ contamination ç™¾åˆ†ä½æ•¸
    if best_threshold is not None:
        print(f"   âœ… ä½¿ç”¨é©—è­‰é›†æ‰¾åˆ°çš„æœ€ä½³é–¾å€¼ï¼š{best_threshold:.4f}")
        y_pred_test = (test_anomaly_scores_normalized >= best_threshold).astype(int)
    else:
        # ç„¡æ¨™ç±¤æˆ–ç„¡æ³•å„ªåŒ–é–¾å€¼æ™‚ï¼Œä½¿ç”¨ contamination ç™¾åˆ†ä½æ•¸
        threshold_test = np.percentile(test_anomaly_scores_normalized, 100 * (1 - contamination))
        print(f"   âš ï¸  ä½¿ç”¨ contamination ç™¾åˆ†ä½æ•¸é–¾å€¼ï¼š{threshold_test:.4f}")
        y_pred_test = (test_anomaly_scores_normalized >= threshold_test).astype(int)
    
    print(f"   æ¸¬è©¦é›†é æ¸¬ç•°å¸¸æ•¸é‡ï¼š{y_pred_test.sum():,} ({y_pred_test.sum()/len(y_pred_test)*100:.2f}%)")
    
    # 9. ä¿å­˜è¨“ç·´çµæœ
    print("\n[æ­¥é©Ÿ 9] ä¿å­˜è¨“ç·´çµæœ...")
    save_training_results(
        output_dir=output_dir,
        model=model,
        scaler=scaler,
        protocol_models=protocol_models,
        protocol_scalers=protocol_scalers,
        X_train=X_train_final,
        X_val=X_val,
        X_test=X_test,
        features_df_train=features_df_train,
        features_df_val=features_df_val,
        features_df_test=features_df_test,
        y_train=y_train_final,
        y_val=y_val,
        y_test=y_test,
        train_anomaly_scores=train_anomaly_scores_normalized,
        val_anomaly_scores=val_anomaly_scores_normalized,
        test_anomaly_scores=test_anomaly_scores_normalized,
        best_threshold=best_threshold,
        contamination=contamination,
        use_protocol_grouping=use_protocol_grouping,
        feature_robust_scaler=feature_robust_scaler,
        transformed_feature_cols=transformed_feature_cols,  # 28å€‹ï¼šè½‰æ›å¾Œçš„ç‰¹å¾µ
        final_feature_cols=final_feature_cols  # 15å€‹ï¼šæœ€çµ‚ç”¨æ–¼æ¨¡å‹è¨“ç·´çš„ç‰¹å¾µ
    )
    
    # 10. åŸºæœ¬è©•ä¼°ï¼ˆå¦‚æœæœ‰æ¨™ç±¤ï¼‰
    if use_data_split and y_test is not None:
        print("\n[æ­¥é©Ÿ 10] æ¸¬è©¦é›†åŸºæœ¬è©•ä¼°...")
        print("   ğŸ’¡ åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°åŸºæœ¬æ€§èƒ½ï¼ˆæœªæ‡‰ç”¨ç™½åå–®ï¼‰...")
        print("   ğŸ’¡ å®Œæ•´è©•ä¼°ï¼ˆå«ç™½åå–®ï¼‰è«‹ä½¿ç”¨ postprocess_with_whitelist.py")
        
        if best_threshold is not None:
            y_pred_test = (test_anomaly_scores_normalized >= best_threshold).astype(int)
        else:
            threshold_test = np.percentile(test_anomaly_scores_normalized, 100 * (1 - contamination))
            y_pred_test = (test_anomaly_scores_normalized >= threshold_test).astype(int)
        
        evaluate_and_print(
            y_test, y_pred_test,
            show_confusion_matrix=True,
            show_summary=True
        )
    elif y_true is not None:
        # ç„¡è³‡æ–™åˆ†å‰²ä½†æœ‰æ¨™ç±¤ï¼ˆèˆŠçš„è©•ä¼°æ–¹å¼ï¼Œåƒ…ç”¨æ–¼å‘å¾Œå…¼å®¹ï¼‰
        print("\n[æ­¥é©Ÿ 10] æ¨¡å‹è©•ä¼°ï¼ˆè­¦å‘Šï¼šæœªé€²è¡Œè³‡æ–™åˆ†å‰²ï¼‰...")
        print("   âš ï¸  è­¦å‘Šï¼šæœªé€²è¡Œ train/test åˆ†å‰²ï¼Œè©•ä¼°çµæœå¯èƒ½éæ–¼æ¨‚è§€")
        
        if best_threshold is not None:
            y_pred_test = (test_anomaly_scores_normalized >= best_threshold).astype(int)
        else:
            threshold_test = np.percentile(test_anomaly_scores_normalized, 100 * (1 - contamination))
            y_pred_test = (test_anomaly_scores_normalized >= threshold_test).astype(int)
        
        evaluate_and_print(
            y_true, y_pred_test,
            show_confusion_matrix=True,
            show_detailed=True,
            indent="  "
        )
    
    # èˆŠçš„è©•ä¼°ä»£ç¢¼ï¼ˆä¿ç•™ç”¨æ–¼å‘å¾Œå…¼å®¹ï¼Œä½†å·²ç°¡åŒ–ï¼‰
    # 8. åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°ï¼ˆå¦‚æœæœ‰çœŸå¯¦æ¨™ç±¤ï¼‰
    if False and use_data_split and y_test is not None:
        print("\n[æ­¥é©Ÿ 8] æ¸¬è©¦é›†æ¨¡å‹è©•ä¼°...")
        print("   ğŸ’¡ åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°æœ€çµ‚æ€§èƒ½ï¼ˆæœªä½¿ç”¨æ¸¬è©¦é›†æ¨™ç±¤å„ªåŒ–é–¾å€¼ï¼‰...")
        pass
    
    total_time = time.time() - start_time
    print("\n" + "=" * 60)
    print(f"âœ… æ¨¡å‹è¨“ç·´å®Œæˆï¼ˆç¸½è€—æ™‚ï¼š{total_time:.2f} ç§’ï¼‰")
    print(f"   è³‡æ–™è¼‰å…¥ï¼š{load_time:.2f} ç§’")
    print(f"   è¨“ç·´çµæœå·²ä¿å­˜è‡³ï¼š{output_dir}")
    print(f"   ğŸ’¡ ä¸‹ä¸€æ­¥ï¼šåŸ·è¡Œ postprocess_with_whitelist.py é€²è¡Œç™½åå–®å¾Œè™•ç†")
    print("=" * 60)

if __name__ == "__main__":
    main()

