"""
å¿«é€Ÿæ¨¡å‹åŸºæº–æ¸¬è©¦å·¥å…·ï¼ˆä½¿ç”¨æŠ½æ¨£è³‡æ–™ï¼‰

ä½¿ç”¨æŠ½æ¨£è³‡æ–™å¿«é€Ÿè©•ä¼°å’Œæ¯”è¼ƒå¤šå€‹ç„¡ç›£ç£ç•°å¸¸æª¢æ¸¬æ¨¡å‹ï¼Œ
å”åŠ©é¸æ“‡æœ€ä½³æ¨¡å‹é€²è¡Œå®Œæ•´è¨“ç·´ã€‚

è¨­è¨ˆæ¨¡å¼ï¼š
- Factory Pattern: ä½¿ç”¨ ModelFactory å‰µå»ºä¸åŒæ¨¡å‹
- Benchmark Pattern: çµ±ä¸€è³‡æ–™å’Œè©•ä¼°æ¨™æº–é€²è¡Œå…¬å¹³æ¯”è¼ƒ

ä½¿ç”¨æ–¹æ³•ï¼š
    # åŸ·è¡Œæ‰€æœ‰æ¨¡å‹ï¼ˆé è¨­ï¼‰
    python scripts/unsupervised_model_selection/quick_model_benchmark.py
    
    # åŸ·è¡Œå–®ä¸€æ¨¡å‹
    python scripts/unsupervised_model_selection/quick_model_benchmark.py --model isolation_forest
    python scripts/unsupervised_model_selection/quick_model_benchmark.py --model lof
    python scripts/unsupervised_model_selection/quick_model_benchmark.py --model one_class_svm
    
    # ä½¿ç”¨åˆ¥å
    python scripts/unsupervised_model_selection/quick_model_benchmark.py --model if
    python scripts/unsupervised_model_selection/quick_model_benchmark.py --model ocsvm
    
    # æ¯”è¼ƒçµæœ
    python scripts/unsupervised_model_selection/compare_model_results.py
"""
import sys
import time
import json
from pathlib import Path
import argparse

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥ Python è·¯å¾‘
PROJECT_ROOT = Path(__file__).parent.parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src import (
    DataLoaderFactory,
    DataSourceType,
    ModelFactory,
    ModelType,
    extract_features,
    transform_features_for_unsupervised,
    DEFAULT_SKEWED_FEATURES,
    convert_label_to_binary,
    prepare_feature_set,
    FeatureSelector,
    FeatureSelectionStrategy,
    StandardFeatureProcessor,
    calculate_metrics
)
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
import numpy as np
import pandas as pd

# æ¨¡å‹åç¨±æ˜ å°„
MODEL_MAP = {
    'isolation_forest': ModelType.ISOLATION_FOREST,
    'if': ModelType.ISOLATION_FOREST,
    'lof': ModelType.LOCAL_OUTLIER_FACTOR,
    'local_outlier_factor': ModelType.LOCAL_OUTLIER_FACTOR,
    'one_class_svm': ModelType.ONE_CLASS_SVM,
    'ocsvm': ModelType.ONE_CLASS_SVM,
    'svm': ModelType.ONE_CLASS_SVM,
}

# æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼ˆç”¨æ–¼é è¨­åŸ·è¡Œæ‰€æœ‰æ¨¡å‹ï¼‰
ALL_MODELS = ['isolation_forest', 'lof', 'one_class_svm']


def print_feature_breakdown(features_df, transformed_feature_cols, actual_stage):
    """
    æ‹†åˆ†ä¸¦åˆ—å‡ºæ¯å€‹éšæ®µçš„ç‰¹å¾µ
    
    Args:
        features_df: åŸå§‹ç‰¹å¾µ DataFrame
        transformed_feature_cols: è½‰æ›å¾Œçš„ç‰¹å¾µæ¬„ä½åˆ—è¡¨
        actual_stage: å¯¦éš›ä½¿ç”¨çš„ç‰¹å¾µéšæ®µ
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š ç‰¹å¾µéšæ®µæ‹†åˆ†èˆ‡æ¸…å–®")
    print("=" * 60)
    
    # å®šç¾©å„éšæ®µç‰¹å¾µ
    base_features = [
        'DstBytes', 'flow_ratio', 'bytes_symmetry', 'is_scanning',
        'src_ratio', 'dst_ratio', 'packet_size', 'bytes_per_second', 'packets_per_second'
    ]
    
    time_features_stage1 = [
        'hour', 'day_of_week', 'day_of_month', 'is_weekend', 'is_work_hour', 'is_night',
        'sin_hour', 'cos_hour', 'sin_day_of_week', 'cos_day_of_week', 
        'sin_day_of_month', 'cos_day_of_month'
    ]
    
    time_features_stage2 = [
        'time_since_last_flow', 'time_to_next_flow'
    ]
    
    time_features_stage3 = [
        'flows_per_minute_by_src', 'unique_dst_per_minute_by_src',
        'unique_dport_per_minute_by_src', 'total_bytes_per_minute_by_src'
    ]
    
    time_features_stage4 = [
        'bidirectional_flow_count', 'bidirectional_total_bytes', 'bidirectional_total_packets',
        'bidirectional_total_src_bytes', 'bidirectional_total_dst_bytes', 'bidirectional_symmetry',
        'bidirectional_avg_bytes_per_flow', 'bidirectional_avg_packets_per_flow', 'bidirectional_avg_duration',
        'bidirectional_window_flow_ratio'  # æ™‚é–“çª—å£å…§èšåˆå¾Œçš„ä¸Šä¸‹è¡Œæµé‡æ¯”
    ]
    
    # å¾å¯¦éš› DataFrame ä¸­æ‰¾å‡ºå­˜åœ¨çš„ç‰¹å¾µ
    all_stage_features = {
        'åŸºç¤ç‰¹å¾µ': base_features,
        'éšæ®µ1ï¼ˆåŸºæœ¬æ™‚é–“ç‰¹å¾µï¼‰': time_features_stage1,
        'éšæ®µ2ï¼ˆæ™‚é–“é–“éš”ç‰¹å¾µï¼‰': time_features_stage2,
        'éšæ®µ3ï¼ˆæ™‚é–“çª—å£èšåˆç‰¹å¾µï¼‰': time_features_stage3,
        'éšæ®µ4ï¼ˆé›™å‘æµ Pair èšåˆç‰¹å¾µï¼‰': time_features_stage4
    }
    
    # åŸå§‹ç‰¹å¾µæ‹†åˆ†
    print("\n[åŸå§‹ç‰¹å¾µæ‹†åˆ†]")
    original_by_stage = {}
    for stage_name, stage_features in all_stage_features.items():
        if 'éšæ®µ4' in stage_name and actual_stage < 4:
            continue
        if 'éšæ®µ3' in stage_name and actual_stage < 3:
            continue
        if 'éšæ®µ2' in stage_name and actual_stage < 2:
            continue
        
        available = [f for f in stage_features if f in features_df.columns]
        if available:
            original_by_stage[stage_name] = available
            print(f"\n  {stage_name} ({len(available)} å€‹):")
            for feat in available:
                print(f"    - {feat}")
    
    # æ‰¾å‡ºå…¶ä»–åŸå§‹ç‰¹å¾µï¼ˆä¸åœ¨å®šç¾©åˆ—è¡¨ä¸­çš„ï¼‰
    defined_features = set()
    for features in all_stage_features.values():
        defined_features.update(features)
    
    other_original = [col for col in features_df.columns 
                     if col not in defined_features 
                     and col not in ['Label', 'StartTime', 'SrcAddr', 'DstAddr', 'Sport', 'Dport', 'State', 'Proto']
                     and pd.api.types.is_numeric_dtype(features_df[col])]
    
    if other_original:
        print(f"\n  å…¶ä»–åŸå§‹ç‰¹å¾µ ({len(other_original)} å€‹):")
        for feat in sorted(other_original)[:20]:  # åªé¡¯ç¤ºå‰20å€‹
            print(f"    - {feat}")
        if len(other_original) > 20:
            print(f"    ... é‚„æœ‰ {len(other_original) - 20} å€‹ç‰¹å¾µ")
    
    # è½‰æ›å¾Œç‰¹å¾µæ‹†åˆ†
    print("\n[è½‰æ›å¾Œç‰¹å¾µæ‹†åˆ†]")
    if transformed_feature_cols:
        # åˆ†é¡è½‰æ›å¾Œçš„ç‰¹å¾µ
        log_features = [f for f in transformed_feature_cols if f.startswith('log_')]
        stage4_transformed = [f for f in transformed_feature_cols if 'bidirectional' in f]
        stage3_transformed = [f for f in transformed_feature_cols if any(s in f for s in ['per_minute_by_src'])]
        stage2_transformed = [f for f in transformed_feature_cols if any(s in f for s in ['time_since', 'time_to'])]
        stage1_transformed = [f for f in transformed_feature_cols if f in time_features_stage1]
        base_transformed = [f for f in transformed_feature_cols if f in base_features]
        other_transformed = [f for f in transformed_feature_cols 
                           if f not in log_features 
                           and f not in stage4_transformed 
                           and f not in stage3_transformed 
                           and f not in stage2_transformed 
                           and f not in stage1_transformed 
                           and f not in base_transformed]
        
        print(f"\n  Log è½‰æ›ç‰¹å¾µ ({len(log_features)} å€‹):")
        for feat in sorted(log_features):
            print(f"    - {feat}")
        
        if base_transformed:
            print(f"\n  åŸºç¤ç‰¹å¾µ ({len(base_transformed)} å€‹):")
            for feat in sorted(base_transformed):
                print(f"    - {feat}")
        
        if stage1_transformed:
            print(f"\n  éšæ®µ1ç‰¹å¾µ ({len(stage1_transformed)} å€‹):")
            for feat in sorted(stage1_transformed):
                print(f"    - {feat}")
        
        if stage2_transformed:
            print(f"\n  éšæ®µ2ç‰¹å¾µ ({len(stage2_transformed)} å€‹):")
            for feat in sorted(stage2_transformed):
                print(f"    - {feat}")
        
        if stage3_transformed:
            print(f"\n  éšæ®µ3ç‰¹å¾µ ({len(stage3_transformed)} å€‹):")
            for feat in sorted(stage3_transformed):
                print(f"    - {feat}")
        
        if stage4_transformed:
            print(f"\n  éšæ®µ4ç‰¹å¾µ ({len(stage4_transformed)} å€‹):")
            for feat in sorted(stage4_transformed):
                print(f"    - {feat}")
        
        if other_transformed:
            print(f"\n  å…¶ä»–è½‰æ›ç‰¹å¾µ ({len(other_transformed)} å€‹):")
            for feat in sorted(other_transformed):
                print(f"    - {feat}")
    
    # çµ±è¨ˆæ‘˜è¦
    print("\n[çµ±è¨ˆæ‘˜è¦]")
    print(f"  åŸå§‹ç‰¹å¾µç¸½æ•¸: {features_df.shape[1]} å€‹")
    print(f"  è½‰æ›å¾Œç‰¹å¾µç¸½æ•¸: {len(transformed_feature_cols) if transformed_feature_cols else 0} å€‹")
    print(f"  å¯¦éš›ä½¿ç”¨éšæ®µ: {actual_stage}")
    
    # å®Œæ•´ç‰¹å¾µæ¸…å–®
    print("\n" + "=" * 60)
    print("ğŸ“‹ å®Œæ•´è½‰æ›å¾Œç‰¹å¾µæ¸…å–®")
    print("=" * 60)
    if transformed_feature_cols:
        for i, feat in enumerate(sorted(transformed_feature_cols), 1):
            print(f"  {i:2d}. {feat}")
    else:
        print("  ï¼ˆç„¡è½‰æ›å¾Œç‰¹å¾µï¼‰")
    
    print("=" * 60)


def prepare_data():
    """æº–å‚™è¨“ç·´å’Œæ¸¬è©¦è³‡æ–™"""
    print("=" * 60)
    print("æº–å‚™è³‡æ–™")
    print("=" * 60)
    
    # 1. è¼‰å…¥è³‡æ–™
    print("\n[æ­¥é©Ÿ 1] è¼‰å…¥è³‡æ–™...")
    start_time = time.time()
    
    parquet_path = Path("data/processed/capture20110817_cleaned_spark.parquet")
    if not parquet_path.exists():
        raise FileNotFoundError(
            f"æ‰¾ä¸åˆ° Parquet æª”æ¡ˆ: {parquet_path}\n"
            f"è«‹å…ˆåŸ·è¡Œè³‡æ–™è™•ç†è…³æœ¬ç”Ÿæˆ Parquet æª”æ¡ˆã€‚"
        )
    
    print(f"   ä½¿ç”¨ Pandas è®€å– Parquet: {parquet_path}")
    raw_df = pd.read_parquet(parquet_path, engine='pyarrow')
    
    load_time = time.time() - start_time
    print(f"âœ… è¼‰å…¥å®Œæˆï¼š{len(raw_df):,} ç­†è³‡æ–™ï¼ˆè€—æ™‚ {load_time:.2f} ç§’ï¼‰")
    
    # 2. æ¸…æ´—è³‡æ–™
    print("\n[æ­¥é©Ÿ 2] æ¸…æ´—è³‡æ–™...")
    loader = DataLoaderFactory.create(DataSourceType.BIDIRECTIONAL_BINETFLOW)
    cleaned_df = loader.clean(raw_df)
    print(f"âœ… æ¸…æ´—å®Œæˆï¼š{len(cleaned_df):,} ç­†è³‡æ–™")
    
    # 3. ç‰¹å¾µè™•ç†ï¼ˆä½¿ç”¨ FeatureProcessorï¼‰
    print("\n[æ­¥é©Ÿ 3] ç‰¹å¾µè™•ç†...")
    print("   ä½¿ç”¨éšæ®µ4æ™‚é–“ç‰¹å¾µï¼ˆæœ€å®Œæ•´ï¼šåŒ…å«æ‰€æœ‰éšæ®µç‰¹å¾µï¼‰")
    print("   - éšæ®µ1ï¼šåŸºæœ¬æ™‚é–“ç‰¹å¾µ")
    print("   - éšæ®µ2ï¼šæ™‚é–“é–“éš”ç‰¹å¾µ")
    print("   - éšæ®µ3ï¼šæ™‚é–“çª—å£èšåˆç‰¹å¾µï¼ˆæŒ‰ SrcAddrï¼‰")
    print("   - éšæ®µ4ï¼šé›™å‘æµ Pair èšåˆç‰¹å¾µï¼ˆæŒ‰ IP Pairï¼Œéœ€è¦ PySparkï¼‰")
    
    processor = StandardFeatureProcessor(time_feature_stage=4)
    
    # æª¢æŸ¥æ˜¯å¦å·²æœ‰è™•ç†å¥½çš„ç‰¹å¾µï¼ˆåˆ†éšæ®µæª¢æŸ¥ï¼‰
    features_stage3_path = Path("data/processed/features_stage3.parquet")
    features_stage4_path = Path("data/processed/features_stage4.parquet")
    transformed_cache_path = Path("data/processed/features_transformed.parquet")
    
    if features_stage4_path.exists() and transformed_cache_path.exists():
        print(f"\n   ğŸ’¾ ç™¼ç¾å·²è™•ç†çš„ç‰¹å¾µå¿«å–ï¼Œç›´æ¥è¼‰å…¥...")
        cache_start_time = time.time()
        
        # è¼‰å…¥åŸå§‹ç‰¹å¾µ
        features_df = processor.load_features()
        
        # æª¢æŸ¥ä¸¦æ·»åŠ  bidirectional_window_flow_ratioï¼ˆå¦‚æœä¸å­˜åœ¨ä½†æ‡‰è©²å­˜åœ¨ï¼‰
        if (processor.time_feature_stage == 4 and 
            'bidirectional_window_flow_ratio' not in features_df.columns and
            'bidirectional_total_src_bytes' in features_df.columns and
            'bidirectional_total_dst_bytes' in features_df.columns):
            print("   ğŸ”§ æª¢æ¸¬åˆ°ç¼ºå°‘ bidirectional_window_flow_ratioï¼Œæ­£åœ¨è¨ˆç®—ä¸¦æ·»åŠ ...")
            features_df['bidirectional_window_flow_ratio'] = (
                features_df['bidirectional_total_src_bytes'].astype(float) / 
                (features_df['bidirectional_total_dst_bytes'].astype(float) + 1)
            ).fillna(0.0).replace([np.inf, -np.inf], 0.0)
            print("   âœ… bidirectional_window_flow_ratio å·²æ·»åŠ ")
        
        # è¼‰å…¥è½‰æ›å¾Œçš„ç‰¹å¾µå’Œ scaler
        X_transformed, robust_scaler, transformed_feature_cols = processor.load_transformed_features()

        # ç¢ºä¿åªä¿ç•™è½‰æ›å¾Œçš„ç‰¹å¾µæ¬„ä½ï¼ˆé‡è¦ï¼šåªä¿ç•™ scaler è¨“ç·´æ™‚ä½¿ç”¨çš„ç‰¹å¾µï¼‰
        if transformed_feature_cols:
            # åªä¿ç•™ scaler è¨“ç·´æ™‚ä½¿ç”¨çš„ç‰¹å¾µ
            available_cols = [col for col in transformed_feature_cols if col in X_transformed.columns]
            if len(available_cols) != len(transformed_feature_cols):
                print(f"   âš ï¸  è­¦å‘Šï¼šéƒ¨åˆ†è½‰æ›ç‰¹å¾µä¸å­˜åœ¨æ–¼ DataFrame ä¸­")
                print(f"      é æœŸï¼š{len(transformed_feature_cols)} å€‹ï¼Œå¯¦éš›å¯ç”¨ï¼š{len(available_cols)} å€‹")
            X_transformed = X_transformed[available_cols].copy()
        else:
            # å¦‚æœæ²’æœ‰ transformed_feature_colsï¼Œè‡³å°‘ç¢ºä¿åªä¿ç•™æ•¸å€¼æ¬„ä½
            X_transformed = X_transformed.select_dtypes(include=[np.number])
        
        # ç¢ºä¿ç´¢å¼•å°é½Šï¼ˆä½¿ç”¨ features_df çš„ç´¢å¼•ï¼‰
        if len(X_transformed) == len(features_df):
            # å¦‚æœé•·åº¦ç›¸åŒï¼Œä½¿ç”¨ features_df çš„ç´¢å¼•é‡æ–°ç´¢å¼• X_transformed
            X_transformed = X_transformed.reindex(features_df.index)
            # å¡«å……å¯èƒ½å‡ºç¾çš„ NaNï¼ˆç†è«–ä¸Šä¸æ‡‰è©²æœ‰ï¼Œä½†ç‚ºäº†å®‰å…¨ï¼‰
            X_transformed = X_transformed.fillna(0)
        
        cache_load_time = time.time() - cache_start_time
        print(f"   âœ… å¿«å–è¼‰å…¥å®Œæˆï¼ˆè€—æ™‚ {cache_load_time:.2f} ç§’ï¼‰")
        print(f"   ğŸ“Š åŸå§‹ç‰¹å¾µæ•¸ï¼š{features_df.shape[1]} å€‹")
        print(f"   ğŸ“Š è½‰æ›ç‰¹å¾µæ•¸ï¼š{len(transformed_feature_cols)} å€‹")
        
        # è¼¸å‡ºç‰¹å¾µæ‹†åˆ†èˆ‡æ¸…å–®
        print_feature_breakdown(features_df, transformed_feature_cols, 4)
    elif features_stage3_path.exists() and not features_stage4_path.exists():
        # æœ‰éšæ®µ3ä½†æ²’æœ‰éšæ®µ4ï¼Œå¢é‡åŸ·è¡Œéšæ®µ4
        print(f"\n   ğŸ“‚ ç™¼ç¾éšæ®µ3å¿«å–ï¼Œå°‡åœ¨æ­¤åŸºç¤ä¸ŠåŸ·è¡Œéšæ®µ4ï¼ˆPySparkï¼‰...")
        print(f"   â±ï¸  éšæ®µ4é è¨ˆéœ€è¦ 30-60 åˆ†é˜ï¼Œè«‹è€å¿ƒç­‰å¾…...")
        
        # åˆªé™¤å¯èƒ½å­˜åœ¨çš„èˆŠ scalerï¼ˆé¿å…ç‰¹å¾µæ•¸é‡ä¸åŒ¹é…ï¼‰
        old_scaler_path = Path("data/processed/features_transformed.scaler.pkl")
        old_transformed_path = Path("data/processed/features_transformed.parquet")
        old_info_path = Path("data/processed/features_transformed.info.json")
        
        if old_scaler_path.exists():
            print("   ğŸ—‘ï¸  åˆªé™¤èˆŠçš„ scaler æª”æ¡ˆï¼ˆé¿å…ç‰¹å¾µæ•¸é‡ä¸åŒ¹é…ï¼‰...")
            try:
                old_scaler_path.unlink()
                if old_transformed_path.exists():
                    old_transformed_path.unlink()
                if old_info_path.exists():
                    old_info_path.unlink()
                print("   âœ… èˆŠçš„ scaler æª”æ¡ˆå·²åˆªé™¤")
            except Exception as e:
                print(f"   âš ï¸  åˆªé™¤èˆŠ scaler å¤±æ•—: {e}ï¼ˆå°‡ç¹¼çºŒåŸ·è¡Œï¼‰")
        
        features_start_time = time.time()
        
        # å¢é‡åŸ·è¡Œéšæ®µ4
        features_df, X_transformed, robust_scaler, transformed_feature_cols = processor.process(
            cleaned_df,
            save_features=True,
            save_transformed=True,
            incremental=True  # å¢é‡æ¨¡å¼ï¼šå¾éšæ®µ3åˆ°éšæ®µ4
        )
        
        # æª¢æŸ¥æ˜¯å¦æˆåŠŸç”¢ç”Ÿéšæ®µ4ç‰¹å¾µ
        stage4_features = [
            'bidirectional_flow_count',
            'bidirectional_total_bytes',
            'bidirectional_symmetry'
        ]
        has_stage4 = any(col in features_df.columns for col in stage4_features)
        
        # ç¢ºä¿åªä¿ç•™æ•¸å€¼æ¬„ä½ï¼ˆç§»é™¤å¯èƒ½çš„ Timestamp æ¬„ä½ï¼‰
        X_transformed = X_transformed.select_dtypes(include=[np.number])
        
        features_time = time.time() - features_start_time
        
        if has_stage4:
            print(f"   âœ… éšæ®µ4ç‰¹å¾µè™•ç†å®Œæˆï¼ˆè€—æ™‚ {features_time:.2f} ç§’ï¼‰")
            print(f"   ğŸ’¾ ç‰¹å¾µå·²è‡ªå‹•å„²å­˜ï¼Œä¸‹æ¬¡åŸ·è¡Œå°‡ç›´æ¥è¼‰å…¥")
        else:
            print(f"   âš ï¸  éšæ®µ4ç‰¹å¾µè™•ç†å¤±æ•—ï¼Œä½¿ç”¨éšæ®µ3ç‰¹å¾µï¼ˆè€—æ™‚ {features_time:.2f} ç§’ï¼‰")
            print(f"   ğŸ’¡ æç¤ºï¼šå¯èƒ½æ˜¯ PySpark åœ¨ Windows ä¸Šä¸ç©©å®šï¼Œå·²è‡ªå‹•å›é€€åˆ°éšæ®µ3")
        
        print(f"   ğŸ“Š åŸå§‹ç‰¹å¾µæ•¸ï¼š{features_df.shape[1]} å€‹")
        print(f"   ğŸ“Š è½‰æ›ç‰¹å¾µæ•¸ï¼š{len(transformed_feature_cols)} å€‹")
        
        # è¼¸å‡ºç‰¹å¾µæ‹†åˆ†èˆ‡æ¸…å–®
        actual_stage = 4 if has_stage4 else 3
        print_feature_breakdown(features_df, transformed_feature_cols, actual_stage)
    elif not features_stage3_path.exists() and not features_stage4_path.exists():
        # éƒ½æ²’æœ‰ï¼Œå…ˆåŸ·è¡Œéšæ®µ3ï¼ˆå¿«é€Ÿï¼‰
        print(f"\n   ğŸ”„ æœªç™¼ç¾å¿«å–æª”æ¡ˆï¼Œå…ˆåŸ·è¡Œéšæ®µ3ç‰¹å¾µå·¥ç¨‹ï¼ˆç´„ 10-15 åˆ†é˜ï¼‰...")
        print(f"   ğŸ’¡ éšæ®µ3å®Œæˆå¾Œï¼Œå¯ä»¥é¸æ“‡åŸ·è¡Œéšæ®µ4ï¼ˆç´„ 30-60 åˆ†é˜ï¼‰")
        
        # å…ˆåŸ·è¡Œéšæ®µ3
        processor_stage3 = StandardFeatureProcessor(time_feature_stage=3)
        features_start_time = time.time()
        
        features_df, X_transformed, robust_scaler, transformed_feature_cols = processor_stage3.process(
            cleaned_df,
            save_features=True,
            save_transformed=False  # éšæ®µ3å…ˆä¸å„²å­˜è½‰æ›å¾Œçš„ç‰¹å¾µ
        )
        
        # ç¢ºä¿åªä¿ç•™æ•¸å€¼æ¬„ä½
        X_transformed = X_transformed.select_dtypes(include=[np.number])
        
        features_time = time.time() - features_start_time
        print(f"   âœ… éšæ®µ3ç‰¹å¾µè™•ç†å®Œæˆï¼ˆè€—æ™‚ {features_time:.2f} ç§’ï¼‰")
        print(f"   ğŸ’¾ éšæ®µ3ç‰¹å¾µå·²å„²å­˜")
        print(f"   ğŸ“Š åŸå§‹ç‰¹å¾µæ•¸ï¼š{features_df.shape[1]} å€‹")
        print(f"   ğŸ“Š è½‰æ›ç‰¹å¾µæ•¸ï¼š{len(transformed_feature_cols)} å€‹")
        
        # è¼¸å‡ºç‰¹å¾µæ‹†åˆ†èˆ‡æ¸…å–®
        print_feature_breakdown(features_df, transformed_feature_cols, 3)
        
        # è©¢å•æ˜¯å¦åŸ·è¡Œéšæ®µ4
        print(f"\n   â“ æ˜¯å¦è¦ç¹¼çºŒåŸ·è¡Œéšæ®µ4ï¼ˆPySparkï¼Œç´„ 30-60 åˆ†é˜ï¼‰ï¼Ÿ")
        print(f"   ğŸ’¡ æç¤ºï¼šéšæ®µ4æœƒå¢åŠ é›™å‘æµ Pair èšåˆç‰¹å¾µï¼Œæå‡æ¨¡å‹æ•ˆæœ")
        print(f"   â¸ï¸  å¦‚æœé¸æ“‡è·³éï¼Œå¯ä»¥ç¨å¾Œé‡æ–°åŸ·è¡Œæ­¤è…³æœ¬ï¼Œæœƒè‡ªå‹•å¾éšæ®µ3ç¹¼çºŒ")
        
        # è‡ªå‹•åŸ·è¡Œéšæ®µ4ï¼ˆå¯ä»¥æ”¹ç‚ºäº’å‹•å¼ï¼‰
        execute_stage4 = True  # é è¨­åŸ·è¡Œéšæ®µ4
        
        if execute_stage4:
            print(f"\n   ğŸ”„ é–‹å§‹åŸ·è¡Œéšæ®µ4ç‰¹å¾µå·¥ç¨‹ï¼ˆPySparkï¼‰...")
            stage4_start_time = time.time()
            
            # å¢é‡åŸ·è¡Œéšæ®µ4
            features_df, X_transformed, robust_scaler, transformed_feature_cols = processor.process(
                cleaned_df,
                save_features=True,
                save_transformed=True,
                incremental=True  # å¢é‡æ¨¡å¼ï¼šå¾éšæ®µ3åˆ°éšæ®µ4
            )
            
            # ç¢ºä¿åªä¿ç•™æ•¸å€¼æ¬„ä½
            X_transformed = X_transformed.select_dtypes(include=[np.number])
            
            stage4_time = time.time() - stage4_start_time
            total_time = time.time() - features_start_time
            print(f"   âœ… éšæ®µ4ç‰¹å¾µè™•ç†å®Œæˆï¼ˆè€—æ™‚ {stage4_time:.2f} ç§’ï¼‰")
            print(f"   âœ… ç¸½è¨ˆè€—æ™‚ï¼š{total_time:.2f} ç§’")
            print(f"   ğŸ’¾ éšæ®µ4ç‰¹å¾µå·²å„²å­˜ï¼Œä¸‹æ¬¡åŸ·è¡Œå°‡ç›´æ¥è¼‰å…¥")
            print(f"   ğŸ“Š æœ€çµ‚åŸå§‹ç‰¹å¾µæ•¸ï¼š{features_df.shape[1]} å€‹")
            print(f"   ğŸ“Š æœ€çµ‚è½‰æ›ç‰¹å¾µæ•¸ï¼š{len(transformed_feature_cols)} å€‹")
            
            # è¼¸å‡ºç‰¹å¾µæ‹†åˆ†èˆ‡æ¸…å–®
            print_feature_breakdown(features_df, transformed_feature_cols, 4)
        else:
            print(f"   â¸ï¸  å·²è·³ééšæ®µ4ï¼Œä½¿ç”¨éšæ®µ3ç‰¹å¾µç¹¼çºŒåŸ·è¡Œ")
            # é‡æ–°è¼‰å…¥éšæ®µ3ç‰¹å¾µä¸¦è½‰æ›
            features_df = processor_stage3.load_features(stage=3)
            # æº–å‚™ç‰¹å¾µé›†ä¸¦è½‰æ›
            from src import prepare_feature_set, FeatureSelector, FeatureSelectionStrategy
            X = prepare_feature_set(
                features_df,
                include_base_features=True,
                include_time_features=True,
                time_feature_stage=3
            )
            selector = FeatureSelector(
                remove_constant=True,
                remove_low_variance=True,
                remove_high_correlation=True,
                remove_inf=True,
                remove_high_missing=True,
                correlation_threshold=0.98
            )
            X, _ = selector.select_features(
                X,
                features_df=features_df,
                strategies=[FeatureSelectionStrategy.ALL],
                verbose=False
            )
            X_transformed, robust_scaler, transformed_feature_cols = processor_stage3.transform(
                features_df,
                feature_columns=list(X.columns)
            )
            X_transformed = X_transformed.select_dtypes(include=[np.number])
            
            # è¼¸å‡ºç‰¹å¾µæ‹†åˆ†èˆ‡æ¸…å–®
            print_feature_breakdown(features_df, transformed_feature_cols, 3)
    else:
        # å…¶ä»–æƒ…æ³ï¼šåŸ·è¡Œå®Œæ•´æµç¨‹
        print(f"\n   ğŸ”„ åŸ·è¡Œå®Œæ•´ç‰¹å¾µè™•ç†æµç¨‹ï¼ˆéšæ®µ1-4ï¼‰...")
        features_start_time = time.time()
        
        features_df, X_transformed, robust_scaler, transformed_feature_cols = processor.process(
            cleaned_df,
            save_features=True,
            save_transformed=True
        )
        
        # ç¢ºä¿åªä¿ç•™æ•¸å€¼æ¬„ä½
        X_transformed = X_transformed.select_dtypes(include=[np.number])
        
        features_time = time.time() - features_start_time
        print(f"   âœ… ç‰¹å¾µè™•ç†å®Œæˆï¼ˆè€—æ™‚ {features_time:.2f} ç§’ï¼‰")
        print(f"   ğŸ’¾ ç‰¹å¾µå·²è‡ªå‹•å„²å­˜ï¼Œä¸‹æ¬¡åŸ·è¡Œå°‡ç›´æ¥è¼‰å…¥")
        print(f"   ğŸ“Š åŸå§‹ç‰¹å¾µæ•¸ï¼š{features_df.shape[1]} å€‹")
        print(f"   ğŸ“Š è½‰æ›ç‰¹å¾µæ•¸ï¼š{len(transformed_feature_cols)} å€‹")
        
        # è¼¸å‡ºç‰¹å¾µæ‹†åˆ†èˆ‡æ¸…å–®
        print_feature_breakdown(features_df, transformed_feature_cols, processor.time_feature_stage)
    
    print(f"âœ… ç‰¹å¾µè™•ç†å®Œæˆ")
    
    # 5.5. çµ±ä¸€æŠ½æ¨£ 50,000 ç­†ï¼ˆç¢ºä¿ä¸‰å€‹æ¨¡å‹ä½¿ç”¨ç›¸åŒè³‡æ–™ï¼‰
    print("\n[æ­¥é©Ÿ 5.5] çµ±ä¸€æŠ½æ¨£è³‡æ–™ï¼ˆç¢ºä¿æ¨¡å‹å¯æ¯”è¼ƒæ€§ï¼‰...")
    SAMPLE_SIZE = 50000
    if len(X_transformed) > SAMPLE_SIZE:
        print(f"   åŸå§‹è³‡æ–™é‡: {len(X_transformed):,} ç­†")
        print(f"   ç›®æ¨™æŠ½æ¨£é‡: {SAMPLE_SIZE:,} ç­†")
        
        if 'Label' in features_df.columns:
            # ä½¿ç”¨çµ±ä¸€çš„æ¨™ç±¤è½‰æ›å‡½æ•¸
            features_df = convert_label_to_binary(features_df, verbose=True)
            y_binary = features_df['label_binary']
            
            if (y_binary == 1).sum() == 0:
                print(f"   âš ï¸  è­¦å‘Šï¼šè½‰æ›å¾Œæ²’æœ‰ç•°å¸¸æ¨£æœ¬ï¼è«‹æª¢æŸ¥æ¨™ç±¤è½‰æ›é‚è¼¯")
                # å¦‚æœæ²’æœ‰ç•°å¸¸æ¨£æœ¬ï¼Œç„¡æ³•ä½¿ç”¨ stratifyï¼Œæ”¹ç”¨éš¨æ©ŸæŠ½æ¨£
                print(f"   âš ï¸  æ”¹ç”¨éš¨æ©ŸæŠ½æ¨£ï¼ˆç„¡æ³•ä½¿ç”¨åˆ†å±¤æŠ½æ¨£ï¼‰")
                X_sampled = X_transformed.sample(n=SAMPLE_SIZE, random_state=42)
                y_sampled = y_binary.loc[X_sampled.index].copy()
                features_df_sampled = features_df.loc[X_sampled.index].copy()
            else:
                print(f"   âœ… æ¨™ç±¤è½‰æ›æ­£å¸¸ï¼Œç•°å¸¸æ¨£æœ¬: {(y_binary == 1).sum():,} ç­†")
                
                # ä½¿ç”¨ stratify æŠ½æ¨£ï¼Œç¢ºä¿ç•°å¸¸æ¯”ä¾‹ä¿ç•™
                # ä½¿ç”¨ StratifiedShuffleSplit é€²è¡Œåˆ†å±¤æŠ½æ¨£ä»¥ç¢ºä¿ç²¾ç¢ºæ•¸é‡
                sss = StratifiedShuffleSplit(n_splits=1, train_size=SAMPLE_SIZE, random_state=42)
                train_idx, _ = next(sss.split(X_transformed, y_binary))
                # ä½¿ç”¨ iloc ç²å–ä½ç½®ç´¢å¼•å°æ‡‰çš„è³‡æ–™ï¼Œä¿æŒåŸå§‹ç´¢å¼•
                X_sampled = X_transformed.iloc[train_idx].copy()
                y_sampled = y_binary.iloc[train_idx].copy()
                print(f"   âœ… åˆ†å±¤æŠ½æ¨£å®Œæˆï¼š{len(X_sampled):,} ç­†")
                print(f"   æŠ½æ¨£å¾Œç•°å¸¸æ¯”ä¾‹: {y_sampled.sum()/len(y_sampled)*100:.2f}%")
                print(f"   æŠ½æ¨£å¾Œç•°å¸¸æ¨£æœ¬æ•¸: {y_sampled.sum():,} ç­†")
                
                # æ›´æ–° features_df ç´¢å¼•ä»¥åŒ¹é…æŠ½æ¨£å¾Œçš„è³‡æ–™ï¼ˆä½¿ç”¨ç›¸åŒçš„ç´¢å¼•ï¼‰
                features_df_sampled = features_df.loc[X_sampled.index].copy()
        else:
            # ç„¡æ¨™ç±¤æ™‚ä½¿ç”¨ç°¡å–®éš¨æ©ŸæŠ½æ¨£
            X_sampled = X_transformed.sample(n=SAMPLE_SIZE, random_state=42)
            y_sampled = None
            features_df_sampled = features_df.loc[X_sampled.index].copy()
            print(f"   âœ… éš¨æ©ŸæŠ½æ¨£å®Œæˆï¼š{len(X_sampled):,} ç­†ï¼ˆç„¡æ¨™ç±¤ï¼‰")
        
        X_transformed = X_sampled
        features_df = features_df_sampled
        # æ›´æ–° y_binary ç‚ºæŠ½æ¨£å¾Œçš„æ¨™ç±¤
        if 'Label' in features_df.columns:
            y_binary = y_sampled
    else:
        print(f"   è³‡æ–™é‡ ({len(X_transformed):,} ç­†) å°æ–¼ç›®æ¨™æŠ½æ¨£é‡ ({SAMPLE_SIZE:,} ç­†)ï¼Œè·³éæŠ½æ¨£")
        if 'Label' in features_df.columns:
            # ä½¿ç”¨çµ±ä¸€çš„æ¨™ç±¤è½‰æ›å‡½æ•¸
            if 'label_binary' not in features_df.columns:
                features_df = convert_label_to_binary(features_df, verbose=False)
            y_binary = features_df['label_binary']
        else:
            y_binary = None
    
    # 6. åˆ†å‰²è³‡æ–™
    print("\n[æ­¥é©Ÿ 6] åˆ†å‰²è³‡æ–™...")
    if 'Label' in features_df.columns:
        # ç¢ºä¿æ¨™ç±¤å·²è½‰æ›
        if 'label_binary' not in features_df.columns:
            features_df = convert_label_to_binary(features_df, verbose=False)
        y_binary = features_df['label_binary']
        
        # ç¢ºä¿ X_transformed å’Œ y_binary çš„é•·åº¦ä¸€è‡´
        if len(X_transformed) != len(y_binary):
            print(f"   âš ï¸  è­¦å‘Šï¼šX_transformed ({len(X_transformed):,} ç­†) å’Œ y_binary ({len(y_binary):,} ç­†) é•·åº¦ä¸ä¸€è‡´")
            print(f"   ä½¿ç”¨ X_transformed çš„ç´¢å¼•ä¾†å°é½Š y_binary")
            y_binary = y_binary.loc[X_transformed.index]
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ç•°å¸¸æ¨£æœ¬ï¼Œå¦‚æœæ²’æœ‰å‰‡ä¸ä½¿ç”¨ stratify
        if (y_binary == 1).sum() == 0:
            print(f"   âš ï¸  è­¦å‘Šï¼šæ²’æœ‰ç•°å¸¸æ¨£æœ¬ï¼Œç„¡æ³•ä½¿ç”¨ stratifyï¼Œæ”¹ç”¨éš¨æ©Ÿåˆ†å‰²")
            X_train, X_test, y_train, y_test = train_test_split(
                X_transformed, y_binary, test_size=0.3, random_state=42
            )
        else:
            X_train, X_test, y_train, y_test = train_test_split(
                X_transformed, y_binary, test_size=0.3, random_state=42, stratify=y_binary
            )
        print(f"âœ… è³‡æ–™åˆ†å‰²å®Œæˆï¼šè¨“ç·´é›† {len(X_train):,} ç­†ï¼Œæ¸¬è©¦é›† {len(X_test):,} ç­†")
        print(f"   è¨“ç·´é›†ç•°å¸¸æ¯”ä¾‹: {y_train.sum()/len(y_train)*100:.2f}% ({y_train.sum():,} ç­†ç•°å¸¸)")
        print(f"   æ¸¬è©¦é›†ç•°å¸¸æ¯”ä¾‹: {y_test.sum()/len(y_test)*100:.2f}% ({y_test.sum():,} ç­†ç•°å¸¸)")
        
        # è¨ºæ–·ï¼šæª¢æŸ¥æ¸¬è©¦é›†æ˜¯å¦æœ‰ç•°å¸¸æ¨£æœ¬
        if y_test.sum() == 0:
            print(f"   âš ï¸  è­¦å‘Šï¼šæ¸¬è©¦é›†ä¸­æ²’æœ‰ç•°å¸¸æ¨£æœ¬ï¼")
            print(f"      é€™æœƒå°è‡´ TP=0, FN=0ï¼Œç²¾ç¢ºç‡å’Œå¬å›ç‡éƒ½ç‚º 0")
        else:
            print(f"   âœ… æ¸¬è©¦é›†åŒ…å« {y_test.sum():,} ç­†ç•°å¸¸æ¨£æœ¬ï¼Œå¯ç”¨æ–¼è©•ä¼°")
    else:
        X_train, X_test = train_test_split(
            X_transformed, test_size=0.3, random_state=42
        )
        y_test = None
        print(f"âœ… è³‡æ–™åˆ†å‰²å®Œæˆï¼šè¨“ç·´é›† {len(X_train):,} ç­†ï¼Œæ¸¬è©¦é›† {len(X_test):,} ç­†ï¼ˆç„¡æ¨™ç±¤ï¼‰")
    
    # æ”¶é›†ç‰¹å¾µçµ±è¨ˆä¿¡æ¯
    # éæ¿¾æ‰æ¨™ç±¤æ¬„ä½å’Œéç‰¹å¾µæ¬„ä½
    non_feature_columns = ['Label', 'label_binary', 'StartTime', 'SrcAddr', 'DstAddr', 
                          'Sport', 'Dport', 'State', 'Proto']
    feature_columns = [col for col in features_df.columns 
                      if col not in non_feature_columns]
    
    feature_info = {
        'original_feature_count': len(feature_columns),
        'original_feature_names': feature_columns,  # åªåŒ…å«çœŸæ­£çš„ç‰¹å¾µ
        'transformed_feature_count': len(transformed_feature_cols) if transformed_feature_cols else 0,
        'transformed_feature_names': transformed_feature_cols if transformed_feature_cols else []
    }
    
    return X_train, X_test, y_test, robust_scaler, transformed_feature_cols, feature_info


def evaluate_and_save_model(model_type_name, X_train, X_test, y_test, robust_scaler):
    """è©•ä¼°å–®å€‹æ¨¡å‹ä¸¦ä¿å­˜çµæœ"""
    model_type = MODEL_MAP.get(model_type_name.lower())
    if model_type is None:
        raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹é¡å‹: {model_type_name}ã€‚å¯ç”¨é¡å‹: {list(MODEL_MAP.keys())}")
    
    model_name_map = {
        ModelType.ISOLATION_FOREST: "Isolation Forest",
        ModelType.LOCAL_OUTLIER_FACTOR: "Local Outlier Factor (LOF)",
        ModelType.ONE_CLASS_SVM: "One-Class SVM",
    }
    model_name = model_name_map[model_type]
    
    print("\n" + "=" * 60)
    print(f"è©•ä¼°æ¨¡å‹: {model_name}")
    print("=" * 60)
    
    # å‰µå»ºæ¨¡å‹
    model = ModelFactory.create(model_type)
    
    # æ‰€æœ‰æ¨¡å‹ç¾åœ¨éƒ½ä½¿ç”¨ç›¸åŒçš„è¨“ç·´è³‡æ–™ï¼ˆå·²åœ¨ prepare_data() ä¸­çµ±ä¸€æŠ½æ¨£ï¼‰
    # ä¸å†éœ€è¦å€‹åˆ¥æŠ½æ¨£ï¼Œç¢ºä¿ä¸‰å€‹æ¨¡å‹ä½¿ç”¨å®Œå…¨ç›¸åŒçš„è³‡æ–™
    X_train_actual = X_train
    print(f"  ğŸ“Œ ä½¿ç”¨çµ±ä¸€æŠ½æ¨£çš„è¨“ç·´è³‡æ–™ï¼š{len(X_train_actual):,} ç­†")
    
    # æ¨¡å‹ç‰¹å®šåƒæ•¸è¨­å®š
    train_kwargs = {}
    if model_type == ModelType.LOCAL_OUTLIER_FACTOR:
        # LOF éœ€è¦è¨­å®š n_neighbors
        n_neighbors = min(20, max(5, len(X_train_actual) // 100))
        train_kwargs['n_neighbors'] = n_neighbors
        print(f"  ğŸ“Œ ä½¿ç”¨ n_neighbors={n_neighbors}")
    elif model_type == ModelType.ONE_CLASS_SVM:
        print(f"  ğŸ’¡ æç¤ºï¼šOne-Class SVM è¨ˆç®—è¤‡é›œåº¦è¼ƒé«˜ï¼Œå·²ä½¿ç”¨çµ±ä¸€æŠ½æ¨£åŠ é€Ÿè¨“ç·´")
    
    # è¨“ç·´æ¨¡å‹
    print(f"\n[è¨“ç·´éšæ®µ]")
    start_time = time.time()
    
    if robust_scaler is not None:
        X_train_scaled = robust_scaler.transform(X_train_actual.values)
        X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train_actual.columns, index=X_train_actual.index)
        train_params = {
            'contamination': 0.1,
            'use_external_scaler': True,
            'external_scaler': robust_scaler,
            **train_kwargs
        }
        trained_model, model_scaler = model.train(X_train_scaled_df, **train_params)
    else:
        train_params = {
            'contamination': 0.1,
            **train_kwargs
        }
        trained_model, model_scaler = model.train(X_train_actual, **train_params)
    
    train_time = time.time() - start_time
    print(f"âœ… è¨“ç·´å®Œæˆï¼ˆè€—æ™‚ {train_time:.2f} ç§’ï¼‰")
    
    # é æ¸¬
    print(f"\n[é æ¸¬éšæ®µ]")
    start_time = time.time()
    
    if robust_scaler is not None:
        X_test_scaled = robust_scaler.transform(X_test.values)
        X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)
        scores = model.predict(X_test_scaled_df)
    else:
        scores = model.predict(X_test)
    
    predict_time = time.time() - start_time
    print(f"âœ… é æ¸¬å®Œæˆï¼ˆè€—æ™‚ {predict_time:.2f} ç§’ï¼‰")
    
    # è¨ˆç®—ç•°å¸¸æ¨™ç±¤
    contamination = 0.1
    threshold = np.percentile(scores, contamination * 100)
    predictions = (scores <= threshold).astype(int)
    
    y_test_binary = (y_test == 1).astype(int) if y_test is not None else None
    
    # ç²å–æœ€çµ‚ç”¨æ–¼æ¨¡å‹çš„ç‰¹å¾µæ¬„ä½
    feature_columns = list(X_train_actual.columns)
    
    # è¨ˆç®—æŒ‡æ¨™
    result = {
        'model_name': model_name,
        'model_type': model_type_name.lower(),
        'train_time': train_time,
        'predict_time': predict_time,
        'scores': scores.tolist(),  # è½‰æ›ç‚ºåˆ—è¡¨ä»¥ä¾¿ JSON åºåˆ—åŒ–
        'predictions': predictions.tolist(),
        'threshold': float(threshold),
        'contamination': contamination,
        'feature_columns': feature_columns,  # æœ€çµ‚ç”¨æ–¼æ¨¡å‹çš„ç‰¹å¾µæ¬„ä½åˆ—è¡¨
        'feature_count': len(feature_columns),  # ç‰¹å¾µæ•¸é‡
        'feature_info': {
            'model_feature_count': len(feature_columns),
            'model_feature_names': feature_columns
        }
    }
    
    if y_test_binary is not None:
        print(f"\n[æ•ˆèƒ½æŒ‡æ¨™]")
        print(f"  ç•°å¸¸åˆ†æ•¸ç¯„åœ: [{scores.min():.4f}, {scores.max():.4f}]")
        print(f"  ç•°å¸¸åˆ†æ•¸å¹³å‡å€¼: {scores.mean():.4f}")
        print(f"  ç•°å¸¸åˆ†æ•¸æ¨™æº–å·®: {scores.std():.4f}")
        print(f"  é æ¸¬ç•°å¸¸æ•¸é‡: {predictions.sum()} ({predictions.sum()/len(predictions)*100:.2f}%)")
        print(f"  å¯¦éš›ç•°å¸¸æ•¸é‡: {y_test_binary.sum()} ({y_test_binary.sum()/len(y_test_binary)*100:.2f}%)")
        
        # ä½¿ç”¨è©•ä¼°æ¨¡çµ„è¨ˆç®—æŒ‡æ¨™
        metrics = calculate_metrics(y_test_binary, predictions)
        
        # è¼¸å‡ºæ··æ·†çŸ©é™£ï¼ˆç°¡åŒ–æ ¼å¼ï¼Œç¬¦åˆåŸæœ‰é¢¨æ ¼ï¼‰
        print(f"\n  æ··æ·†çŸ©é™£:")
        print(f"    TN={metrics.tn}, FP={metrics.fp}")
        print(f"    FN={metrics.fn}, TP={metrics.tp}")
        
        # è¼¸å‡ºåŸºæœ¬æŒ‡æ¨™
        print(f"\n  æº–ç¢ºç‡ (Accuracy): {metrics.accuracy:.4f}")
        print(f"  ç²¾ç¢ºç‡ (Precision): {metrics.precision:.4f}")
        print(f"  å¬å›ç‡ (Recall): {metrics.recall:.4f}")
        print(f"  F1 åˆ†æ•¸: {metrics.f1:.4f}")
        
        # è¼¸å‡ºè©³ç´°æŒ‡æ¨™ï¼ˆåŒ…å«å…¬å¼èªªæ˜ï¼‰
        print(f"\n  ç•°å¸¸é¡åˆ¥ï¼ˆæ­£é¡ï¼‰æŒ‡æ¨™:")
        print(f"    ç²¾ç¢ºç‡ (Precision): {metrics.precision:.4f}  [TP/(TP+FP) = {metrics.tp}/({metrics.tp}+{metrics.fp})]")
        print(f"    å¬å›ç‡ (Recall): {metrics.recall:.4f}  [TP/(TP+FN) = {metrics.tp}/({metrics.tp}+{metrics.fn})]")
        print(f"    F1 åˆ†æ•¸: {metrics.f1:.4f}")
        
        print(f"\n  æ­£å¸¸é¡åˆ¥ï¼ˆè² é¡ï¼‰æŒ‡æ¨™:")
        print(f"    ç²¾ç¢ºç‡ (Precision): {metrics.precision_normal:.4f}  [TN/(TN+FN) = {metrics.tn}/({metrics.tn}+{metrics.fn})]")
        print(f"    å¬å›ç‡ (Recall): {metrics.recall_normal:.4f}  [TN/(TN+FP) = {metrics.tn}/({metrics.tn}+{metrics.fp})]")
        
        # è¨ˆç®— ROC AUC
        try:
            scores_normalized = (scores - scores.min()) / (scores.max() - scores.min() + 1e-10)
            scores_prob = 1 - scores_normalized
            roc_auc = roc_auc_score(y_test_binary, scores_prob)
            print(f"  ROC AUC: {roc_auc:.4f}")
        except Exception as e:
            print(f"  ROC AUC: ç„¡æ³•è¨ˆç®— ({str(e)})")
            roc_auc = None
        
        # æ›´æ–° result å­—å…¸
        result.update({
            'accuracy': metrics.accuracy,
            'precision': metrics.precision,  # ç•°å¸¸é¡åˆ¥çš„ç²¾ç¢ºç‡ï¼ˆä¿æŒå‘å¾Œå…¼å®¹ï¼‰
            'recall': metrics.recall,  # ç•°å¸¸é¡åˆ¥çš„å¬å›ç‡ï¼ˆä¿æŒå‘å¾Œå…¼å®¹ï¼‰
            'f1': metrics.f1,
            'roc_auc': roc_auc,
            'precision_anomaly': metrics.precision_anomaly,
            'recall_anomaly': metrics.recall_anomaly,
            'precision_normal': metrics.precision_normal,
            'recall_normal': metrics.recall_normal,
            'tn': metrics.tn,
            'fp': metrics.fp,
            'fn': metrics.fn,
            'tp': metrics.tp,
            'has_labels': True,
        })
        
        # ä¿å­˜è©³ç´°åˆ†é¡å ±å‘Šåˆ°çµæœä¸­ï¼ˆä¸æ‰“å°ï¼‰
        report = classification_report(y_test_binary, predictions, target_names=['æ­£å¸¸', 'ç•°å¸¸'], zero_division=0, output_dict=True)
        result['classification_report'] = report
    else:
        result['has_labels'] = False
    
    # ä¿å­˜çµæœ
    output_dir = Path("output/unsupervised_model_selection")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    output_file = output_dir / f"{model_type_name.lower()}_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… çµæœå·²ä¿å­˜è‡³: {output_file}")
    print(f"ğŸ“Š æ¨¡å‹ä½¿ç”¨çš„ç‰¹å¾µæ•¸é‡: {len(feature_columns)} å€‹")
    print(f"ğŸ“‹ ç‰¹å¾µæ¬„ä½åˆ—è¡¨å·²ä¿å­˜è‡³çµæœæª”æ¡ˆ")
    
    return result


def normalize_model_name(model_name: str) -> str:
    """
    å°‡æ¨¡å‹åç¨±æ¨™æº–åŒ–ç‚ºä¸»è¦åç¨±
    
    Args:
        model_name: æ¨¡å‹åç¨±ï¼ˆå¯èƒ½æ˜¯åˆ¥åï¼‰
    
    Returns:
        æ¨™æº–åŒ–çš„æ¨¡å‹åç¨±ï¼ˆä¸»è¦åç¨±ï¼‰
    
    >>> normalize_model_name('if')
    'isolation_forest'
    >>> normalize_model_name('isolation_forest')
    'isolation_forest'
    >>> normalize_model_name('ocsvm')
    'one_class_svm'
    """
    model_name_lower = model_name.lower()
    
    # å¦‚æœå·²ç¶“æ˜¯ä¸»è¦åç¨±ï¼Œç›´æ¥è¿”å›
    if model_name_lower in ALL_MODELS:
        return model_name_lower
    
    # å°‡åˆ¥åæ˜ å°„åˆ°ä¸»è¦åç¨±
    alias_map = {
        'if': 'isolation_forest',
        'local_outlier_factor': 'lof',
        'ocsvm': 'one_class_svm',
        'svm': 'one_class_svm',
    }
    
    # å…ˆæª¢æŸ¥åˆ¥åæ˜ å°„
    if model_name_lower in alias_map:
        return alias_map[model_name_lower]
    
    # å¦‚æœ MODEL_MAP ä¸­æœ‰ï¼Œå˜—è©¦æ‰¾åˆ°å°æ‡‰çš„ä¸»è¦åç¨±
    if model_name_lower in MODEL_MAP:
        model_type = MODEL_MAP[model_name_lower]
        # åå‘æŸ¥æ‰¾ä¸»è¦åç¨±
        for main_name in ALL_MODELS:
            if MODEL_MAP[main_name] == model_type:
                return main_name
    
    # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›åŸå§‹åç¨±ï¼ˆæœƒåœ¨ä¸‹å±¤æª¢æŸ¥æ™‚å ±éŒ¯ï¼‰
    return model_name_lower


def main():
    parser = argparse.ArgumentParser(
        description='å¿«é€Ÿæ¨¡å‹åŸºæº–æ¸¬è©¦å·¥å…·ï¼ˆä½¿ç”¨æŠ½æ¨£è³‡æ–™ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  # åŸ·è¡Œæ‰€æœ‰æ¨¡å‹ï¼ˆé è¨­ï¼‰
  python scripts/unsupervised_model_selection/quick_model_benchmark.py
  
  # åŸ·è¡Œå–®ä¸€æ¨¡å‹
  python scripts/unsupervised_model_selection/quick_model_benchmark.py --model isolation_forest
  python scripts/unsupervised_model_selection/quick_model_benchmark.py --model lof
  python scripts/unsupervised_model_selection/quick_model_benchmark.py --model one_class_svm
  
  # ä½¿ç”¨åˆ¥å
  python scripts/unsupervised_model_selection/quick_model_benchmark.py --model if
  python scripts/unsupervised_model_selection/quick_model_benchmark.py --model ocsvm
  
  # æ¯”è¼ƒçµæœ
  python scripts/unsupervised_model_selection/compare_model_results.py
        """
    )
    parser.add_argument(
        '--model',
        '-m',
        choices=['isolation_forest', 'if', 'lof', 'local_outlier_factor', 'one_class_svm', 'ocsvm', 'svm', 'all'],
        default='all',
        help='è¦é‹è¡Œçš„æ¨¡å‹é¡å‹ï¼ˆé è¨­ï¼šallï¼ŒåŸ·è¡Œæ‰€æœ‰æ¨¡å‹ï¼‰'
    )
    
    args = parser.parse_args()
    
    # ç¢ºå®šè¦åŸ·è¡Œçš„æ¨¡å‹åˆ—è¡¨
    if args.model.lower() == 'all':
        models_to_run = ALL_MODELS
        print("=" * 60)
        print("åŸ·è¡Œæ‰€æœ‰æ¨¡å‹")
        print("=" * 60)
        print(f"å°‡åŸ·è¡Œä»¥ä¸‹æ¨¡å‹ï¼š{', '.join(models_to_run)}")
    else:
        normalized_name = normalize_model_name(args.model)
        if normalized_name not in ALL_MODELS:
            raise ValueError(
                f"æœªçŸ¥çš„æ¨¡å‹é¡å‹: {args.model}ã€‚\n"
                f"å¯ç”¨é¡å‹: {', '.join(ALL_MODELS)}\n"
                f"å¯ç”¨åˆ¥å: if, local_outlier_factor, ocsvm, svm"
            )
        
        models_to_run = [normalized_name]
        print("=" * 60)
        print(f"åŸ·è¡Œå–®ä¸€æ¨¡å‹: {normalized_name}")
        print("=" * 60)
    
    # æº–å‚™è³‡æ–™ï¼ˆæ‰€æœ‰æ¨¡å‹å…±ç”¨ç›¸åŒè³‡æ–™ï¼Œåªéœ€æº–å‚™ä¸€æ¬¡ï¼‰
    print("\næº–å‚™è³‡æ–™ï¼ˆæ‰€æœ‰æ¨¡å‹å…±ç”¨ï¼‰...")
    X_train, X_test, y_test, robust_scaler, feature_cols, feature_info = prepare_data()
    
    # åŸ·è¡Œæ¯å€‹æ¨¡å‹
    results = {}
    total_start_time = time.time()
    
    for i, model_name in enumerate(models_to_run, 1):
        print("\n" + "=" * 60)
        print(f"[{i}/{len(models_to_run)}] åŸ·è¡Œæ¨¡å‹: {model_name}")
        print("=" * 60)
        
        try:
            # è©•ä¼°ä¸¦ä¿å­˜æ¨¡å‹
            result = evaluate_and_save_model(model_name, X_train, X_test, y_test, robust_scaler)
            results[model_name] = result
            
            # æ›´æ–°çµæœæ–‡ä»¶ï¼Œæ·»åŠ å®Œæ•´çš„ç‰¹å¾µä¿¡æ¯
            output_dir = Path("output/unsupervised_model_selection")
            output_file = output_dir / f"{model_name}_results.json"
            
            # è®€å–ç¾æœ‰çµæœä¸¦æ›´æ–°
            if output_file.exists():
                with open(output_file, 'r', encoding='utf-8') as f:
                    result_data = json.load(f)
            else:
                result_data = result.copy()
            
            # ç¢ºä¿ feature_info éµå­˜åœ¨
            if 'feature_info' not in result_data:
                result_data['feature_info'] = {}
            
            # æ·»åŠ å®Œæ•´çš„ç‰¹å¾µä¿¡æ¯
            result_data['feature_info'].update({
                'original_feature_count': feature_info['original_feature_count'],
                'original_feature_names': feature_info['original_feature_names'],
                'transformed_feature_count': feature_info['transformed_feature_count'],
                'transformed_feature_names': feature_info['transformed_feature_names']
            })
            
            # ä¿å­˜æ›´æ–°å¾Œçš„çµæœ
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, indent=2, ensure_ascii=False)
            
            print(f"\nâœ… æ¨¡å‹ {model_name} åŸ·è¡Œå®Œæˆ")
            print(f"ğŸ“Š ç‰¹å¾µçµ±è¨ˆä¿¡æ¯å·²æ›´æ–°åˆ°çµæœæ–‡ä»¶")
            print(f"   åŸå§‹ç‰¹å¾µæ•¸: {feature_info['original_feature_count']} å€‹")
            print(f"   è½‰æ›å¾Œç‰¹å¾µæ•¸: {feature_info['transformed_feature_count']} å€‹")
            print(f"   æ¨¡å‹ä½¿ç”¨ç‰¹å¾µæ•¸: {result_data['feature_info']['model_feature_count']} å€‹")
            print(f"   çµæœå·²ä¿å­˜è‡³: {output_file}")
            
        except Exception as e:
            print(f"\nâŒ æ¨¡å‹ {model_name} åŸ·è¡Œå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            results[model_name] = {'error': str(e)}
            continue
    
    total_time = time.time() - total_start_time
    
    # ç¸½çµ
    print("\n" + "=" * 60)
    print("åŸ·è¡Œç¸½çµ")
    print("=" * 60)
    print(f"ç¸½è€—æ™‚: {total_time:.2f} ç§’")
    print(f"æˆåŠŸåŸ·è¡Œ: {len([r for r in results.values() if 'error' not in r])}/{len(models_to_run)} å€‹æ¨¡å‹")
    
    if len(models_to_run) > 1:
        print(f"\næ‰€æœ‰çµæœå·²ä¿å­˜è‡³: output/unsupervised_model_selection/")
        print(f"ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¯”è¼ƒçµæœ:")
        print(f"  python scripts/unsupervised_model_selection/compare_model_results.py")
    else:
        model_name = models_to_run[0]
        print(f"\nçµæœå·²ä¿å­˜è‡³: output/unsupervised_model_selection/{model_name}_results.json")
        print(f"ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¯”è¼ƒçµæœ:")
        print(f"  python scripts/unsupervised_model_selection/compare_model_results.py")


if __name__ == "__main__":
    main()

